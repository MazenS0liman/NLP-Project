{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Setup</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import typing\n",
    "import random\n",
    "import emoji\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(file_path: str) -> list:\n",
    "    '''\n",
    "    process youtube/podcast documents\n",
    "\n",
    "    @param file_path: a string represent file path\n",
    "    '''\n",
    "\n",
    "    with open(file_path, 'r', errors=\"ignore\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Remove numbers followed by ':'\n",
    "    text = re.sub(r'\\d+.*\\d*\\s*:', '', text)\n",
    "\n",
    "    # Define sentence delimiters for Arabic\n",
    "    sentence_endings = r'(?<=[.!؟؛،])\\s+'\n",
    "\n",
    "    # Split sentences while preserving dependencies\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 1] # Remove empty strings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_df(sentences: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    convert a list of sentences to a dataframe\n",
    "\n",
    "    @param sentences: a list of sentences\n",
    "    '''\n",
    "\n",
    "    return pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path: str = \"./data\") -> pd.DataFrame:\n",
    "    '''\n",
    "    Load dataset from a directory\n",
    "\n",
    "    :params: **file_path**: a string representing file path to dataset.\n",
    "    '''\n",
    "    output_df = []\n",
    "\n",
    "    data_folder = \"./data\"\n",
    "\n",
    "    for file_name in os.listdir(data_folder):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(data_folder, file_name)\n",
    "            tmp_df = sentences_to_df(extract_sentences(file_path))\n",
    "            output_df.append(tmp_df)\n",
    "\n",
    "    output_df = pd.concat(output_df, ignore_index=True)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                      0\n",
       "0                لكل شخص في آخر الشهر،\n",
       "1               ما معاهوش غير 50 جنيه،\n",
       "2           ومحتاج ياكل أكلة تشبّعه...\n",
       "3                    لكل واحد \"فورمة\"،\n",
       "4              محتاج أكلة سريعة الهضم،\n",
       "..                                 ...\n",
       "733          لازم السندوتش معاه بطاطس.\n",
       "734              وأقوم مطلّع بطاطساية،\n",
       "735          وأقوم حاططها في السندوتش!\n",
       "736  دا أنا أحيانًا بجيب سندوتش بطاطس،\n",
       "737    من \"الحرمين\" اللي في \"الحُصري\".\n",
       "\n",
       "[738 rows x 1 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./data/البطاطس  الدحيح.txt\"\n",
    "df = load_dataset(file_path)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Libraries & Models</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ghalatawi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ghalatawi:** an Arabic Autocorrect library مكتبة للتصحيح التلقائي للغة العربية\n",
    "\n",
    "Source: https://github.com/linuxscout/ghalatawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regex': True, 'wordlist': True, 'punct': True, 'typo': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ghalatawi.autocorrector import AutoCorrector\n",
    "\n",
    "autoco = AutoCorrector()\n",
    "\n",
    "autoco.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_Note:_**</span> The library allow for fixing spelling, adjusting punctuations, typos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyArabic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyArabic:** a Arabic language library for Python, provides basic functions to manipulate Arabic letters and text, like detecting Arabic letters, Arabic letters groups and characteristics, remove diacritics etc.\n",
    "\n",
    "Source: https://github.com/linuxscout/pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CAMeL Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CAMeL Bert:** a collection of BERT models pre-trained on Arabic texts with different sizes and variants.\n",
    "\n",
    "Source: https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name CAMeL-Lab/bert-base-arabic-camelbert-msa. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# Load Arabic SBERT Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model = SentenceTransformer(\"CAMeL-Lab/bert-base-arabic-camelbert-msa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DSAraby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DSAraby:** is a library that aims to transliterate text which is to write a word using the closest corresponding letters of a different alphabet or language.\n",
    "\n",
    "Source: https://github.com/saobou/DSAraby/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsaraby import DSAraby\n",
    "\n",
    "ds = DSAraby()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tashaphyne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tashaphyne:** is an Arabic light stemmer and segmentor. It mainly supports light stemming (removng prefixes and suffixes) and gives all possible segmentations. it uses a modified finite state automation, which allows it to generate all segmentations.\n",
    "\n",
    "Source: https://github.com/linuxscout/tashaphyne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from tashaphyne.arabicstopwords import STOPWORDS as TASHAPHYNE_STOPWORDS\n",
    "\n",
    "tashaphyne_stemmer = ArabicLightStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CaMeL Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CaMeL Tools:** is suite of Arabic natural language processing tools developed by the CAMeL Lab at New York University Abu Dhabi.\n",
    "\n",
    "Source: https://github.com/CAMeL-Lab/camel_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mazen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\camel_tools\\cli\\camel_data.py\n"
     ]
    }
   ],
   "source": [
    "import camel_tools\n",
    "\n",
    "from camel_tools.data import downloader\n",
    "from camel_tools.ner import NERecognizer\n",
    "from camel_tools.morphology import analyzer\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "# from camel_tools.dialectid import DIDPred\n",
    "from camel_tools.morphology.analyzer import Analyzer\n",
    "from camel_tools.tagger.default import DefaultTagger\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "from camel_tools.morphology.reinflector import Reinflector\n",
    "from camel_tools.morphology.generator import Generator\n",
    "\n",
    "camel_data_path = os.path.join(os.path.dirname(camel_tools.__file__), 'cli', 'camel_data.py')\n",
    "print(camel_data_path)\n",
    "\n",
    "downloader.DownloaderError(\"calima-msa-r13\")\n",
    "\n",
    "morph_db = MorphologyDB.builtin_db(flags = 'r')\n",
    "analyzer = Analyzer(morph_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Farasa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Farasa:** is the state-of-the-art library for dealing with Arabic Language Processing. It has been developed by Arabic Language Technologies Group at Qatar Computing Research Institute (QCRI).\n",
    "\n",
    "Source: https://github.com/MagedSaeed/farasapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farasa.stemmer import FarasaStemmer\n",
    "\n",
    "farasa_stemmer = FarasaStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tnkeeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tnkeeh:** is an Arabic preprocessing library for python. it was designe dusing `re` for creating quick replacement expressions for several examples such as Quick cleaning, Segmentation, Normalization and Data splitting.\n",
    "\n",
    "Source: https://github.com/ARBML/tnkeeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tnkeeh as tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK:** a leading platform for building Python programs to work with human language data.\n",
    "\n",
    "Source: https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\mazen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\mazen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mazen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "NLTK_STOPWORDS = set(stopwords.words('arabic'))\n",
    "nltk_stemmer = ISRIStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SinaTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SinaTools:** an Open-Source Toolkit for Arabic NLP and NLU developed by SinaLab at Birzeit University.\n",
    "\n",
    "Models:\n",
    "- morph: https://sina.birzeit.edu/lemmas_dic.pickle,\n",
    "- ner: https://sina.birzeit.edu/Wj27012000.tar.gz,\n",
    "- wsd_model: https://sina.birzeit.edu/bert-base-arabertv02_22_May_2021_00h_allglosses_unused01.zip,\n",
    "- wsd_tokenizer: https://sina.birzeit.edu/bert-base-arabertv02.zip,\n",
    "- one_gram: https://sina.birzeit.edu/one_gram.pickle,\n",
    "- five_grams: https://sina.birzeit.edu/five_grams.pickle,\n",
    "- four_grams: https://sina.birzeit.edu/four_grams.pickle,\n",
    "- three_grams: https://sina.birzeit.edu/three_grams.pickle,\n",
    "- two_grams: https://sina.birzeit.edu/two_grams.pickle,\n",
    "- graph_l2: https://sina.birzeit.edu/graph_l2.pkl,\n",
    "- graph_l3: https://sina.birzeit.edu/graph_l3.pkl,\n",
    "- relation: https://sina.birzeit.edu/relation_model.zip\n",
    "\n",
    "\n",
    "Source: https://github.com/SinaLab/SinaTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinatools.morphology import morph_analyzer\n",
    "from sinatools.utils import text_transliteration\n",
    "from sinatools.synonyms.synonyms_generator import evaluate_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use different python versions (other than 3.12)\n",
    "# from sinatools.ner.entity_extractor import extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🤗 Transformers:** a library that provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.\n",
    "\n",
    "Source: https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-Learn**: a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license.\n",
    "\n",
    "Source: https://github.com/scikit-learn/scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Arabert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arabert**: is an Arabic pretrained language model based on Google's BERT architecture.\n",
    "\n",
    "Source: https://huggingface.co/aubmindlab/bert-base-arabertv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arabert.preprocess import ArabertPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Cleaning</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tidying Up Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Orthographic mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Spelling inconsistencies (Text Correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_correct(text: str) -> str:\n",
    "    '''\n",
    "    A method that that fixes typos, punctuation and spelling mistakes.\n",
    "    '''\n",
    "    autoco = AutoCorrector()\n",
    "\n",
    "    autoco.show_config()\n",
    "\n",
    "    output = autoco.spell(text)\n",
    "\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "إذا أردت استعارة كتاب، اذهب إلى المكتبة أو الادارة في الظهيرة.\n"
     ]
    }
   ],
   "source": [
    "text = \"إذا أردت إستعارة كتاب، اذهب إلى المكتبة أو الادارة في الضهيرة.\"\n",
    "auto_correct(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سنقر لا تسرك\n"
     ]
    }
   ],
   "source": [
    "text = 'سنقر لا تسرك'\n",
    "auto_correct(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_OBSERVATION:_**</span> The library does not always fix typos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Unknown characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Repeated letters and with spaces in the words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Reshape Text\n",
    "\n",
    "https://pypi.org/project/arabic-reshaper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_NOTE:_**</span> One of the problems in text collected from youtube/podcast is that their is no true sentence structure is made that we split text upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_sentence_segmentation(paragraph: str) -> str:\n",
    "    '''\n",
    "    A method that segmente arabic pargaraphs to meaningful sentences\n",
    "\n",
    "    @param paragraph: a bunch of sentences that are segmented to meaningful sentences.\n",
    "    '''\n",
    "\n",
    "    # Compute sentence embeddings\n",
    "    embeddings = sbert_model.encode(paragraph)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    # Find semantic breakpoints (low similarity)\n",
    "    threshold = 0.5  # Adjust this based on experimentation\n",
    "    split_points = [i for i in range(len(paragraph) - 1) if sim_matrix[i, i+1] < threshold]\n",
    "\n",
    "    # Generate semantic splits\n",
    "    segments = []\n",
    "    start = 0\n",
    "    for split in split_points:\n",
    "        segments.append(\" \".join(paragraph[start:split+1]))\n",
    "        start = split + 1\n",
    "\n",
    "    segments.append(\" \".join(paragraph[start:]))\n",
    "\n",
    "    # Remove empty strings\n",
    "    segments = [seg for seg in segments if seg.strip()]\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nلكل شخص في آخر الشهر،', 'ما معاهوش غير 50 جنيه،', 'لكل واحد مش عارف ياكل،', 'ونفسه في أكلة جانبية مع الأكل،', 'القاهره هي عاصمة مصر،', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nلكل شخص في آخر الشهر، ما معاهوش غير 50 جنيه، لكل واحد مش عارف ياكل، ونفسه في أكلة جانبية مع الأكل،',\n",
       " 'القاهره هي عاصمة مصر،']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "لكل شخص في آخر الشهر،\n",
    "ما معاهوش غير 50 جنيه،\n",
    "لكل واحد مش عارف ياكل،\n",
    "ونفسه في أكلة جانبية مع الأكل،\n",
    "القاهره هي عاصمة مصر،\n",
    "\"\"\"\n",
    "\n",
    "# Define sentence delimiters for Arabic\n",
    "sentence_endings = r'(?<=[.!؟؛،])\\s+'\n",
    "\n",
    "# Split sentences while preserving dependencies\n",
    "sentences = re.split(sentence_endings, paragraph)\n",
    "print(sentences)\n",
    "\n",
    "arabic_sentence_segmentation(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> handle text that contains both english and arabic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Arabizi to Arabic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Arabizi:_**</span> is a sentence that contain Latin words (3ami)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabizi_to_arabic(text: str) -> tuple:\n",
    "    '''\n",
    "    A method that gives the possible words in Arabic based on a given word in Latin by mapping\n",
    "    the Latin letters to Arabic ones, then takes the most frequent word existing in a corpus.\n",
    "    \n",
    "    :param text: A sentence containing Arabizi (e.g., \"3ami\") that needs to be converted to Arabic.\n",
    "\n",
    "    :return: A tuple of two values: \n",
    "        - The transliterated text based on the given schema. \n",
    "        - A boolean flag indicating whether all characters in the input text were successfully transliterated or not.\n",
    "    '''\n",
    "\n",
    "    transliterate_text = text_transliteration.perform_transliteration(text, \"bw2ar\")[0]\n",
    "\n",
    "    conversion_dict = {\n",
    "        'a': 'ا', 'b': 'ب', 't': 'ت', 'th': 'ث', 'g': 'ج', '7': 'ح', 'kh': 'خ',\n",
    "        'd': 'د', 'dh': 'ذ', 'r': 'ر', 'z': 'ز', 's': 'س', 'sh': 'ش', '9': 'ص',\n",
    "        '6': 'ط', '3': 'ع', 'gh': 'غ', 'f': 'ف', 'q': 'ق', 'k': 'ك', 'l': 'ل',\n",
    "        'm': 'م', 'n': 'ن', 'h': 'ه', 'w': 'و', 'y': 'ي', '?': \"؟\"\n",
    "    }\n",
    "    \n",
    "    for latin, arabic in conversion_dict.items():\n",
    "        transliterate_text = transliterate_text.replace(latin, arabic)\n",
    "\n",
    "    transliterate_text = ds.transliterate(transliterate_text)\n",
    "\n",
    "    # Check if all characters in the result are Arabic\n",
    "    valid_arabic_regex = re.compile(r'^[\\u0600-\\u06FF\\s.,،؟!؛]+$')\n",
    "    transliterate_successed = all(valid_arabic_regex.match(char) for char in transliterate_text)\n",
    "\n",
    "    return transliterate_text, transliterate_successed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabizi: mar7aba, kayf 7alak?\n",
      "Arabic: ('مَرحَبَۥ كَيف حَلَك؟', True)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "arabizi_text = \"mar7aba, kayf 7alak?\"\n",
    "arabic_text = arabizi_to_arabic(arabizi_text)\n",
    "\n",
    "print(\"Arabizi:\", arabizi_text)\n",
    "print(\"Arabic:\", arabic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Stemming:_**</span> is the process of reducing a word to its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_stemming(text: str, tool: str) -> str:\n",
    "    '''\n",
    "    A method that perform arabic text stemming\n",
    "\n",
    "    @param text: A sentence that requires stemming\n",
    "    '''\n",
    "    zen = TextBlob(text) # check for alternatives\n",
    "    words = zen.words\n",
    "    \n",
    "    if tool == 'camel':\n",
    "        return ' '.join([analyzer.analyze(word)[0]['stem'] for word in words])\n",
    "    elif tool == 'farasa':\n",
    "        return farasa_stemmer.stem(text)\n",
    "    elif tool == \"light\":\n",
    "        return ' '.join([tashaphyne_stemmer.light_stem(word) for word in words])\n",
    "    else:\n",
    "        return ' '.join([nltk_stemmer.stem(word) for word in words])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_Note:_**</span> ISRI Stemmer is a stemming process that is based on algorithm (Arabic Stemming without a root dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ذَهِّب طُلّاب آل مُدَرِّس صُباح عُود فِي مَساء\n",
      "ذهب طالب إلى مدرسة صباح عاد في مساء .\n",
      "ذهب طلاب إلى مدرس صباحا عود في مساء\n",
      "ذهب طلب الى درس صبح يعد في ساء\n"
     ]
    }
   ],
   "source": [
    "text = \"يذهب الطلاب إلى المدرسة صباحًا ويعودون في المساء.\"\n",
    "print(arabic_stemming(text, \"camel\"))\n",
    "print(arabic_stemming(text, \"farasa\"))\n",
    "print(arabic_stemming(text, \"light\"))\n",
    "print(arabic_stemming(text, \"nltk\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_Question:_**</span> How to determine which of the models is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Lemmatization:_**</span> is the process of reducing the different forms of a word to one single form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_lemmatization(text: str) -> str:\n",
    "    '''\n",
    "    A method that perform arabic text lemmatization\n",
    "\n",
    "    @param text: A sentence that requires lemmatization\n",
    "    '''\n",
    "    words = simple_word_tokenize(text) # check for alternatives\n",
    "\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        lemma = morph_analyzer.analyze(word)[0][\"lemma\"].split(\"|\")[0]\n",
    "\n",
    "        # Remove any character that is not in the Arabic Unicode range\n",
    "        clean_lemma = re.sub(r'[^\\u0600-\\u06FF]', '', lemma)\n",
    "        if clean_lemma:\n",
    "            lemmatized_words.append(clean_lemma)\n",
    "    \n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 1:  ذَهَبَ طَالِبٌ إِلَى مَدْرَسَةٌ صَباحٌ عَادَ فِي مَسَاءٌ\n",
      "example 2: رَجُلٌ أَحَبَّ طِفْلٌ نِسَاءٌ قَرَأَ كِتَابٌ\n"
     ]
    }
   ],
   "source": [
    "text = \"يذهب الطلاب إلى المدرسة صباحًا ويعودون في المساء.\"\n",
    "print(\"example 1: \", arabic_lemmatization(text))\n",
    "\n",
    "text = \"الرجال يحبون الأطفال والنساء يقرأن الكتب.\"\n",
    "print(\"example 2:\", arabic_lemmatization(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Observation:_**</span> The `arabic_lemmatization` method produce lemmatized text with diacritics (tashkel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Stopwords:_**</span> are most common terms in an Arabic language such as حروف الجر."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_stopwords(text: str, custom_stopwords: bool=None, use_nltk: bool=True, use_tashaphyne: bool=True) -> str:\n",
    "    '''\n",
    "    A method that remove stopwords in text\n",
    "\n",
    "    @param text: a sentence that requires removing stopwords.\n",
    "    '''\n",
    "\n",
    "    # Get Arabic stopwords\n",
    "    stopwords = set()\n",
    "\n",
    "    if use_nltk:\n",
    "        stopwords.update(NLTK_STOPWORDS)\n",
    "    if use_tashaphyne:\n",
    "        stopwords.update(TASHAPHYNE_STOPWORDS)\n",
    "    if custom_stopwords:\n",
    "        stopwords.update(custom_stopwords)\n",
    "\n",
    "    stopwords_comp = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\n",
    "                    \"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\n",
    "                    \"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\n",
    "                    \"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\n",
    "                    \"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\n",
    "                    \"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\n",
    "                    \"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\n",
    "                    \"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\n",
    "                    \"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\n",
    "                    \"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\n",
    "                    \"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\n",
    "                    \"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\n",
    "                    \"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\n",
    "                    \"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\n",
    "                    \"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\n",
    "                    \"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\n",
    "                    \"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\n",
    "                    \"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\n",
    "                    \"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\n",
    "                    \"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\n",
    "                    \"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\n",
    "                    \"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\n",
    "                    \"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\n",
    "                    \"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\n",
    "                    \"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\n",
    "                    \"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\n",
    "                    \"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\n",
    "                    \"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\n",
    "                    \"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\n",
    "                    \"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\n",
    "                    \"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\n",
    "                    \"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\n",
    "                    \"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\n",
    "                    \"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\n",
    "                    \"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\n",
    "                    \"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\n",
    "                    \"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\n",
    "                    \"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\n",
    "                    \"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\n",
    "                    \"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\n",
    "                    \"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\n",
    "                    \"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\n",
    "                    \"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\n",
    "                    \"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\n",
    "                    \"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\n",
    "                    \"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\n",
    "                    \"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\n",
    "                    \"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\n",
    "                    \"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\n",
    "                    \"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\n",
    "                    \"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\n",
    "                    \"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\n",
    "                    \"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\n",
    "                    \"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\n",
    "                    \"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\"}\n",
    "\n",
    "    words = simple_word_tokenize(text)\n",
    "\n",
    "    return \" \".join([w for w in words if not w in stopwords and not w in stopwords_comp and len(w) >= 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Original: أنا أحب التفاح في الصباح\n",
      "Filtered: أحب التفاح الصباح\n",
      "\n",
      "Example 2\n",
      "Original: الطالب المجتهد يدرس في الجامعة\n",
      "Filtered: الطالب المجتهد يدرس الجامعة\n",
      "\n",
      "Example 3\n",
      "Original: الطالب المجتهد يدرس في الجامعة\n",
      "Filtered with custom stopwords: الطالب يدرس الجامعة\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple sentence with a few common stopwords.\n",
    "text1 = \"أنا أحب التفاح في الصباح\"\n",
    "# Expected behavior: Words like \"أنا\" and \"في\" (if included in our stopword sets) should be removed.\n",
    "print(\"Example 1\")\n",
    "print(\"Original:\", text1)\n",
    "print(\"Filtered:\", remove_arabic_stopwords(text1))\n",
    "print()\n",
    "\n",
    "# Example 2: Sentence with additional stopwords from stopwords_comp.\n",
    "text2 = \"الطالب المجتهد يدرس في الجامعة\"\n",
    "# Expected behavior: Words that are common stopwords (e.g., \"في\") should be removed.\n",
    "print(\"Example 2\")\n",
    "print(\"Original:\", text2)\n",
    "print(\"Filtered:\", remove_arabic_stopwords(text2))\n",
    "print()\n",
    "\n",
    "# Example 3: Using custom stopwords to remove an additional word.\n",
    "custom_stops = {\"المجتهد\"}\n",
    "text3 = \"الطالب المجتهد يدرس في الجامعة\"\n",
    "# Expected behavior: In addition to the default stopwords, \"المجتهد\" should be removed.\n",
    "print(\"Example 3\")\n",
    "print(\"Original:\", text3)\n",
    "print(\"Filtered with custom stopwords:\", remove_arabic_stopwords(text3, custom_stopwords=custom_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Handling Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Purpose:* The idea is that arabic text can sometimes contains hashtags as for example \"مبارك_عليكم_الشهر ربي اجعل شهر رمضان فاتحة خير لنا وبداية أجمل أقدارنا وحقق لنا ما نتمنى يا كريم#\" which need to be converted to \" مبارك عليكم الشهر ربي اجعل شهر رمضان فاتحةخير لنا وبداية أجمل أقدارنا\n",
    "وحقق لنا ما نتمنى يا كريم\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_hashtag(word: str) -> bool:\n",
    "    '''\n",
    "    Checks whether a word starts or ends with a hashtag.\n",
    "    @param word: a single word\n",
    "    @return: True if the word starts or ends with \"#\", False otherwise.\n",
    "    '''\n",
    "    return word.startswith(\"#\") or word.endswith(\"#\")\n",
    "\n",
    "def split_hashtag_to_words(tag: str) -> list:\n",
    "    '''\n",
    "    Converts a hashtag to a list of words.\n",
    "    If the hashtag uses underscores, they are used as delimiters;\n",
    "    otherwise, it applies a camel-case splitting pattern.\n",
    "    \n",
    "    @param tag: a hashtag (e.g., \"#مبارك_عليكم_الشهر\")\n",
    "    @return: a list of words extracted from the hashtag.\n",
    "    '''\n",
    "    tag = tag.replace('#', '')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1:\n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def extract_hashtag(text: str) -> list:\n",
    "    '''\n",
    "    Extracts words from hashtags found in the input text.\n",
    "    It removes trailing punctuation and then splits the hashtag.\n",
    "    \n",
    "    @param text: a sentence that contains one or more hashtags.\n",
    "    @return: a list of words extracted from hashtags.\n",
    "    '''\n",
    "    hash_list = [re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\") or i.endswith(\"#\")]\n",
    "    word_list = []\n",
    "    for word in hash_list:\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list\n",
    "\n",
    "def clean_arabic_hashtag(text: str) -> str:\n",
    "    '''\n",
    "    Replaces each hashtag in the text with its space-separated equivalent.\n",
    "    Note: Only words starting with \"#\" are processed.\n",
    "    \n",
    "    @param text: a sentence that contains hashtags.\n",
    "    @return: the text with cleaned hashtags.\n",
    "    '''\n",
    "    words = text.split()\n",
    "    output = []\n",
    "    for word in words:\n",
    "        if has_hashtag(word):\n",
    "            output.extend(extract_hashtag(word))\n",
    "        else:\n",
    "            output.append(word)\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "Original: #مبارك_عليكم_الشهر ربي اجعل شهر رمضان فاتحة خير لنا وبداية أجمل أقدارنا وحقق لنا ما نتمنى يا كريم#\n",
      "Cleaned:  مبارك عليكم الشهر ربي اجعل شهر رمضان فاتحة خير لنا وبداية أجمل أقدارنا وحقق لنا ما نتمنى يا\n"
     ]
    }
   ],
   "source": [
    "print(\"Example:\")\n",
    "# Hashtags at both beginning and end.\n",
    "test3 = \"#مبارك_عليكم_الشهر ربي اجعل شهر رمضان فاتحة خير لنا وبداية أجمل أقدارنا وحقق لنا ما نتمنى يا كريم#\"\n",
    "print(\"Original:\", test3)\n",
    "print(\"Cleaned: \", clean_arabic_hashtag(test3))\n",
    "# Expected output:\n",
    "# the first token starts with '#' so it gets converted to \"مبارك عليكم الشهر\"\n",
    "# Thus, output will be: \"مبارك عليكم الشهر ربي اجعل شهر رمضان فاتحة خير لنا وبداية أجمل أقدارنا وحقق لنا ما نتمنى يا كريم\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Handling Emojis 🤪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_emojis(text: str, mode: str = 'remove') -> str:\n",
    "    '''\n",
    "    A method that handles emojis.\n",
    "    '''\n",
    "    if mode == 'remove':\n",
    "        return emoji.replace_emoji(text, '')\n",
    "    elif mode == 'description':\n",
    "        return emoji.demojize(text, language='ar')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: أنا أحب القراءة 📚 وأستمتع بها كثيراً 😊\n",
      "Without emojis: أنا أحب القراءة  وأستمتع بها كثيراً \n",
      "With emoji descriptions: أنا أحب القراءة :كتب: وأستمتع بها كثيراً :وجه_باسم_بعينين_باسمتين:\n"
     ]
    }
   ],
   "source": [
    "text_with_emoji = \"أنا أحب القراءة 📚 وأستمتع بها كثيراً 😊\"\n",
    "text_without_emoji = handle_emojis(text_with_emoji, 'remove')\n",
    "text_with_descriptions = handle_emojis(text_with_emoji, 'description')\n",
    "\n",
    "print(\"Original:\", text_with_emoji)\n",
    "print(\"Without emojis:\", text_without_emoji)\n",
    "print(\"With emoji descriptions:\", text_with_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> Search on how to extract meaning from emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Normalization:_**</span> match digits that have the same writing but different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text: str, tool: str) -> str:\n",
    "    '''\n",
    "    A method that match digits that have same writing but different encodings\n",
    "\n",
    "    @param text: a sentence that requires normalizing its text.\n",
    "    @param tool: determining which library name to utilize in normalizing text.\n",
    "    '''\n",
    "    \n",
    "    if tool == \"tnkeeh\":\n",
    "        normalizer = tn.Tnkeeh(normalize=True)\n",
    "        output = normalizer.clean_raw_text(text)\n",
    "        return output[0]\n",
    "    elif tool == \"camel\":\n",
    "        return normalize_unicode(text)\n",
    "    else:\n",
    "        text = text.strip()\n",
    "        text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "        text = re.sub(\"ى\", \"ي\", text)\n",
    "        text = re.sub(\"ؤ\", \"ء\", text)\n",
    "        text = re.sub(\"ئ\", \"ء\", text)\n",
    "        text = re.sub(\"ة\", \"ه\", text)\n",
    "        text = re.sub(\"گ\", \"ك\", text)\n",
    "        text = re.sub(\"ڤ\", \"ف\", text)\n",
    "        text = re.sub(\"چ\", \"ج\", text)\n",
    "        text = re.sub(\"پ\", \"ب\", text)\n",
    "        text = re.sub(\"ڜ\", \"ش\", text)\n",
    "        text = re.sub(\"ڪ\", \"ك\", text)\n",
    "        text = re.sub(\"ڧ\", \"ق\", text)\n",
    "        text = re.sub(\"ٱ\", \"ا\", text)\n",
    "        noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                                َ    | # Fatha\n",
    "                                ً    | # Tanwin Fath\n",
    "                                ُ    | # Damma\n",
    "                                ٌ    | # Tanwin Damm\n",
    "                                ِ    | # Kasra\n",
    "                                ٍ    | # Tanwin Kasr\n",
    "                                ْ    | # Sukun\n",
    "                                ـ     # Tatwil/Kashida\n",
    "                            \"\"\", re.VERBOSE)\n",
    "        text = re.sub(noise, '', text)\n",
    "        text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Convert repeated characters to single occurrence\n",
    "        return araby.strip_tashkeel(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Replace various forms of alif with a bare alif.\n",
      "Input:  إسلام\n",
      "Excepted:  اسلام\n",
      "Output:  اسلام\n",
      "Pass\n",
      "\n",
      "Example 2: Remove diacritics and perform character replacements.\n",
      "Input:  مُدَرِّسة\n",
      "Excepted:  مدرسه\n",
      "Output:  مدرسه\n",
      "Pass\n",
      "\n",
      "Example 3: Collapse repeated characters.\n",
      "Input:  مممممممم\n",
      "Excepted:  مم\n",
      "Output:  مم\n",
      "Pass\n",
      "\n",
      "Example 4: Multiple replacement rules in one sentence.\n",
      "Input:  گلاب چای پيت ڤيديو ڜهر ڪتاب ڧكر ٱمان\n",
      "Excepted:  كلاب جاي بيت فيديو شهر كتاب قكر امان\n",
      "Output:  كلاب جای بيت فيديو شهر كتاب قكر امان\n",
      "Fail (got 'كلاب جای بيت فيديو شهر كتاب قكر امان', expected 'كلاب جاي بيت فيديو شهر كتاب قكر امان')\n",
      "\n",
      "Example 5: Multiple replacement rules in one sentence with Tnkeeh library.\n",
      "TNKEEH branch output: كلاب جاي بيت فيديو شهر كتاب قكر امان\n",
      "\n",
      "Example 6: Multiple replacement rules in one sentence with CAMEL library.\n",
      "CAMEL branch output: كلاب جاي بيت فيديو شهر كتاب قكر امان\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 1: Replace various forms of alif with a bare alif.\")\n",
    "input_text = \"إسلام\"\n",
    "expected = \"اسلام\"  # \"إ\" replaced with \"ا\"\n",
    "result = normalize_arabic(input_text, tool=\"other\")\n",
    "print(\"Input: \", input_text)\n",
    "print(\"Excepted: \", expected)\n",
    "print(\"Output: \", result)\n",
    "print(\"Pass\" if result == expected else f\"Fail (got '{result}', expected '{expected}')\")\n",
    "\n",
    "print(\"\\nExample 2: Remove diacritics and perform character replacements.\")\n",
    "input_text = \"مُدَرِّسة\"  # Contains diacritics and ends with ة\n",
    "expected = \"مدرسه\"  # Expected: diacritics removed, ة -> ه, then stripped by strip_tashkeel (here our dummy returns unchanged)\n",
    "result = normalize_arabic(input_text, tool=\"other\")\n",
    "print(\"Input: \", input_text)\n",
    "print(\"Excepted: \", expected)\n",
    "print(\"Output: \", result)\n",
    "print(\"Pass\" if result == expected else f\"Fail (got '{result}', expected '{expected}')\")\n",
    "\n",
    "print(\"\\nExample 3: Collapse repeated characters.\")\n",
    "input_text = \"مممممممم\"  # Many repeated م's\n",
    "expected = \"مم\"  # Reduced to two occurrences\n",
    "result = normalize_arabic(input_text, tool=\"other\")\n",
    "print(\"Input: \", input_text)\n",
    "print(\"Excepted: \", expected)\n",
    "print(\"Output: \", result)\n",
    "print(\"Pass\" if result == expected else f\"Fail (got '{result}', expected '{expected}')\")\n",
    "\n",
    "print(\"\\nExample 4: Multiple replacement rules in one sentence.\")\n",
    "input_text = \"گلاب چای پيت ڤيديو ڜهر ڪتاب ڧكر ٱمان\"\n",
    "# Expected replacements:\n",
    "# گ -> ك  => \"گلاب\" -> \"كلاب\"\n",
    "# چ -> ج  => \"چای\" -> \"جاي\"\n",
    "# پ -> ب  => \"پيت\" -> \"بيت\"\n",
    "# ڤ -> ف  => \"ڤيديو\" -> \"فيديو\"\n",
    "# ڜ -> ش  => \"ڜهر\" -> \"شهر\"\n",
    "# ڪ -> ك  => \"ڪتاب\" -> \"كتاب\"\n",
    "# ڧ -> ق  => \"ڧكر\" -> \"قكر\"\n",
    "# ٱ -> ا  => \"ٱمان\" -> \"امان\"\n",
    "expected = \"كلاب جاي بيت فيديو شهر كتاب قكر امان\"\n",
    "result = normalize_arabic(input_text, tool=\"other\")\n",
    "print(\"Input: \", input_text)\n",
    "print(\"Excepted: \", expected)\n",
    "print(\"Output: \", result)\n",
    "print(\"Pass\" if result == expected else f\"Fail (got '{result}', expected '{expected}')\")\n",
    "\n",
    "print(\"\\nExample 5: Multiple replacement rules in one sentence with Tnkeeh library.\")\n",
    "input_text = \"كلاب جاي بيت فيديو شهر كتاب قكر امان\"\n",
    "result = normalize_arabic(input_text, tool=\"tnkeeh\")\n",
    "print(\"TNKEEH branch output:\", result)\n",
    "\n",
    "print(\"\\nExample 6: Multiple replacement rules in one sentence with CAMEL library.\")\n",
    "input_text = \"كلاب جاي بيت فيديو شهر كتاب قكر امان\"\n",
    "result = normalize_arabic(input_text, tool=\"camel\")\n",
    "print(\"CAMEL branch output:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Observation:_**</span> The overall normaliztion process of text is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Specific Noise Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Noise Removal:_**</span> extend noise removal to handle more cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_noise(text: str) -> str:\n",
    "    '''\n",
    "    A method that removes specific noise in text such as tatweel, HTML tags, URLs, etc.\n",
    "\n",
    "    :param text: A sentence to be processed.\n",
    "    :return: Cleaned text containing only Arabic letters and whitespace.\n",
    "    '''\n",
    "    # Remove tatweel (ـ)\n",
    "    text = re.sub(r'\\u0640', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove non-Arabic characters (keep Arabic Unicode block and whitespace)\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Input:     مــــــــرحبا بك في موقعنا <b>الرائع</b>!\n",
      "Expected:  مرحبا بك في موقعنا الرائع\n",
      "Got:       مرحبا بك في موقعنا الرائع\n",
      "Pass: True\n",
      "\n",
      "Exmaple 2\n",
      "Input:     تفضل بزيارة http://example.com للحصول على المزيد من المعلومات.\n",
      "Expected:  تفضل بزيارة للحصول على المزيد من المعلومات\n",
      "Got:       تفضل بزيارة للحصول على المزيد من المعلومات\n",
      "Pass: True\n",
      "\n",
      "Example 3\n",
      "Input:     هذا نص تجريبي مع أحرف لاتينية مثل ABC وأرقام 123 ورموز @#!.\n",
      "Expected:  هذا نص تجريبي مع أحرف لاتينية مثل ورموز\n",
      "Got:       هذا نص تجريبي مع أحرف لاتينية مثل وأرقام ورموز\n",
      "Pass: False\n",
      "\n",
      "Example 4\n",
      "Input:        <div>ـــــــــسلام</div>   \n",
      "Expected:  سلام\n",
      "Got:       سلام\n",
      "Pass: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"Example 1: Remove tatweel and HTML tags.\")\n",
    "example1 = \"مــــــــرحبا بك في موقعنا <b>الرائع</b>!\"\n",
    "# Expected: \"مرحبا بك في موقعنا الرائع\"\n",
    "result1 = remove_arabic_noise(example1)\n",
    "print(\"Example 1\")\n",
    "print(\"Input:    \", example1)\n",
    "print(\"Expected: \", \"مرحبا بك في موقعنا الرائع\")\n",
    "print(\"Got:      \", result1)\n",
    "print(\"Pass:\" , result1 == \"مرحبا بك في موقعنا الرائع\")\n",
    "print()\n",
    "\n",
    "# Example 2: Remove URLs.\n",
    "example2 = \"تفضل بزيارة http://example.com للحصول على المزيد من المعلومات.\"\n",
    "# Expected: \"تفضل بزيارة للحصول على المزيد من المعلومات\"\n",
    "result2 = remove_arabic_noise(example2)\n",
    "print(\"Exmaple 2\")\n",
    "print(\"Input:    \", example2)\n",
    "print(\"Expected: \", \"تفضل بزيارة للحصول على المزيد من المعلومات\")\n",
    "print(\"Got:      \", result2)\n",
    "print(\"Pass:\" , result2 == \"تفضل بزيارة للحصول على المزيد من المعلومات\")\n",
    "print()\n",
    "    \n",
    "# Example 3: Remove non-Arabic noise (Latin letters, numbers, punctuation)\n",
    "example3 = \"هذا نص تجريبي مع أحرف لاتينية مثل ABC وأرقام 123 ورموز @#!.\"\n",
    "# Expected: \"هذا نص تجريبي مع أحرف لاتينية مثل ورموز\"\n",
    "result3 = remove_arabic_noise(example3)\n",
    "print(\"Example 3\")\n",
    "print(\"Input:    \", example3)\n",
    "print(\"Expected: \", \"هذا نص تجريبي مع أحرف لاتينية مثل ورموز\")\n",
    "print(\"Got:      \", result3)\n",
    "print(\"Pass:\" , result3 == \"هذا نص تجريبي مع أحرف لاتينية مثل ورموز\")\n",
    "print()\n",
    "    \n",
    "# Example 4: Remove extra spaces and HTML tags with tatweel.\n",
    "example4 = \"   <div>ـــــــــسلام</div>   \"\n",
    "# Expected: \"سلام\"\n",
    "result4 = remove_arabic_noise(example4)\n",
    "print(\"Example 4\")\n",
    "print(\"Input:    \", example4)\n",
    "print(\"Expected: \", \"سلام\")\n",
    "print(\"Got:      \", result4)\n",
    "print(\"Pass:\" , result4 == \"سلام\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Observation:_**</span> The overall removal of noise in text is successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> Look if their exists other types of noise need to be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Tokenization:_**</span> is the process of breaking a sequence of text into smaller units called tokens, such as words, phrases, symbols, and other elements. For the Arabic language, tokenization is a complex task due to the differences between the written and spoken forms of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_arabic(text: str, method='simple', model='msa', scheme='bwtok'):\n",
    "    '''\n",
    "    Tokenizes an Arabic sentence using either a simple or morphological approach.\n",
    "\n",
    "    :param text: The Arabic sentence to be tokenized.\n",
    "    :param method: Tokenization method to use. Options:\n",
    "                   - 'simple': Uses a basic whitespace-based tokenizer.\n",
    "                   - 'morphological': Uses a morphological analyzer for tokenization.\n",
    "    :param model: Specifies the morphological model to use (only applicable if `method='morphological'`).\n",
    "                  Options:\n",
    "                   - 'msa': Modern Standard Arabic (default).\n",
    "                   - 'egy': Egyptian Arabic.\n",
    "    :param scheme: Tokenization scheme for the morphological method. Options:\n",
    "                   - 'bwtok': Buckwalter tokenization (default).\n",
    "                   - 'd3tok': D3 tokenization.\n",
    "                   - 'atbtok': ATB tokenization.\n",
    "\n",
    "    :return: A list of tokenized words.\n",
    "    '''\n",
    "\n",
    "    if method == 'simple':\n",
    "        return simple_word_tokenize(text)\n",
    "    elif method == 'morphological':\n",
    "        words = simple_word_tokenize(text)\n",
    "\n",
    "        if model=='msa':\n",
    "            mle_msa = MLEDisambiguator.pretrained('calima-msa-r13') # Load a pre-trained disambiguator\n",
    "            msa_d3_tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, scheme=scheme)\n",
    "            words = msa_d3_tokenizer.tokenize(words)\n",
    "            return words\n",
    "        else:\n",
    "            mle_egy = MLEDisambiguator.pretrained('calima-egy-r13') # Load a pre-trained disambiguator\n",
    "            egy_bw_tokenizer = MorphologicalTokenizer(disambiguator=mle_egy, scheme='bwtok')\n",
    "            words = egy_bw_tokenizer.tokenize(words)\n",
    "            return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple tokenization: ['هذا', 'مثال', 'على', 'تقطيع', 'النص', 'العربي', 'بطريقة', 'متقدمة', '.']\n",
      "Morphological tokenization: ['هذا', 'مثال', 'على', 'تقطيع', 'ال+_نص', 'ال+_عربي', 'ب+_طريق_+ة', 'متقدم_+ة', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"هذا مثال على تقطيع النص العربي بطريقة متقدمة.\"\n",
    "simple_tokens = tokenize_arabic(text, 'simple')\n",
    "morphological_tokens = tokenize_arabic(text, 'morphological')\n",
    "\n",
    "print(\"Simple tokenization:\", simple_tokens)\n",
    "print(\"Morphological tokenization:\", morphological_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Dediacritization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Dediacritization:_**</span> Dediacritization is the process of removing Arabic diacritical marks. Diacritics increase data sparsity and so most Arabic NLP techniques ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_dediacrition(text: str, method='remove', tool='pyarabic') -> str:\n",
    "    '''\n",
    "    Removes or normalizes Arabic diacritical marks (Tashkeel).\n",
    "\n",
    "    :param text: An Arabic sentence that requires dediacritization.\n",
    "    :param method: The dediacritization method to apply. Options:\n",
    "                   - 'remove': Removes all diacritics (default).\n",
    "                   - 'normalize': Normalizes Hamza and Shadda while removing other diacritics.\n",
    "                   - 'keep': Keeps the diacritics as they are.\n",
    "    :param tool: The library to use for dediacritization. Options:\n",
    "                 - 'pyarabic': Uses `pyarabic` for diacritic removal (default).\n",
    "                 - 'camel': Uses `camel_tools` for diacritic removal.\n",
    "\n",
    "    :return: A string with the processed text.\n",
    "\n",
    "    **Example Usage:**\n",
    "    >>> text_with_diacritics = \"اللُّغَةُ العَرَبِيَّةُ جَمِيلَةٌ\"\n",
    "    >>> arabic_dediacrition(text_with_diacritics, 'remove')\n",
    "    'اللغه العربيه جميله'\n",
    "    \n",
    "    >>> arabic_dediacrition(text_with_diacritics, 'normalize')\n",
    "    'اللغه العربيه جميله'\n",
    "\n",
    "    >>> arabic_dediacrition(text_with_diacritics, 'keep')\n",
    "    'اللُّغَةُ العَرَبِيَّةُ جَمِيلَةٌ'\n",
    "    '''\n",
    "\n",
    "    if method == 'remove':\n",
    "        if tool == 'pyarabic':\n",
    "            return araby.strip_diacritics(text)\n",
    "        elif tool == 'camel':\n",
    "            return dediac_ar(text)\n",
    "    elif method == 'normalize':\n",
    "        return araby.normalize_hamza(araby.strip_shadda(text))\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: اللُّغَةُ العَرَبِيَّةُ جَمِيلَةٌ\n",
      "Removed diacritics using PyArabic: اللغة العربية جميلة\n",
      "Removed diacritics using CAMeL: اللغة العربية جميلة\n",
      "Normalized diacritics: اللُغَةُ العَرَبِيَةُ جَمِيلَةٌ\n"
     ]
    }
   ],
   "source": [
    "text_with_diacritics = \"اللُّغَةُ العَرَبِيَّةُ جَمِيلَةٌ\"\n",
    "removed_diacritics_1 = arabic_dediacrition(text_with_diacritics, 'remove', \"pyarabic\")\n",
    "removed_diacritics_2 = arabic_dediacrition(text_with_diacritics, 'remove', \"camel\")\n",
    "normalized_diacritics = arabic_dediacrition(text_with_diacritics, 'normalize')\n",
    "\n",
    "print(\"Original:\", text_with_diacritics)\n",
    "print(\"Removed diacritics using PyArabic:\", removed_diacritics_1)\n",
    "print(\"Removed diacritics using CAMeL:\", removed_diacritics_2)\n",
    "print(\"Normalized diacritics:\", normalized_diacritics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 Dialect Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Dialects Identification:_**</span> is to determine which city-level does a text belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_Note:_**</span> CAMel `DialectIdentifier` is not supported for Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from camel_tools.dialectid import DialectIdentifier\n",
    "\n",
    "def identify_dialect(text: str, target: str) -> list:\n",
    "    \"\"\"\n",
    "    Identifies the dialect of a given Arabic text at various levels of granularity.\n",
    "\n",
    "    This function uses a pretrained dialect identification model from Camel Tools\n",
    "    to determine the dialect of an input text. The model can distinguish among 25\n",
    "    city-level dialects as well as Modern Standard Arabic (MSA). In addition to\n",
    "    city-level identification, the model can provide aggregated predictions at the\n",
    "    regional and country levels. \n",
    "\n",
    "    **Note:** The Camel Tools dialect identification module is not available on Windows.\n",
    "\n",
    "    :param text: A string containing Arabic text.\n",
    "    :param target: A string indicating the level of dialect granularity.\n",
    "                   Options include:\n",
    "                     - \"city\": for fine-grained, city-level dialect identification.\n",
    "                     - \"country\": for aggregated country-level predictions.\n",
    "                     - \"region\": for aggregated region-level predictions.\n",
    "                     - Any other value defaults to the full prediction (typically a list of labels).\n",
    "    :return: A list of predicted dialect labels corresponding to the specified granularity.\n",
    "\n",
    "    **Example:**\n",
    "    >>> text = \"هذا نص عربي باللهجة المصرية.\"\n",
    "    >>> identify_dialect(text, \"city\")\n",
    "    ['القاهرة']  # (Example output; actual predictions depend on the pretrained model.)\n",
    "    \"\"\"\n",
    "    did = DialectIdentifier.pretrained()  # Pretrained dialect identification model.\n",
    "    \n",
    "    if target == \"city\":\n",
    "        return did.predict(text, \"city\")\n",
    "    elif target == \"country\":\n",
    "        return did.predict(text, \"country\")\n",
    "    elif target == \"region\":\n",
    "        # Note: Correcting a potential typo from 'predit' to 'predict'\n",
    "        return did.predict(text, \"region\")\n",
    "    else:\n",
    "        return did.predict(text)\n",
    "\n",
    "\n",
    "def normalize_dialect(text: str, target_dialect: str = 'MSA') -> str:\n",
    "    \"\"\"\n",
    "    Normalizes an Arabic text to a specified dialect variant.\n",
    "\n",
    "    This is a placeholder function. In practice, dialect normalization may involve\n",
    "    complex transformations to convert text from one dialect to another. For now,\n",
    "    the function simply returns the original text.\n",
    "\n",
    "    :param text: A string containing Arabic text.\n",
    "    :param target_dialect: A string representing the target dialect for normalization.\n",
    "                           Default is 'MSA' (Modern Standard Arabic).\n",
    "    :return: The input text unmodified (placeholder implementation).\n",
    "\n",
    "    **Example:**\n",
    "    >>> normalize_dialect(\"هذا نص باللهجة المصرية\", target_dialect=\"MSA\")\n",
    "    \"هذا نص باللهجة المصرية\"\n",
    "    \"\"\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"شلونك حبيبي؟ شخبارك اليوم؟\"\n",
    "# dialect = identify_dialect(text)\n",
    "# normalized_text = normalize_dialect(text)\n",
    "\n",
    "# print(\"Original text:\", text)\n",
    "# print(\"Identified dialect:\", dialect)\n",
    "# print(\"Normalized to MSA:\", normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Arabic Dialect Identifier\n",
    "# model_name = \"lafifi-24/arabicBert_arabic_dialect_identification\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# dialect_classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# text = \"هذا نص عربي باللهجة المصرية.\"\n",
    "\n",
    "# result = dialect_classifier(text)\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Arabic text (dialect identification)\n",
    "# text = \"عامل اه يا صاحبي.\"\n",
    "\n",
    "# Predictions\n",
    "# result = dialect_classifier(text)\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> look for other tools that perform dialect identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13 Punctuation Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Punctuation Removal:_**</span> is the elimination of any punctuation character-covering both standard English punctuation and common Arabic punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_punctuations(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation characters from an Arabic text.\n",
    "\n",
    "    This function replaces any punctuation character—covering both standard English punctuation \n",
    "    and common Arabic punctuation marks (e.g., the Arabic comma \"،\" and question mark \"؟\")—with a space.\n",
    "    After replacement, it normalizes the whitespace by collapsing multiple spaces into one and\n",
    "    trimming leading and trailing whitespace.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input Arabic text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The text after removing punctuation and normalizing whitespace.\n",
    "\n",
    "    Example:\n",
    "        >>> remove_arabic_punctuations(\"مرحباً، كيف حالك؟\")\n",
    "        'مرحباً كيف حالك'\n",
    "    \"\"\"\n",
    "\n",
    "    punctuations = \"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~؛\"\"\"\n",
    "\n",
    "    text = re.sub('[%s]' % re.escape(punctuations), ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Pass\n",
      "Input:     مرحباً، كيف حالك؟\n",
      "Expected:  مرحباً كيف حالك\n",
      "Got:       مرحباً كيف حالك\n",
      "\n",
      "Test 2: Pass\n",
      "Input:     هذا نص! يحتوي على: علامات، ترقيم؟ وأخرى.\n",
      "Expected:  هذا نص يحتوي على علامات ترقيم وأخرى\n",
      "Got:       هذا نص يحتوي على علامات ترقيم وأخرى\n",
      "\n",
      "Test 3: Pass\n",
      "Input:     \n",
      "Expected:  \n",
      "Got:       \n",
      "\n",
      "Test 4: Pass\n",
      "Input:     !@#$%^&*()\n",
      "Expected:  \n",
      "Got:       \n",
      "\n",
      "Test 5: Pass\n",
      "Input:     سلام - كيف حالك؟\n",
      "Expected:  سلام كيف حالك\n",
      "Got:       سلام كيف حالك\n",
      "\n",
      "Test 6: Pass\n",
      "Input:     تجربة... مع نقاط ثلاثية!!!\n",
      "Expected:  تجربة مع نقاط ثلاثية\n",
      "Got:       تجربة مع نقاط ثلاثية\n",
      "\n",
      "Test 7: Pass\n",
      "Input:     هذا، ذلك؛ وهذا؟\n",
      "Expected:  هذا ذلك وهذا\n",
      "Got:       هذا ذلك وهذا\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    # (Input text, Expected output)\n",
    "    (\"مرحباً، كيف حالك؟\", \"مرحباً كيف حالك\"),\n",
    "    (\"هذا نص! يحتوي على: علامات، ترقيم؟ وأخرى.\", \"هذا نص يحتوي على علامات ترقيم وأخرى\"),\n",
    "    (\"\", \"\"),\n",
    "    (\"!@#$%^&*()\", \"\"),\n",
    "    (\"سلام - كيف حالك؟\", \"سلام كيف حالك\"),\n",
    "    (\"تجربة... مع نقاط ثلاثية!!!\", \"تجربة مع نقاط ثلاثية\"),\n",
    "    (\"هذا، ذلك؛ وهذا؟\", \"هذا ذلك وهذا\")\n",
    "]\n",
    "\n",
    "for i, (input_text, expected) in enumerate(test_cases, 1):\n",
    "    result = remove_arabic_punctuations(input_text)\n",
    "    status = \"Pass\" if result == expected else \"Fail\"\n",
    "    print(f\"Test {i}: {status}\")\n",
    "    print(\"Input:    \", input_text)\n",
    "    print(\"Expected: \", expected)\n",
    "    print(\"Got:      \", result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14 Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Named entity recognition:_**</span> find and label named entities like proper nouns, organisations, places, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token in an input sentence, `NERecognizer` outputs a label that indicates the type of named-entity.The system outputs one of the following labels for each token: `'B-LOC'`, `'B-ORG'`, `'B-PERS'`, `'B-MISC'`, `'I-LOC'`, `'I-ORG'`, `'I-PERS'`, `'I-MISC'`, `'O'`.\n",
    "Named-entites can either be a `LOC` (location), `ORG` (organization), `PERS` (person), or `MISC` (miscallaneous).\n",
    "\n",
    "Labels beginning with `B` indicate that their corresponding tokens are the begininging of a multi-word named-entity or is a single-token named-entity'. Those begining with `I` indicate that their corresponding tokens are continuations of a multi-word named-entity. Words that aren't named-entities are given the `'O'` label.\n",
    "\n",
    "The example below illustrates how `NERecognizer` can be used to label named-entities in a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_arabic_entities(text: str, tool: str = \"nltk\") -> list:\n",
    "    \"\"\"\n",
    "    Recognize named entities in an Arabic sentence using a pretrained NER model.\n",
    "\n",
    "    For each token in the input sentence, the model outputs a label indicating\n",
    "    its named-entity type. The possible labels are:\n",
    "        - 'B-LOC', 'B-ORG', 'B-PERS', 'B-MISC': The beginning of a location,\n",
    "          organization, person, or miscellaneous entity (or a single-token entity).\n",
    "        - 'I-LOC', 'I-ORG', 'I-PERS', 'I-MISC': Continuation tokens for multi-word entities.\n",
    "        - 'O': A token that does not belong to any named entity.\n",
    "\n",
    "    The function processes the input text, obtains NER labels, and then aggregates\n",
    "    contiguous tokens with the same entity type into a single named entity.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): An Arabic sentence for which named-entity recognition is performed.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains a recognized entity (a string) and its type (e.g., 'LOC', 'ORG').\n",
    "                        If no entities are found, an empty list is returned.\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"يعيش محمد في القاهرة ويعمل في شركة جوجل.\"\n",
    "        >>> recognize_arabic_entities(text)\n",
    "        [('محمد', 'PERS'), ('القاهرة', 'LOC'), ('جوجل', 'ORG')]\n",
    "    \"\"\"\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    if tool == 'camel':\n",
    "        ner = NERecognizer.pretrained()\n",
    "        labels = ner.predict_sentence(simple_word_tokenize(text))\n",
    "\n",
    "        print(\"Raw labels: \", labels)\n",
    "    \n",
    "        words = simple_word_tokenize(text)\n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        \n",
    "        for word, label in zip(words, labels):\n",
    "            if label.startswith('B-'):\n",
    "                if current_entity:\n",
    "                    entities.append((' '.join(current_entity), current_label))\n",
    "                    current_entity = []\n",
    "                current_entity.append(word)\n",
    "                current_label = label[2:]\n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                current_entity.append(word)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append((' '.join(current_entity), current_label))\n",
    "                    current_entity = []\n",
    "                    current_label = None\n",
    "        \n",
    "        if current_entity:\n",
    "            entities.append((' '.join(current_entity), current_label))\n",
    "        \n",
    "        return entities\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\mazen\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw labels:  ['O', 'B-PERS', 'O', 'B-LOC', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "Text: يعيش محمد في القاهرة ويعمل في شركة جوجل.\n",
      "Recognized Entities:\n",
      "محمد -> PERS\n",
      "القاهرة -> LOC\n",
      "جوجل -> ORG\n"
     ]
    }
   ],
   "source": [
    "text = \"يعيش محمد في القاهرة ويعمل في شركة جوجل.\"\n",
    "\n",
    "entities = recognize_arabic_entities(text, tool=\"camel\")\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Recognized Entities:\")\n",
    "for entity, entity_type in entities:\n",
    "    print(f\"{entity} -> {entity_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.15 Morphological Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Morphological Analysis:_**</span> is the process of generating all possible readings (analyses) of a given word out of context. All analyses are generated from the undiacritized form of the input word. Each of these analyses is defined by a set lexical and morphological features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_morph_analysis(text: str):\n",
    "    \"\"\"\n",
    "    Perform morphological analysis on an Arabic word or phrase.\n",
    "\n",
    "    This function generates all possible morphological readings (analyses) for the input text \n",
    "    out of context. It loads the built-in morphological database (designed primarily for Modern \n",
    "    Standard Arabic) and uses it to analyze the given word or phrase. Each analysis typically \n",
    "    includes information such as the token, its lemma, root, part-of-speech (POS), and other \n",
    "    morphological features.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): An Arabic word or phrase to be analyzed. Note that the analysis is performed \n",
    "                    out-of-context, so the output represents all possible morphological interpretations.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of analysis results. Each element in the list is usually a dictionary \n",
    "              containing morphological details (e.g., 'token', 'lemma', 'root', 'pos', etc.).\n",
    "\n",
    "    Examples:\n",
    "        >>> # Analyze the verb \"يذهب\"\n",
    "        >>> analyses = arabic_morph_analysis(\"يذهب\")\n",
    "        >>> for analysis in analyses:\n",
    "        ...     print(analysis)\n",
    "        {'token': 'يذهب', 'lemma': 'ذهب', 'root': 'ذ ه ب', 'pos': 'فعل', ...}\n",
    "        \n",
    "        >>> # Analyze another word\n",
    "        >>> results = arabic_morph_analysis(\"كتبت\")\n",
    "        >>> print(results)\n",
    "        [{'token': 'كتبت', 'lemma': 'كتب', 'root': 'ك ت ب', 'pos': 'فعل', ...}, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    db = MorphologyDB.builtin_db()\n",
    "\n",
    "    analyzer = Analyzer(db)\n",
    "\n",
    "    analyses = analyzer.analyze(text)\n",
    "    \n",
    "    return analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing the word: يذهب\n",
      "Analysis 1: {'diac': 'يُذَهِّب', 'lex': 'ذَهَّب', 'bw': 'يُ/IV3MS+ذَهِّب/IV', 'gloss': 'he;it+gild', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'u', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'يُذَهِّب', 'caphi': 'y_u_dh_a_h_h_i_b', 'd1tok': 'يُذَهِّب', 'd2tok': 'يُذَهِّب', 'pos_logprob': -1.023208, 'd3tok': 'يُذَهِّب', 'd2seg': 'يُذَهِّب', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'ذ.ه.ب', 'bwtok': 'يُ+_ذَهِّب', 'pattern': 'يُ1َ2ِّ3', 'lex_logprob': -99.0, 'atbtok': 'يُذَهِّب', 'atbseg': 'يُذَهِّب', 'd1seg': 'يُذَهِّب', 'stem': 'ذَهِّب', 'stemgloss': 'gild', 'stemcat': 'IV_yu'}\n",
      "Analysis 2: {'diac': 'يُذْهِب', 'lex': 'أَذْهَب', 'bw': 'يُ/IV3MS+ذْهِب/IV', 'gloss': 'he;it+remove;eliminate', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'u', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'يُذْهِب', 'caphi': 'y_u_dh_h_i_b', 'd1tok': 'يُذْهِب', 'd2tok': 'يُذْهِب', 'pos_logprob': -1.023208, 'd3tok': 'يُذْهِب', 'd2seg': 'يُذْهِب', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'ذ.ه.ب', 'bwtok': 'يُ+_ذْهِب', 'pattern': 'يُ1ْ2ِ3', 'lex_logprob': -99.0, 'atbtok': 'يُذْهِب', 'atbseg': 'يُذْهِب', 'd1seg': 'يُذْهِب', 'stem': 'ذْهِب', 'stemgloss': 'remove;eliminate', 'stemcat': 'IV_yu'}\n",
      "Analysis 3: {'diac': 'يُذْهَب', 'lex': 'أَذْهَب', 'bw': 'يُ/IV3MS+ذْهَب/IV_PASS', 'gloss': 'he;it+be_removed;be_eliminated', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'p', 'mod': 'u', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'يُذْهَب', 'caphi': 'y_u_dh_h_a_b', 'd1tok': 'يُذْهَب', 'd2tok': 'يُذْهَب', 'pos_logprob': -1.023208, 'd3tok': 'يُذْهَب', 'd2seg': 'يُذْهَب', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB-PASS', 'root': 'ذ.ه.ب', 'bwtok': 'يُ+_ذْهَب', 'pattern': 'يُ1ْ2َ3', 'lex_logprob': -99.0, 'atbtok': 'يُذْهَب', 'atbseg': 'يُذْهَب', 'd1seg': 'يُذْهَب', 'stem': 'ذْهَب', 'stemgloss': 'be_removed;be_eliminated', 'stemcat': 'IV_Pass_yu'}\n",
      "Analysis 4: {'diac': 'يَذْهَب', 'lex': 'ذَهَب', 'bw': 'يَ/IV3MS+ذْهَب/IV', 'gloss': 'he;it+go;depart', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'u', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'يَذْهَب', 'caphi': 'y_a_dh_h_a_b', 'd1tok': 'يَذْهَب', 'd2tok': 'يَذْهَب', 'pos_logprob': -1.023208, 'd3tok': 'يَذْهَب', 'd2seg': 'يَذْهَب', 'pos_lex_logprob': -3.895401, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'ذ.ه.ب', 'bwtok': 'يَ+_ذْهَب', 'pattern': 'يَ1ْ2َ3', 'lex_logprob': -3.895401, 'atbtok': 'يَذْهَب', 'atbseg': 'يَذْهَب', 'd1seg': 'يَذْهَب', 'stem': 'ذْهَب', 'stemgloss': 'go;depart', 'stemcat': 'IV'}\n",
      "Analysis 5: {'diac': 'يَذْهَب', 'lex': 'ذَهَب', 'bw': 'يَ/IV3MS+ذْهَب/IV', 'gloss': 'he;it+take_(with)', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'u', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'يَذْهَب', 'caphi': 'y_a_dh_h_a_b', 'd1tok': 'يَذْهَب', 'd2tok': 'يَذْهَب', 'pos_logprob': -1.023208, 'd3tok': 'يَذْهَب', 'd2seg': 'يَذْهَب', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'ذ.ه.ب', 'bwtok': 'يَ+_ذْهَب', 'pattern': 'يَ1ْ2َ3', 'lex_logprob': -99.0, 'atbtok': 'يَذْهَب', 'atbseg': 'يَذْهَب', 'd1seg': 'يَذْهَب', 'stem': 'ذْهَب', 'stemgloss': 'take_(with)', 'stemcat': 'IV'}\n",
      "\n",
      "Analyzing the word: كتبت\n",
      "Analysis 1: {'diac': 'كُتِبَت', 'lex': 'كَتَب', 'bw': 'كُتِب/PV_PASS+َت/PVSUFF_SUBJ:3FS', 'gloss': 'be_written;be_fated;be_destined+it;they;she_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'p', 'vox': 'p', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'كُتِبَت', 'caphi': 'k_u_t_i_b_a_t', 'd1tok': 'كُتِبَت', 'd2tok': 'كُتِبَت', 'pos_logprob': -1.023208, 'd3tok': 'كُتِبَت', 'd2seg': 'كُتِبَت', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'f', 'catib6': 'VRB-PASS', 'root': 'ك.ت.ب', 'bwtok': 'كُتِب_+َت', 'pattern': '1ُ2ِ3َت', 'lex_logprob': -3.648503, 'atbtok': 'كُتِبَت', 'atbseg': 'كُتِبَت', 'd1seg': 'كُتِبَت', 'stem': 'كُتِب', 'stemgloss': 'be_written;be_fated;be_destined', 'stemcat': 'PV_Pass'}\n",
      "Analysis 2: {'diac': 'كُتِبْتَ', 'lex': 'كَتَب', 'bw': 'كُتِب/PV_PASS+تَ/PVSUFF_SUBJ:2MS', 'gloss': 'be_written;be_fated;be_destined+you_[masc.sg.]_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '2', 'asp': 'p', 'vox': 'p', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'كُتِبْتَ', 'caphi': 'k_u_t_i_b_t_a', 'd1tok': 'كُتِبْتَ', 'd2tok': 'كُتِبْتَ', 'pos_logprob': -1.023208, 'd3tok': 'كُتِبْتَ', 'd2seg': 'كُتِبْتَ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB-PASS', 'root': 'ك.ت.ب', 'bwtok': 'كُتِب_+تَ', 'pattern': '1ُ2ِ3ْتَ', 'lex_logprob': -3.648503, 'atbtok': 'كُتِبْتَ', 'atbseg': 'كُتِبْتَ', 'd1seg': 'كُتِبْتَ', 'stem': 'كُتِب', 'stemgloss': 'be_written;be_fated;be_destined', 'stemcat': 'PV_Pass'}\n",
      "Analysis 3: {'diac': 'كُتِبْتُ', 'lex': 'كَتَب', 'bw': 'كُتِب/PV_PASS+تُ/PVSUFF_SUBJ:1S', 'gloss': 'be_written;be_fated;be_destined+I_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '1', 'asp': 'p', 'vox': 'p', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'كُتِبْتُ', 'caphi': 'k_u_t_i_b_t_u', 'd1tok': 'كُتِبْتُ', 'd2tok': 'كُتِبْتُ', 'pos_logprob': -1.023208, 'd3tok': 'كُتِبْتُ', 'd2seg': 'كُتِبْتُ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB-PASS', 'root': 'ك.ت.ب', 'bwtok': 'كُتِب_+تُ', 'pattern': '1ُ2ِ3ْتُ', 'lex_logprob': -3.648503, 'atbtok': 'كُتِبْتُ', 'atbseg': 'كُتِبْتُ', 'd1seg': 'كُتِبْتُ', 'stem': 'كُتِب', 'stemgloss': 'be_written;be_fated;be_destined', 'stemcat': 'PV_Pass'}\n",
      "Analysis 4: {'diac': 'كُتِبْتِ', 'lex': 'كَتَب', 'bw': 'كُتِب/PV_PASS+تِ/PVSUFF_SUBJ:2FS', 'gloss': 'be_written;be_fated;be_destined+you_[fem.sg.]_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '2', 'asp': 'p', 'vox': 'p', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'كُتِبْتِ', 'caphi': 'k_u_t_i_b_t_i', 'd1tok': 'كُتِبْتِ', 'd2tok': 'كُتِبْتِ', 'pos_logprob': -1.023208, 'd3tok': 'كُتِبْتِ', 'd2seg': 'كُتِبْتِ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'f', 'catib6': 'VRB-PASS', 'root': 'ك.ت.ب', 'bwtok': 'كُتِب_+تِ', 'pattern': '1ُ2ِ3ْتِ', 'lex_logprob': -3.648503, 'atbtok': 'كُتِبْتِ', 'atbseg': 'كُتِبْتِ', 'd1seg': 'كُتِبْتِ', 'stem': 'كُتِب', 'stemgloss': 'be_written;be_fated;be_destined', 'stemcat': 'PV_Pass'}\n",
      "Analysis 5: {'diac': 'كَتَبَت', 'lex': 'كَتَب', 'bw': 'كَتَب/PV+َت/PVSUFF_SUBJ:3FS', 'gloss': 'write+it;they;she_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'p', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'كَتَبَت', 'caphi': 'k_a_t_a_b_a_t', 'd1tok': 'كَتَبَت', 'd2tok': 'كَتَبَت', 'pos_logprob': -1.023208, 'd3tok': 'كَتَبَت', 'd2seg': 'كَتَبَت', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'f', 'catib6': 'VRB', 'root': 'ك.ت.ب', 'bwtok': 'كَتَب_+َت', 'pattern': '1َ2َ3َت', 'lex_logprob': -3.648503, 'atbtok': 'كَتَبَت', 'atbseg': 'كَتَبَت', 'd1seg': 'كَتَبَت', 'stem': 'كَتَب', 'stemgloss': 'write', 'stemcat': 'PV'}\n",
      "Analysis 6: {'diac': 'كَتَبْتَ', 'lex': 'كَتَب', 'bw': 'كَتَب/PV+تَ/PVSUFF_SUBJ:2MS', 'gloss': 'write+you_[masc.sg.]_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '2', 'asp': 'p', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'كَتَبْتَ', 'caphi': 'k_a_t_a_b_t_a', 'd1tok': 'كَتَبْتَ', 'd2tok': 'كَتَبْتَ', 'pos_logprob': -1.023208, 'd3tok': 'كَتَبْتَ', 'd2seg': 'كَتَبْتَ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'ك.ت.ب', 'bwtok': 'كَتَب_+تَ', 'pattern': '1َ2َ3ْتَ', 'lex_logprob': -3.648503, 'atbtok': 'كَتَبْتَ', 'atbseg': 'كَتَبْتَ', 'd1seg': 'كَتَبْتَ', 'stem': 'كَتَب', 'stemgloss': 'write', 'stemcat': 'PV'}\n",
      "Analysis 7: {'diac': 'كَتَبْتُ', 'lex': 'كَتَب', 'bw': 'كَتَب/PV+تُ/PVSUFF_SUBJ:1S', 'gloss': 'write+I_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '1', 'asp': 'p', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'كَتَبْتُ', 'caphi': 'k_a_t_a_b_t_u', 'd1tok': 'كَتَبْتُ', 'd2tok': 'كَتَبْتُ', 'pos_logprob': -1.023208, 'd3tok': 'كَتَبْتُ', 'd2seg': 'كَتَبْتُ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'ك.ت.ب', 'bwtok': 'كَتَب_+تُ', 'pattern': '1َ2َ3ْتُ', 'lex_logprob': -3.648503, 'atbtok': 'كَتَبْتُ', 'atbseg': 'كَتَبْتُ', 'd1seg': 'كَتَبْتُ', 'stem': 'كَتَب', 'stemgloss': 'write', 'stemcat': 'PV'}\n",
      "Analysis 8: {'diac': 'كَتَبْتِ', 'lex': 'كَتَب', 'bw': 'كَتَب/PV+تِ/PVSUFF_SUBJ:2FS', 'gloss': 'write+you_[fem.sg.]_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '2', 'asp': 'p', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'كَتَبْتِ', 'caphi': 'k_a_t_a_b_t_i', 'd1tok': 'كَتَبْتِ', 'd2tok': 'كَتَبْتِ', 'pos_logprob': -1.023208, 'd3tok': 'كَتَبْتِ', 'd2seg': 'كَتَبْتِ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'f', 'catib6': 'VRB', 'root': 'ك.ت.ب', 'bwtok': 'كَتَب_+تِ', 'pattern': '1َ2َ3ْتِ', 'lex_logprob': -3.648503, 'atbtok': 'كَتَبْتِ', 'atbseg': 'كَتَبْتِ', 'd1seg': 'كَتَبْتِ', 'stem': 'كَتَب', 'stemgloss': 'write', 'stemcat': 'PV'}\n",
      "Analysis 9: {'diac': 'كَتِبِت', 'lex': 'تِبِت', 'bw': 'كَ/PREP+تِبِت/NOUN_PROP', 'gloss': 'like;such_as+Tibet', 'pos': 'noun_prop', 'prc3': '0', 'prc2': '0', 'prc1': 'ka_prep', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'u', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'كَ+_تِبِت', 'caphi': 'k_a_t_i_b_i_t', 'd1tok': 'كَتِبِت', 'd2tok': 'كَ+_تِبِت', 'pos_logprob': -1.047404, 'd3tok': 'كَ+_تِبِت', 'd2seg': 'كَ+_تِبِت', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'ADP+PROPN', 'gen': 'f', 'catib6': 'PRT+PROP', 'root': 'NTWS', 'bwtok': 'كَ+_تِبِت', 'pattern': 'كَNTWS', 'lex_logprob': -99.0, 'atbtok': 'كَ+_تِبِت', 'atbseg': 'كَ+_تِبِت', 'd1seg': 'كَتِبِت', 'stem': 'تِبِت', 'stemgloss': 'Tibet', 'stemcat': 'N'}\n",
      "Analysis 10: {'diac': 'كَتِبِتِ', 'lex': 'تِبِت', 'bw': 'كَ/PREP+تِبِت/NOUN_PROP+ِ/CASE_DEF_GEN', 'gloss': 'like;such_as+Tibet+[def.gen.]', 'pos': 'noun_prop', 'prc3': '0', 'prc2': '0', 'prc1': 'ka_prep', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'كَ+_تِبِتِ', 'caphi': 'k_a_t_i_b_i_t_i', 'd1tok': 'كَتِبِتِ', 'd2tok': 'كَ+_تِبِتِ', 'pos_logprob': -1.047404, 'd3tok': 'كَ+_تِبِتِ', 'd2seg': 'كَ+_تِبِتِ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'ADP+PROPN', 'gen': 'f', 'catib6': 'PRT+PROP', 'root': 'NTWS', 'bwtok': 'كَ+_تِبِت_+ِ', 'pattern': 'كَNTWSِ', 'lex_logprob': -99.0, 'atbtok': 'كَ+_تِبِتِ', 'atbseg': 'كَ+_تِبِتِ', 'd1seg': 'كَتِبِتِ', 'stem': 'تِبِت', 'stemgloss': 'Tibet', 'stemcat': 'N'}\n",
      "Analysis 11: {'diac': 'كَتِبِتٍ', 'lex': 'تِبِت', 'bw': 'كَ/PREP+تِبِت/NOUN_PROP+ٍ/CASE_INDEF_GEN', 'gloss': 'like;such_as+Tibet+[indef.gen.]', 'pos': 'noun_prop', 'prc3': '0', 'prc2': '0', 'prc1': 'ka_prep', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'كَ+_تِبِتٍ', 'caphi': 'k_a_t_i_b_i_t_i_n', 'd1tok': 'كَتِبِتٍ', 'd2tok': 'كَ+_تِبِتٍ', 'pos_logprob': -1.047404, 'd3tok': 'كَ+_تِبِتٍ', 'd2seg': 'كَ+_تِبِتٍ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'ADP+PROPN', 'gen': 'f', 'catib6': 'PRT+PROP', 'root': 'NTWS', 'bwtok': 'كَ+_تِبِت_+ٍ', 'pattern': 'كَNTWSٍ', 'lex_logprob': -99.0, 'atbtok': 'كَ+_تِبِتٍ', 'atbseg': 'كَ+_تِبِتٍ', 'd1seg': 'كَتِبِتٍ', 'stem': 'تِبِت', 'stemgloss': 'Tibet', 'stemcat': 'N'}\n",
      "\n",
      "Analyzing the word: مكتوب\n",
      "Analysis 1: {'diac': 'مَكْتُوب', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/ADJ', 'gloss': 'written', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'u', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوب', 'caphi': 'm_a_k_t_uu_b', 'd1tok': 'مَكْتُوب', 'd2tok': 'مَكْتُوب', 'pos_logprob': -0.9868824, 'd3tok': 'مَكْتُوب', 'd2seg': 'مَكْتُوب', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب', 'pattern': 'مَ1ْ2ُو3', 'lex_logprob': -4.747338, 'atbtok': 'مَكْتُوب', 'atbseg': 'مَكْتُوب', 'd1seg': 'مَكْتُوب', 'stem': 'مَكْتُوب', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 2: {'diac': 'مَكْتُوبَ', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/ADJ+َ/CASE_DEF_ACC', 'gloss': 'written+[def.acc.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'a', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوبَ', 'caphi': 'm_a_k_t_uu_b_a', 'd1tok': 'مَكْتُوبَ', 'd2tok': 'مَكْتُوبَ', 'pos_logprob': -0.9868824, 'd3tok': 'مَكْتُوبَ', 'd2seg': 'مَكْتُوبَ', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب_+َ', 'pattern': 'مَ1ْ2ُو3َ', 'lex_logprob': -4.747338, 'atbtok': 'مَكْتُوبَ', 'atbseg': 'مَكْتُوبَ', 'd1seg': 'مَكْتُوبَ', 'stem': 'مَكْتُوب', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 3: {'diac': 'مَكْتُوبٌ', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/ADJ+ٌ/CASE_INDEF_NOM', 'gloss': 'written+[indef.nom.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'n', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوبٌ', 'caphi': 'm_a_k_t_uu_b_u_n', 'd1tok': 'مَكْتُوبٌ', 'd2tok': 'مَكْتُوبٌ', 'pos_logprob': -0.9868824, 'd3tok': 'مَكْتُوبٌ', 'd2seg': 'مَكْتُوبٌ', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب_+ٌ', 'pattern': 'مَ1ْ2ُو3ٌ', 'lex_logprob': -4.747338, 'atbtok': 'مَكْتُوبٌ', 'atbseg': 'مَكْتُوبٌ', 'd1seg': 'مَكْتُوبٌ', 'stem': 'مَكْتُوب', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 4: {'diac': 'مَكْتُوبِ', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/ADJ+ِ/CASE_DEF_GEN', 'gloss': 'written+[def.gen.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'g', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوبِ', 'caphi': 'm_a_k_t_uu_b_i', 'd1tok': 'مَكْتُوبِ', 'd2tok': 'مَكْتُوبِ', 'pos_logprob': -0.9868824, 'd3tok': 'مَكْتُوبِ', 'd2seg': 'مَكْتُوبِ', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب_+ِ', 'pattern': 'مَ1ْ2ُو3ِ', 'lex_logprob': -4.747338, 'atbtok': 'مَكْتُوبِ', 'atbseg': 'مَكْتُوبِ', 'd1seg': 'مَكْتُوبِ', 'stem': 'مَكْتُوب', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 5: {'diac': 'مَكْتُوبٍ', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/ADJ+ٍ/CASE_INDEF_GEN', 'gloss': 'written+[indef.gen.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'g', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوبٍ', 'caphi': 'm_a_k_t_uu_b_i_n', 'd1tok': 'مَكْتُوبٍ', 'd2tok': 'مَكْتُوبٍ', 'pos_logprob': -0.9868824, 'd3tok': 'مَكْتُوبٍ', 'd2seg': 'مَكْتُوبٍ', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب_+ٍ', 'pattern': 'مَ1ْ2ُو3ٍ', 'lex_logprob': -4.747338, 'atbtok': 'مَكْتُوبٍ', 'atbseg': 'مَكْتُوبٍ', 'd1seg': 'مَكْتُوبٍ', 'stem': 'مَكْتُوب', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 6: {'diac': 'مَكْتُوبُ', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/ADJ+ُ/CASE_DEF_NOM', 'gloss': 'written+[def.nom.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'n', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوبُ', 'caphi': 'm_a_k_t_uu_b_u', 'd1tok': 'مَكْتُوبُ', 'd2tok': 'مَكْتُوبُ', 'pos_logprob': -0.9868824, 'd3tok': 'مَكْتُوبُ', 'd2seg': 'مَكْتُوبُ', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب_+ُ', 'pattern': 'مَ1ْ2ُو3ُ', 'lex_logprob': -4.747338, 'atbtok': 'مَكْتُوبُ', 'atbseg': 'مَكْتُوبُ', 'd1seg': 'مَكْتُوبُ', 'stem': 'مَكْتُوب', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 7: {'diac': 'مَكْتُوب', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/NOUN', 'gloss': 'letter;message', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'u', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوب', 'caphi': 'm_a_k_t_uu_b', 'd1tok': 'مَكْتُوب', 'd2tok': 'مَكْتُوب', 'pos_logprob': -0.4344233, 'd3tok': 'مَكْتُوب', 'd2seg': 'مَكْتُوب', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب', 'pattern': 'مَ1ْ2ُو3', 'lex_logprob': -99.0, 'atbtok': 'مَكْتُوب', 'atbseg': 'مَكْتُوب', 'd1seg': 'مَكْتُوب', 'stem': 'مَكْتُوب', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n",
      "Analysis 8: {'diac': 'مَكْتُوبَ', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/NOUN+َ/CASE_DEF_ACC', 'gloss': 'letter;message+[def.acc.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'a', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوبَ', 'caphi': 'm_a_k_t_uu_b_a', 'd1tok': 'مَكْتُوبَ', 'd2tok': 'مَكْتُوبَ', 'pos_logprob': -0.4344233, 'd3tok': 'مَكْتُوبَ', 'd2seg': 'مَكْتُوبَ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب_+َ', 'pattern': 'مَ1ْ2ُو3َ', 'lex_logprob': -99.0, 'atbtok': 'مَكْتُوبَ', 'atbseg': 'مَكْتُوبَ', 'd1seg': 'مَكْتُوبَ', 'stem': 'مَكْتُوب', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n",
      "Analysis 9: {'diac': 'مَكْتُوبٌ', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/NOUN+ٌ/CASE_INDEF_NOM', 'gloss': 'letter;message+[indef.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'n', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوبٌ', 'caphi': 'm_a_k_t_uu_b_u_n', 'd1tok': 'مَكْتُوبٌ', 'd2tok': 'مَكْتُوبٌ', 'pos_logprob': -0.4344233, 'd3tok': 'مَكْتُوبٌ', 'd2seg': 'مَكْتُوبٌ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب_+ٌ', 'pattern': 'مَ1ْ2ُو3ٌ', 'lex_logprob': -99.0, 'atbtok': 'مَكْتُوبٌ', 'atbseg': 'مَكْتُوبٌ', 'd1seg': 'مَكْتُوبٌ', 'stem': 'مَكْتُوب', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n",
      "Analysis 10: {'diac': 'مَكْتُوبِ', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/NOUN+ِ/CASE_DEF_GEN', 'gloss': 'letter;message+[def.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوبِ', 'caphi': 'm_a_k_t_uu_b_i', 'd1tok': 'مَكْتُوبِ', 'd2tok': 'مَكْتُوبِ', 'pos_logprob': -0.4344233, 'd3tok': 'مَكْتُوبِ', 'd2seg': 'مَكْتُوبِ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب_+ِ', 'pattern': 'مَ1ْ2ُو3ِ', 'lex_logprob': -99.0, 'atbtok': 'مَكْتُوبِ', 'atbseg': 'مَكْتُوبِ', 'd1seg': 'مَكْتُوبِ', 'stem': 'مَكْتُوب', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n",
      "Analysis 11: {'diac': 'مَكْتُوبٍ', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/NOUN+ٍ/CASE_INDEF_GEN', 'gloss': 'letter;message+[indef.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوبٍ', 'caphi': 'm_a_k_t_uu_b_i_n', 'd1tok': 'مَكْتُوبٍ', 'd2tok': 'مَكْتُوبٍ', 'pos_logprob': -0.4344233, 'd3tok': 'مَكْتُوبٍ', 'd2seg': 'مَكْتُوبٍ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب_+ٍ', 'pattern': 'مَ1ْ2ُو3ٍ', 'lex_logprob': -99.0, 'atbtok': 'مَكْتُوبٍ', 'atbseg': 'مَكْتُوبٍ', 'd1seg': 'مَكْتُوبٍ', 'stem': 'مَكْتُوب', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n",
      "Analysis 12: {'diac': 'مَكْتُوبُ', 'lex': 'مَكْتُوب', 'bw': 'مَكْتُوب/NOUN+ُ/CASE_DEF_NOM', 'gloss': 'letter;message+[def.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'n', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مَكْتُوبُ', 'caphi': 'm_a_k_t_uu_b_u', 'd1tok': 'مَكْتُوبُ', 'd2tok': 'مَكْتُوبُ', 'pos_logprob': -0.4344233, 'd3tok': 'مَكْتُوبُ', 'd2seg': 'مَكْتُوبُ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'مَكْتُوب_+ُ', 'pattern': 'مَ1ْ2ُو3ُ', 'lex_logprob': -99.0, 'atbtok': 'مَكْتُوبُ', 'atbseg': 'مَكْتُوبُ', 'd1seg': 'مَكْتُوبُ', 'stem': 'مَكْتُوب', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n"
     ]
    }
   ],
   "source": [
    "example_words = [\n",
    "    \"يذهب\",\n",
    "    \"كتبت\",\n",
    "    \"مكتوب\"\n",
    "]\n",
    "\n",
    "for word in example_words:\n",
    "    print(f\"\\nAnalyzing the word: {word}\")\n",
    "    results = arabic_morph_analysis(word)\n",
    "    if results:\n",
    "        for idx, analysis in enumerate(results, 1):\n",
    "            print(f\"Analysis {idx}: {analysis}\")\n",
    "    else:\n",
    "        print(\"No analyses found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.16 Word Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Word Segmentation:_**</span> is the process of segementing a concatenated Arabic text into a properly spaced sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_word_segmentation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Segment concatenated Arabic text into a properly spaced sentence.\n",
    "\n",
    "    This function uses Camel Tools' MaxMatchSegmenter—a dictionary-based, greedy segmentation \n",
    "    algorithm—to determine the most likely word boundaries in a concatenated Arabic string.\n",
    "    The algorithm attempts to match the longest possible valid words from the beginning of the \n",
    "    string, inserting spaces where appropriate.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): A concatenated Arabic string that requires segmentation.\n",
    "\n",
    "    Returns:\n",
    "        str: The input text segmented into individual words separated by a single space.\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"وقالمصدرإنهناكتحسنافيالوضع\"\n",
    "        >>> arabic_word_segmentation(text)\n",
    "        \"وق المصدر إنه نا كت حس نا في الوضع\"\n",
    "        # (Actual segmentation may vary based on the dictionary and algorithm.)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: وقالمصدرإنهناكتحسنافيالوضع\n",
      "Segmented: ['وقالمصدرإنهناكتحسنافيالوضع']\n"
     ]
    }
   ],
   "source": [
    "text = \"وقالمصدرإنهناكتحسنافيالوضع\"\n",
    "segmented_text = tokenize_arabic(text)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Segmented:\", segmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> look into a way on how to implement the method (may be using min-max greedy approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.17 Part-of-speech tagging (POS tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Part-of-speech tagging:_**</span> is the process of determining of tagging a sentence with noun, verb, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_pos_tagging(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Perform part-of-speech (POS) tagging on an Arabic sentence.\n",
    "\n",
    "    This function uses a pre-trained Maximum Likelihood Estimation (MLE) disambiguator along with \n",
    "    a default POS tagger to assign part-of-speech tags to each token in the input Arabic text. \n",
    "    The process involves:\n",
    "      1. Tokenizing the input sentence using a simple word tokenizer.\n",
    "      2. Using the MLE disambiguator to resolve morphological ambiguities.\n",
    "      3. Tagging each token with its corresponding POS tag according to the model's tagging scheme.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): An Arabic sentence that needs POS tagging. The sentence should be in standard \n",
    "                    Arabic script and is expected to be a complete sentence.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples where each tuple contains a token from the input sentence and its \n",
    "              assigned POS tag. For example:\n",
    "              [('الطلاب', 'NOUN'), ('يذهبون', 'VERB'), ('إلى', 'PREP'), ('المدرسة', 'NOUN')]\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"الطلاب يذهبون إلى المدرسة\"\n",
    "        >>> arabic_pos_tagging(text)\n",
    "        [('الطلاب', 'NOUN'), ('يذهبون', 'VERB'), ('إلى', 'PREP'), ('المدرسة', 'NOUN')]\n",
    "    \"\"\"\n",
    "    mle = MLEDisambiguator.pretrained()\n",
    "    tagger = DefaultTagger(mle, 'pos')\n",
    "    \n",
    "    sentence = simple_word_tokenize(text)\n",
    "    \n",
    "    pos_tags = tagger.tag(sentence)\n",
    "    \n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Basic sentence with a noun, verb, preposition, and noun.\n",
      "Input: الطلاب يذهبون إلى المدرسة\n",
      "Output: ['noun', 'verb', 'prep', 'noun']\n",
      "--------------------------------------------------\n",
      "Example 2: Sentence with a noun, verb, and adverb.\n",
      "Input: الرئيس يتحدث بوضوح\n",
      "Output: ['noun', 'verb', 'noun']\n",
      "--------------------------------------------------\n",
      "Example 3: Empty string should return an empty list.\n",
      "Input: \n",
      "Output: []\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_sentences = [\n",
    "    (\"الطلاب يذهبون إلى المدرسة\", \"Basic sentence with a noun, verb, preposition, and noun.\"),\n",
    "    (\"الرئيس يتحدث بوضوح\", \"Sentence with a noun, verb, and adverb.\"),\n",
    "    (\"\", \"Empty string should return an empty list.\")\n",
    "]\n",
    "\n",
    "for idx, (input_text, description) in enumerate(example_sentences, 1):\n",
    "    print(f\"Example {idx}: {description}\")\n",
    "    print(\"Input:\", input_text)\n",
    "    result = arabic_pos_tagging(input_text)\n",
    "    print(\"Output:\", result)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.18 Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Disambiguation:_**</span> is the process of determining what is the most likely analysis of a word in a given context. Disambiguation is the backbone for many Arabic NLP tasks such as diacritization, POS tagging and morphological tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_disambiguation(text: str, ):\n",
    "    \"\"\"\n",
    "    Perform morphological disambiguation on an Arabic sentence.\n",
    "\n",
    "    This function determines the most likely morphological analysis for each word\n",
    "    in the input text. It uses a pretrained Maximum Likelihood Estimation (MLE)\n",
    "    disambiguator.\n",
    "    \n",
    "    For each word, the disambiguator produces a list of possible analyses sorted from \n",
    "    most likely to least likely. The function extracts the following from the top analysis:\n",
    "        - The diacritized form ('diac')\n",
    "        - The part-of-speech tag ('pos')\n",
    "        - The lemma or lexical form ('lex')\n",
    "    \n",
    "    In cases where a word does not receive any analysis (i.e. the analyses list is empty),\n",
    "    a default value is returned for that token (an empty string for 'diac' and 'lex', and \"O\"\n",
    "    for the POS tag).\n",
    "\n",
    "    Parameters:\n",
    "        text (str): An Arabic sentence to be disambiguated.\n",
    "\n",
    "    Returns:\n",
    "            tuple: Three lists containing the diacritized forms, part-of-speech tags, \n",
    "                   and lemmas for each word in the sentence.\n",
    "    \n",
    "    Example:\n",
    "        >>> text = \"ذهب الرجل إلى البنك\"\n",
    "        >>> diacritized, pos_tags, lemmas = arabic_disambiguation(text)\n",
    "        >>> print(lemmas)\n",
    "        ['ذهب', 'الرجل', 'إلى', 'البنك']\n",
    "    \n",
    "    Note:\n",
    "        Some words may not receive any analysis. In such cases, this function returns default\n",
    "        values (\"\" for diacritized/lemma and \"O\" for POS) for those words.\n",
    "    \"\"\"\n",
    "    mle = MLEDisambiguator.pretrained()\n",
    "    disambig = mle.disambiguate(text.split())\n",
    "    \n",
    "    diacritized = [d.analyses[0].analysis['diac'] if d.analyses else \"\" for d in disambig]\n",
    "    pos_tags    = [d.analyses[0].analysis['pos']  if d.analyses else \"O\" for d in disambig]\n",
    "    lemmas      = [d.analyses[0].analysis['lex']  if d.analyses else \"\" for d in disambig]\n",
    "\n",
    "    return diacritized, pos_tags, lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1: A sentence with a clear verb, noun, preposition, and noun.\n",
      "Input: ذهب الرجل إلى البنك\n",
      "Diacritized: ['ذَهَبَ', 'الرَجُلَ', 'إِلَى', 'البَنْكِ']\n",
      "POS tags:    ['verb', 'noun', 'prep', 'noun']\n",
      "Lemmas:      ['ذَهَب', 'رَجُل', 'إِلَى', 'بَنْك']\n",
      "\n",
      "Example 2: A sentence with a verb, noun, and noun.\n",
      "Input: كتب الطالب الدرس\n",
      "Diacritized: ['كَتَبَ', 'الطالِبُ', 'الدَرْسِ']\n",
      "POS tags:    ['verb', 'noun', 'noun']\n",
      "Lemmas:      ['كَتَب', 'طالِب', 'دَرْس']\n",
      "\n",
      "Example 3: Empty string should return empty lists.\n",
      "Input: \n",
      "Diacritized: []\n",
      "POS tags:    []\n",
      "Lemmas:      []\n"
     ]
    }
   ],
   "source": [
    "example_sentences = [\n",
    "    (\"ذهب الرجل إلى البنك\", \"A sentence with a clear verb, noun, preposition, and noun.\"),\n",
    "    (\"كتب الطالب الدرس\", \"A sentence with a verb, noun, and noun.\"),\n",
    "    (\"\", \"Empty string should return empty lists.\")\n",
    "]\n",
    "\n",
    "for idx, (sentence, description) in enumerate(example_sentences, 1):\n",
    "    print(f\"\\nExample {idx}: {description}\")\n",
    "    print(\"Input:\", sentence)\n",
    "    try:\n",
    "        result = arabic_disambiguation(sentence)\n",
    "        if isinstance(result, tuple):\n",
    "            diacritized, pos_tags, lemmas = result\n",
    "            print(\"Diacritized:\", diacritized)\n",
    "            print(\"POS tags:   \", pos_tags)\n",
    "            print(\"Lemmas:     \", lemmas)\n",
    "    except Exception as e:\n",
    "        print(\"Error during disambiguation:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.19 Elongated Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Elongated Words:_**</span> is reducing sequences of repeated characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_elongated_words(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize elongated words in Arabic text by reducing sequences of repeated characters.\n",
    "\n",
    "    This function replaces any sequence of a character repeated more than once with exactly two \n",
    "    consecutive occurrences of that character. This helps in standardizing words that are often \n",
    "    elongated in informal text (e.g., social media or SMS) to express emphasis. For instance, \n",
    "    \"جميللللل\" would be normalized to \"جميلل\".\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input sentence or word that may contain elongated characters.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized text where any sequence of repeated characters is reduced to two occurrences.\n",
    "\n",
    "    Examples:\n",
    "        >>> normalize_elongated_words(\"هههههههه\")\n",
    "        'هه'\n",
    "        >>> normalize_elongated_words(\"كبيييير\")\n",
    "        'كبيير'\n",
    "        >>> normalize_elongated_words(\"مرررحباا\")\n",
    "        'مررحباا'\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elongated: يااااا سلاااام على هذا البرنااامج الراااائع\n",
      "Normalized: ياا سلاام على هذا البرناامج الراائع\n"
     ]
    }
   ],
   "source": [
    "elongated_text = \"يااااا سلاااام على هذا البرنااامج الراااائع\"\n",
    "normalized_text = normalize_elongated_words(elongated_text)\n",
    "print(\"Elongated:\", elongated_text)\n",
    "print(\"Normalized:\", normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.20 Data Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Data Translation:_**</span> process of replacing each arabic word in the text with one of its english translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_arabic_word(text: str) -> list:\n",
    "    '''\n",
    "    A method that replaces each word in the text with one of its english translation.\n",
    "\n",
    "    :param text: a string to be processed\n",
    "    \n",
    "    :return: a list of translated text strings\n",
    "    '''\n",
    "    db = MorphologyDB.builtin_db()\n",
    "    analyzer = Analyzer(db)\n",
    "\n",
    "    words = text.split()\n",
    "    translated_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        analysis = analyzer.analyze(word)\n",
    "        if analysis:\n",
    "            # Pick a random translation\n",
    "            print(len(analysis))\n",
    "            print(analysis)\n",
    "            word_translation = random.choice(analysis)['stemgloss'].split(\";\")[0]\n",
    "            translated_words.append((word, word_translation))\n",
    "\n",
    "    return translated_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "[{'diac': 'الكُتّاب', 'lex': 'كُتّاب', 'bw': 'ال/DET+كُتّاب/NOUN', 'gloss': 'the+kuttab_(village_school);Quran_school', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'u', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كُتّاب', 'caphi': '2_a_l_k_u_t_t_aa_b', 'd1tok': 'الكُتّاب', 'd2tok': 'الكُتّاب', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كُتّاب', 'd2seg': 'الكُتّاب', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كُتّاب', 'pattern': 'ال1ُ2ّا3', 'lex_logprob': -99.0, 'atbtok': 'الكُتّاب', 'atbseg': 'الكُتّاب', 'd1seg': 'الكُتّاب', 'stem': 'كُتّاب', 'stemgloss': 'kuttab_(village_school);Quran_school', 'stemcat': 'N'}, {'diac': 'الكُتّابَ', 'lex': 'كُتّاب', 'bw': 'ال/DET+كُتّاب/NOUN+َ/CASE_DEF_ACC', 'gloss': 'the+kuttab_(village_school);Quran_school+[def.acc.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'a', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كُتّابَ', 'caphi': '2_a_l_k_u_t_t_aa_b_a', 'd1tok': 'الكُتّابَ', 'd2tok': 'الكُتّابَ', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كُتّابَ', 'd2seg': 'الكُتّابَ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كُتّاب_+َ', 'pattern': 'ال1ُ2ّا3َ', 'lex_logprob': -99.0, 'atbtok': 'الكُتّابَ', 'atbseg': 'الكُتّابَ', 'd1seg': 'الكُتّابَ', 'stem': 'كُتّاب', 'stemgloss': 'kuttab_(village_school);Quran_school', 'stemcat': 'N'}, {'diac': 'الكُتّابِ', 'lex': 'كُتّاب', 'bw': 'ال/DET+كُتّاب/NOUN+ِ/CASE_DEF_GEN', 'gloss': 'the+kuttab_(village_school);Quran_school+[def.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كُتّابِ', 'caphi': '2_a_l_k_u_t_t_aa_b_i', 'd1tok': 'الكُتّابِ', 'd2tok': 'الكُتّابِ', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كُتّابِ', 'd2seg': 'الكُتّابِ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كُتّاب_+ِ', 'pattern': 'ال1ُ2ّا3ِ', 'lex_logprob': -99.0, 'atbtok': 'الكُتّابِ', 'atbseg': 'الكُتّابِ', 'd1seg': 'الكُتّابِ', 'stem': 'كُتّاب', 'stemgloss': 'kuttab_(village_school);Quran_school', 'stemcat': 'N'}, {'diac': 'الكُتّابُ', 'lex': 'كُتّاب', 'bw': 'ال/DET+كُتّاب/NOUN+ُ/CASE_DEF_NOM', 'gloss': 'the+kuttab_(village_school);Quran_school+[def.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'n', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كُتّابُ', 'caphi': '2_a_l_k_u_t_t_aa_b_u', 'd1tok': 'الكُتّابُ', 'd2tok': 'الكُتّابُ', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كُتّابُ', 'd2seg': 'الكُتّابُ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كُتّاب_+ُ', 'pattern': 'ال1ُ2ّا3ُ', 'lex_logprob': -99.0, 'atbtok': 'الكُتّابُ', 'atbseg': 'الكُتّابُ', 'd1seg': 'الكُتّابُ', 'stem': 'كُتّاب', 'stemgloss': 'kuttab_(village_school);Quran_school', 'stemcat': 'N'}, {'diac': 'الكِتاب', 'lex': 'كِتاب', 'bw': 'ال/DET+كِتاب/NOUN', 'gloss': 'the+book', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'u', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كِتاب', 'caphi': '2_a_l_k_i_t_aa_b', 'd1tok': 'الكِتاب', 'd2tok': 'الكِتاب', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كِتاب', 'd2seg': 'الكِتاب', 'pos_lex_logprob': -3.511249, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كِتاب', 'pattern': 'ال1ِ2ا3', 'lex_logprob': -3.511249, 'atbtok': 'الكِتاب', 'atbseg': 'الكِتاب', 'd1seg': 'الكِتاب', 'stem': 'كِتاب', 'stemgloss': 'book', 'stemcat': 'Ndu'}, {'diac': 'الكِتابَ', 'lex': 'كِتاب', 'bw': 'ال/DET+كِتاب/NOUN+َ/CASE_DEF_ACC', 'gloss': 'the+book+[def.acc.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'a', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كِتابَ', 'caphi': '2_a_l_k_i_t_aa_b_a', 'd1tok': 'الكِتابَ', 'd2tok': 'الكِتابَ', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كِتابَ', 'd2seg': 'الكِتابَ', 'pos_lex_logprob': -3.511249, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كِتاب_+َ', 'pattern': 'ال1ِ2ا3َ', 'lex_logprob': -3.511249, 'atbtok': 'الكِتابَ', 'atbseg': 'الكِتابَ', 'd1seg': 'الكِتابَ', 'stem': 'كِتاب', 'stemgloss': 'book', 'stemcat': 'Ndu'}, {'diac': 'الكِتابِ', 'lex': 'كِتاب', 'bw': 'ال/DET+كِتاب/NOUN+ِ/CASE_DEF_GEN', 'gloss': 'the+book+[def.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كِتابِ', 'caphi': '2_a_l_k_i_t_aa_b_i', 'd1tok': 'الكِتابِ', 'd2tok': 'الكِتابِ', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كِتابِ', 'd2seg': 'الكِتابِ', 'pos_lex_logprob': -3.511249, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كِتاب_+ِ', 'pattern': 'ال1ِ2ا3ِ', 'lex_logprob': -3.511249, 'atbtok': 'الكِتابِ', 'atbseg': 'الكِتابِ', 'd1seg': 'الكِتابِ', 'stem': 'كِتاب', 'stemgloss': 'book', 'stemcat': 'Ndu'}, {'diac': 'الكِتابُ', 'lex': 'كِتاب', 'bw': 'ال/DET+كِتاب/NOUN+ُ/CASE_DEF_NOM', 'gloss': 'the+book+[def.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'n', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كِتابُ', 'caphi': '2_a_l_k_i_t_aa_b_u', 'd1tok': 'الكِتابُ', 'd2tok': 'الكِتابُ', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كِتابُ', 'd2seg': 'الكِتابُ', 'pos_lex_logprob': -3.511249, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كِتاب_+ُ', 'pattern': 'ال1ِ2ا3ُ', 'lex_logprob': -3.511249, 'atbtok': 'الكِتابُ', 'atbseg': 'الكِتابُ', 'd1seg': 'الكِتابُ', 'stem': 'كِتاب', 'stemgloss': 'book', 'stemcat': 'Ndu'}, {'diac': 'الكُتّاب', 'lex': 'كاتِب', 'bw': 'ال/DET+كُتّاب/NOUN', 'gloss': 'the+authors;writers', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'u', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كُتّاب', 'caphi': '2_a_l_k_u_t_t_aa_b', 'd1tok': 'الكُتّاب', 'd2tok': 'الكُتّاب', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كُتّاب', 'd2seg': 'الكُتّاب', 'pos_lex_logprob': -3.909189, 'num': 'p', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كُتّاب', 'pattern': 'ال1ُ2ّا3', 'lex_logprob': -3.909189, 'atbtok': 'الكُتّاب', 'atbseg': 'الكُتّاب', 'd1seg': 'الكُتّاب', 'stem': 'كُتّاب', 'stemgloss': 'authors;writers', 'stemcat': 'N'}, {'diac': 'الكُتّابَ', 'lex': 'كاتِب', 'bw': 'ال/DET+كُتّاب/NOUN+َ/CASE_DEF_ACC', 'gloss': 'the+authors;writers+[def.acc.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'a', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كُتّابَ', 'caphi': '2_a_l_k_u_t_t_aa_b_a', 'd1tok': 'الكُتّابَ', 'd2tok': 'الكُتّابَ', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كُتّابَ', 'd2seg': 'الكُتّابَ', 'pos_lex_logprob': -3.909189, 'num': 'p', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كُتّاب_+َ', 'pattern': 'ال1ُ2ّا3َ', 'lex_logprob': -3.909189, 'atbtok': 'الكُتّابَ', 'atbseg': 'الكُتّابَ', 'd1seg': 'الكُتّابَ', 'stem': 'كُتّاب', 'stemgloss': 'authors;writers', 'stemcat': 'N'}, {'diac': 'الكُتّابِ', 'lex': 'كاتِب', 'bw': 'ال/DET+كُتّاب/NOUN+ِ/CASE_DEF_GEN', 'gloss': 'the+authors;writers+[def.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'g', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كُتّابِ', 'caphi': '2_a_l_k_u_t_t_aa_b_i', 'd1tok': 'الكُتّابِ', 'd2tok': 'الكُتّابِ', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كُتّابِ', 'd2seg': 'الكُتّابِ', 'pos_lex_logprob': -3.909189, 'num': 'p', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كُتّاب_+ِ', 'pattern': 'ال1ُ2ّا3ِ', 'lex_logprob': -3.909189, 'atbtok': 'الكُتّابِ', 'atbseg': 'الكُتّابِ', 'd1seg': 'الكُتّابِ', 'stem': 'كُتّاب', 'stemgloss': 'authors;writers', 'stemcat': 'N'}, {'diac': 'الكُتّابُ', 'lex': 'كاتِب', 'bw': 'ال/DET+كُتّاب/NOUN+ُ/CASE_DEF_NOM', 'gloss': 'the+authors;writers+[def.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'n', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_كُتّابُ', 'caphi': '2_a_l_k_u_t_t_aa_b_u', 'd1tok': 'الكُتّابُ', 'd2tok': 'الكُتّابُ', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_كُتّابُ', 'd2seg': 'الكُتّابُ', 'pos_lex_logprob': -3.909189, 'num': 'p', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ك.ت.ب', 'bwtok': 'ال+_كُتّاب_+ُ', 'pattern': 'ال1ُ2ّا3ُ', 'lex_logprob': -3.909189, 'atbtok': 'الكُتّابُ', 'atbseg': 'الكُتّابُ', 'd1seg': 'الكُتّابُ', 'stem': 'كُتّاب', 'stemgloss': 'authors;writers', 'stemcat': 'N'}]\n",
      "6\n",
      "[{'diac': 'مُفِيد', 'lex': 'مُفِيد', 'bw': 'مُفِيد/ADJ', 'gloss': 'useful;beneficial', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'u', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مُفِيد', 'caphi': 'm_u_f_ii_d', 'd1tok': 'مُفِيد', 'd2tok': 'مُفِيد', 'pos_logprob': -0.9868824, 'd3tok': 'مُفِيد', 'd2seg': 'مُفِيد', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ف.#.د', 'bwtok': 'مُفِيد', 'pattern': 'مُ1ِي3', 'lex_logprob': -4.6224, 'atbtok': 'مُفِيد', 'atbseg': 'مُفِيد', 'd1seg': 'مُفِيد', 'stem': 'مُفِيد', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}, {'diac': 'مُفِيدَ', 'lex': 'مُفِيد', 'bw': 'مُفِيد/ADJ+َ/CASE_DEF_ACC', 'gloss': 'useful;beneficial+[def.acc.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'a', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مُفِيدَ', 'caphi': 'm_u_f_ii_d_a', 'd1tok': 'مُفِيدَ', 'd2tok': 'مُفِيدَ', 'pos_logprob': -0.9868824, 'd3tok': 'مُفِيدَ', 'd2seg': 'مُفِيدَ', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ف.#.د', 'bwtok': 'مُفِيد_+َ', 'pattern': 'مُ1ِي3َ', 'lex_logprob': -4.6224, 'atbtok': 'مُفِيدَ', 'atbseg': 'مُفِيدَ', 'd1seg': 'مُفِيدَ', 'stem': 'مُفِيد', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}, {'diac': 'مُفِيدٌ', 'lex': 'مُفِيد', 'bw': 'مُفِيد/ADJ+ٌ/CASE_INDEF_NOM', 'gloss': 'useful;beneficial+[indef.nom.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'n', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مُفِيدٌ', 'caphi': 'm_u_f_ii_d_u_n', 'd1tok': 'مُفِيدٌ', 'd2tok': 'مُفِيدٌ', 'pos_logprob': -0.9868824, 'd3tok': 'مُفِيدٌ', 'd2seg': 'مُفِيدٌ', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ف.#.د', 'bwtok': 'مُفِيد_+ٌ', 'pattern': 'مُ1ِي3ٌ', 'lex_logprob': -4.6224, 'atbtok': 'مُفِيدٌ', 'atbseg': 'مُفِيدٌ', 'd1seg': 'مُفِيدٌ', 'stem': 'مُفِيد', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}, {'diac': 'مُفِيدِ', 'lex': 'مُفِيد', 'bw': 'مُفِيد/ADJ+ِ/CASE_DEF_GEN', 'gloss': 'useful;beneficial+[def.gen.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'g', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مُفِيدِ', 'caphi': 'm_u_f_ii_d_i', 'd1tok': 'مُفِيدِ', 'd2tok': 'مُفِيدِ', 'pos_logprob': -0.9868824, 'd3tok': 'مُفِيدِ', 'd2seg': 'مُفِيدِ', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ف.#.د', 'bwtok': 'مُفِيد_+ِ', 'pattern': 'مُ1ِي3ِ', 'lex_logprob': -4.6224, 'atbtok': 'مُفِيدِ', 'atbseg': 'مُفِيدِ', 'd1seg': 'مُفِيدِ', 'stem': 'مُفِيد', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}, {'diac': 'مُفِيدٍ', 'lex': 'مُفِيد', 'bw': 'مُفِيد/ADJ+ٍ/CASE_INDEF_GEN', 'gloss': 'useful;beneficial+[indef.gen.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'g', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مُفِيدٍ', 'caphi': 'm_u_f_ii_d_i_n', 'd1tok': 'مُفِيدٍ', 'd2tok': 'مُفِيدٍ', 'pos_logprob': -0.9868824, 'd3tok': 'مُفِيدٍ', 'd2seg': 'مُفِيدٍ', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ف.#.د', 'bwtok': 'مُفِيد_+ٍ', 'pattern': 'مُ1ِي3ٍ', 'lex_logprob': -4.6224, 'atbtok': 'مُفِيدٍ', 'atbseg': 'مُفِيدٍ', 'd1seg': 'مُفِيدٍ', 'stem': 'مُفِيد', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}, {'diac': 'مُفِيدُ', 'lex': 'مُفِيد', 'bw': 'مُفِيد/ADJ+ُ/CASE_DEF_NOM', 'gloss': 'useful;beneficial+[def.nom.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'n', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'مُفِيدُ', 'caphi': 'm_u_f_ii_d_u', 'd1tok': 'مُفِيدُ', 'd2tok': 'مُفِيدُ', 'pos_logprob': -0.9868824, 'd3tok': 'مُفِيدُ', 'd2seg': 'مُفِيدُ', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'ف.#.د', 'bwtok': 'مُفِيد_+ُ', 'pattern': 'مُ1ِي3ُ', 'lex_logprob': -4.6224, 'atbtok': 'مُفِيدُ', 'atbseg': 'مُفِيدُ', 'd1seg': 'مُفِيدُ', 'stem': 'مُفِيد', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}]\n",
      "2\n",
      "[{'diac': 'لِلقِراءَة', 'lex': 'قِراءَة', 'bw': 'لِ/PREP+ال/DET+قِراء/NOUN+َة/NSUFF_FEM_SG', 'gloss': 'to;for_+_the+reading+[fem.sg.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': 'li_prep', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'u', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'لِ+_ل+_قِراءَة', 'caphi': 'l_i_l_q_i_r_aa_2_a', 'd1tok': 'لِلقِراءَة', 'd2tok': 'لِ+_القِراءَة', 'pos_logprob': -0.4344233, 'd3tok': 'لِ+_ال+_قِراءَة', 'd2seg': 'لِ+_لقِراءَة', 'pos_lex_logprob': -4.210219, 'num': 's', 'ud': 'ADP+NOUN', 'gen': 'f', 'catib6': 'PRT+NOM', 'root': 'ق.ر.#', 'bwtok': 'لِ+_ال+_قِراء_+َة', 'pattern': 'لِل1ِ2اءَة', 'lex_logprob': -4.210219, 'atbtok': 'لِ+_القِراءَة', 'atbseg': 'لِ+_لقِراءَة', 'd1seg': 'لِلقِراءَة', 'stem': 'قِراء', 'stemgloss': 'reading', 'stemcat': 'NapAt'}, {'diac': 'لِلقِراءَةِ', 'lex': 'قِراءَة', 'bw': 'لِ/PREP+ال/DET+قِراء/NOUN+َة/NSUFF_FEM_SG+ِ/CASE_DEF_GEN', 'gloss': 'to;for_+_the+reading+[fem.sg.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': 'li_prep', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'لِ+_ل+_قِراءَةِ', 'caphi': 'l_i_l_q_i_r_aa_2_a_t_i', 'd1tok': 'لِلقِراءَةِ', 'd2tok': 'لِ+_القِراءَةِ', 'pos_logprob': -0.4344233, 'd3tok': 'لِ+_ال+_قِراءَةِ', 'd2seg': 'لِ+_لقِراءَةِ', 'pos_lex_logprob': -4.210219, 'num': 's', 'ud': 'ADP+NOUN', 'gen': 'f', 'catib6': 'PRT+NOM', 'root': 'ق.ر.#', 'bwtok': 'لِ+_ال+_قِراء_+َة_+ِ', 'pattern': 'لِل1ِ2اءَةِ', 'lex_logprob': -4.210219, 'atbtok': 'لِ+_القِراءَةِ', 'atbseg': 'لِ+_لقِراءَةِ', 'd1seg': 'لِلقِراءَةِ', 'stem': 'قِراء', 'stemgloss': 'reading', 'stemcat': 'NapAt'}]\n",
      "Original: الكتاب مفيد للقراءة\n",
      "Tranlated data:\n",
      "1. original: الكتاب, translated: authors\n",
      "2. original: مفيد, translated: useful\n",
      "3. original: للقراءة, translated: reading\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "original_text = \"الكتاب مفيد للقراءة\"\n",
    "translated_text = translate_arabic_word(original_text)\n",
    "\n",
    "print(\"Original:\", original_text)\n",
    "print(\"Tranlated data:\")\n",
    "for i, (original, translated) in enumerate(translated_text, 1):\n",
    "    print(f\"{i}. original: {original}, translated: {translated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_NOTE:_**</span> The meaning of the text could be altered depending on the tashkeel added by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.21 Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Generation:_**</span> is the process of inflecting a lemma for a set of morphological features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_word_generation(word: str, pos: str = 'noun', gen: str = 'm', num: str = 'p'):\n",
    "    \"\"\"\n",
    "    Inflect an Arabic lemma into its fully diacritized form(s) based on specified morphological features.\n",
    "\n",
    "    This function generates all possible inflected (diacritized) forms for a given Arabic lemma\n",
    "    by applying a set of morphological features. It leverages a built-in morphological database (with\n",
    "    generation flags enabled) and a morphological generator to produce analyses that include details \n",
    "    such as the diacritized form ('diac'), part-of-speech, and other features.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The lemma (base form) of the Arabic word to be inflected.\n",
    "        pos (str, optional): The part-of-speech tag for the word. For example, 'noun' or 'verb'. \n",
    "                             Default is 'noun'.\n",
    "        gen (str, optional): The grammatical gender to be applied. For example, 'm' for masculine or \n",
    "                             'f' for feminine. Default is 'm' (masculine).\n",
    "        num (str, optional): The number specification, such as 's' for singular or 'p' for plural. \n",
    "                             Default is 'p' (plural).\n",
    "\n",
    "    Returns:\n",
    "        set: A set of unique diacritized forms (strings) produced by the morphological generator.\n",
    "             Each element represents a possible inflection for the input word given the features.\n",
    "\n",
    "    Example:\n",
    "        >>> # Inflect the noun \"كتاب\" (book) as a masculine plural.\n",
    "        >>> forms = arabic_word_generation(\"كتاب\", pos=\"noun\", gen=\"m\", num=\"p\")\n",
    "        >>> print(forms)\n",
    "        {'كُتُب', 'كُتُبٌ'}\n",
    "        \n",
    "        >>> # Inflect the adjective \"جديد\" (new) as a masculine singular.\n",
    "        >>> forms = arabic_word_generation(\"جديد\", pos=\"adj\", gen=\"m\", num=\"s\")\n",
    "        >>> print(forms)\n",
    "        {'جَدِيد', 'جَدِيدٌ'}\n",
    "\n",
    "    Note:\n",
    "        The actual output depends on the underlying morphological database and generator.\n",
    "        Ensure that the necessary classes (e.g., MorphologyDB and Generator) are imported and available.\n",
    "    \"\"\"\n",
    "    db = MorphologyDB.builtin_db(flags='g')\n",
    "    \n",
    "    generator = Generator(db)\n",
    "    \n",
    "    lemma = arabic_lemmatization(word)\n",
    "    \n",
    "    features = {\n",
    "        'pos': pos,\n",
    "        'gen': gen,\n",
    "        'num': num\n",
    "    }\n",
    "    \n",
    "    analyses = generator.generate(lemma, features)\n",
    "    \n",
    "    return set([a['diac'] for a in analyses])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_Note:_**</span> `'pos'` is the only *required* feature that needs to be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 - Noun (كتاب) as masculine plural:\n",
      "Input word: كتاب\n",
      "Generated forms: set()\n",
      "--------------------------------------------------\n",
      "Example 2 - Adjective (جديد) as masculine singular:\n",
      "Input word: جديد\n",
      "Generated forms: set()\n",
      "--------------------------------------------------\n",
      "Example 3 - Noun (معلمة) as feminine singular:\n",
      "Input word: معلمة\n",
      "Generated forms: set()\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Inflecting a noun (book) as masculine plural.\n",
    "word1 = \"كتاب\"\n",
    "print(\"Example 1 - Noun (كتاب) as masculine plural:\")\n",
    "forms1 = arabic_word_generation(word1, pos=\"noun\", gen=\"m\", num=\"p\")\n",
    "print(\"Input word:\", word1)\n",
    "print(\"Generated forms:\", forms1)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Example 2: Inflecting an adjective (new) as masculine singular.\n",
    "word2 = \"جديد\"\n",
    "print(\"Example 2 - Adjective (جديد) as masculine singular:\")\n",
    "forms2 = arabic_word_generation(word2, pos=\"adj\", gen=\"m\", num=\"s\")\n",
    "print(\"Input word:\", word2)\n",
    "print(\"Generated forms:\", forms2)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Example 3: Inflecting a noun with feminine features.\n",
    "word3 = \"معلمة\"  # base form might be provided without diacritics\n",
    "print(\"Example 3 - Noun (معلمة) as feminine singular:\")\n",
    "forms3 = arabic_word_generation(word3, pos=\"noun\", gen=\"f\", num=\"s\")\n",
    "print(\"Input word:\", word3)\n",
    "print(\"Generated forms:\", forms3)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> determine why the Generator is not working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.22 Reinflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Reinflection:_**</span> is the process of converting a given word in any form to a different form (i.e. tense, gender, etc). The CAMeL Tools reinflector works similar to the generator except that the word doesn't have to be a lemma and it is not have to be restricted to a specific `'pos'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_reinflection(word: str, num: str = 'd', prc1: str = 'bi_prep') -> set:\n",
    "    \"\"\"\n",
    "    Generate reinflected forms of an Arabic word based on specified morphological features.\n",
    "\n",
    "    This function takes an input word (typically in its lemma form) and applies reinflection\n",
    "    to produce alternative forms. Reinflection is the process of converting a word into different\n",
    "    forms (e.g., adjusting tense, gender, number, or attaching prefixes) as dictated by the desired\n",
    "    morphological features.\n",
    "\n",
    "    The function uses a built-in morphological database loaded with the 'r' (reinflection)\n",
    "    flag, along with a reinflector object, to produce analyses of the word. It then extracts the\n",
    "    diacritized form ('diac') from each analysis and returns a set of unique reinflected forms.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The Arabic word (typically its lemma) to be reinflected.\n",
    "        num (str, optional): A morphological feature representing number or a related property.\n",
    "                             The default value 'd' indicates a default or unspecified number feature.\n",
    "        prc1 (str, optional): A morphological feature typically used to indicate a proclitic (prefix)\n",
    "                              that might be attached to the word (e.g., a preposition). The default value\n",
    "                              'bi_prep' might indicate a proclitic for the preposition \"بـ\". Adjust these\n",
    "                              features based on your reinflection requirements.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of unique diacritized forms (strings) that represent the reinflected variants of the word,\n",
    "             according to the specified morphological features.\n",
    "\n",
    "    Examples:\n",
    "        >>> # Reinflect the word \"كتب\" with default features.\n",
    "        >>> forms = arabic_reinflection(\"كتب\")\n",
    "        >>> print(forms)\n",
    "        {'كُتُب', 'كُتِب'}  # (Example output; actual forms depend on the database and reinflection rules.)\n",
    "        \n",
    "        >>> # Reinflect the word \"درس\" specifying singular number and a proclitic for \"بـ\"\n",
    "        >>> forms = arabic_reinflection(\"درس\", num=\"s\", prc1=\"bi_prep\")\n",
    "        >>> print(forms)\n",
    "        {'دُرِس', 'دُرِسَ'}  # (Example output)\n",
    "\n",
    "    Note:\n",
    "        The actual output is contingent on the underlying morphological database and reinflection rules\n",
    "        provided by the library. Make sure the classes MorphologyDB and Reinflector are correctly imported and\n",
    "        available in your environment.\n",
    "    \"\"\"\n",
    "    db = MorphologyDB.builtin_db(flags='r')\n",
    "\n",
    "    reinflector = Reinflector(db)\n",
    "\n",
    "    features = {\n",
    "        'num': num,\n",
    "        'prc1': prc1\n",
    "    }\n",
    "\n",
    "    analyses = reinflector.reinflect(word, features)\n",
    "\n",
    "    return set(a['diac'] for a in analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Default reinflection for the word 'كتب'.\n",
      "Input word: كتب\n",
      "Reinflected forms: {'بِكِتابَيْنِ', 'بِكِتابَيْ'}\n",
      "--------------------------------------------------\n",
      "Test Case 2: Reinflection for 'درس' with singular number and 'بـ' prefix.\n",
      "Input word: درس\n",
      "Reinflected forms: {'بِدَرْس', 'بِدَرْسِ', 'بِدَرْسٍ'}\n",
      "--------------------------------------------------\n",
      "Test Case 3: Reinflection for 'شرب' with plural number and 'بـ' prefix.\n",
      "Input word: شرب\n",
      "Reinflected forms: set()\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    (\"كتب\", \"d\", \"bi_prep\", \"Default reinflection for the word 'كتب'.\"),\n",
    "    (\"درس\", \"s\", \"bi_prep\", \"Reinflection for 'درس' with singular number and 'بـ' prefix.\"),\n",
    "    (\"شرب\", \"p\", \"bi_prep\", \"Reinflection for 'شرب' with plural number and 'بـ' prefix.\"),\n",
    "]\n",
    "\n",
    "for idx, (word, num, prc1, desc) in enumerate(examples, 1):\n",
    "    print(f\"Test Case {idx}: {desc}\")\n",
    "    forms = arabic_reinflection(word, num, prc1)\n",
    "    print(\"Input word:\", word)\n",
    "    print(\"Reinflected forms:\", forms)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.23 Morphological Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Morphological Tokenization:_**</span> is a type of tokenization whereby Arabic words are split into component prefixes, stems, and suffixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MorphologicalTokenizer` class used to tokenize words in different schemes. It behaves very much like the `DefaultTagger` (used previously) in that it uses a disambiguator to first disambiguate words and then extracts a particular tokenization feature, but it has the following differences:\n",
    "\n",
    "- While the `DefaultTagger` produces exactly one output for each input word, the `MorphologicalTokenizer` might produce multiple output tokens.\n",
    "-  The `MorphologicalTokenizer` can be configured to produce diacritized and undiacritized output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_morphological_tokenization(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Perform morphological tokenization on an Arabic sentence.\n",
    "\n",
    "    This function first tokenizes the input sentence into words using a simple Arabic \n",
    "    word tokenizer. Then, it loads a pretrained morphological disambiguator (using the \n",
    "    'calima-msa-r13' model) and applies a morphological tokenizer to generate detailed \n",
    "    morphological tokens. The tokenizer is configured with:\n",
    "      - scheme='d3tok': specifying a particular morphological tokenization scheme.\n",
    "      - split=True: to output each morphological token as a separate string.\n",
    "      - diac=True: to output the tokens with diacritics.\n",
    "    \n",
    "    The result is a list of tokens that represent the morphological breakdown of the \n",
    "    input text. Note that the exact output depends on the model and its configuration.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): An Arabic sentence to be morphologically tokenized.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of morphological tokens (as strings). Each token represents a segment \n",
    "              of the input word based on its morphological structure, potentially including diacritics.\n",
    "    \n",
    "    Example:\n",
    "        >>> text = \"الطلابُ يدرسونَ في الجامعةِ\"\n",
    "        >>> tokens = arabic_morphological_tokenization(text)\n",
    "        >>> print(tokens)\n",
    "        ['ال', 'طلابُ', 'ي', 'در', 'سونَ', 'في', 'ال', 'جامعةِ']\n",
    "        # (Note: The actual segmentation may vary depending on the disambiguator and tokenizer configuration.)\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence into words using a simple tokenizer.\n",
    "    words = simple_word_tokenize(text)\n",
    "\n",
    "    # Load a pretrained morphological disambiguator (using the calima-msa-r13 model).\n",
    "    mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "\n",
    "    # Initialize the morphological tokenizer with specified configuration:\n",
    "    # - scheme: 'd3tok' to determine the tokenization scheme.\n",
    "    # - split: True to split the output into individual tokens.\n",
    "    # - diac: True to include diacritized forms in the output.\n",
    "    tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True, diac=True)\n",
    "    \n",
    "    # Perform morphological tokenization on the pre-tokenized words.\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: A basic sentence with common morphological structure.\n",
      "Input: الطلابُ يدرسونَ في الجامعةِ\n",
      "Output tokens: ['ال+', 'طُلّابِ', 'يَدْرُسُونَ', 'فِي', 'ال+', 'جامِعَةِ']\n",
      "--------------------------------------------------\n",
      "Test Case 2: A sentence with a verb, noun, and adverb.\n",
      "Input: كتب الطالب الدرس بسرعة\n",
      "Output tokens: ['كَتَبَ', 'ال+', 'طالِبُ', 'ال+', 'دَرْسِ', 'بِ+', 'سُرْعَةٍ']\n",
      "--------------------------------------------------\n",
      "Test Case 3: Empty string should return an empty list.\n",
      "Input: \n",
      "Output tokens: []\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    # (input text, description)\n",
    "    (\"الطلابُ يدرسونَ في الجامعةِ\", \"A basic sentence with common morphological structure.\"),\n",
    "    (\"كتب الطالب الدرس بسرعة\", \"A sentence with a verb, noun, and adverb.\"),\n",
    "    (\"\", \"Empty string should return an empty list.\")\n",
    "]\n",
    "\n",
    "for idx, (input_text, description) in enumerate(examples, 1):\n",
    "    print(f\"Test Case {idx}: {description}\")\n",
    "    print(\"Input:\", input_text)\n",
    "    tokens = arabic_morphological_tokenization(input_text)\n",
    "    print(\"Output tokens:\", tokens)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.24 Finding Synonyms (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_synonyms(text: str, num_augmentations: int = 1, tool: str = 'sina') -> list:\n",
    "    '''\n",
    "    Replaces each word in the text with one of its synonyms using the specified tool.\n",
    "    \n",
    "    Parameters:\n",
    "      text (str): The input text to process.\n",
    "      num_augmentations (int): Number of augmented outputs to generate.\n",
    "      tool (str): The augmentation tool to use ('sina' uses the evaluate_synonyms method).\n",
    "    \n",
    "    Returns:\n",
    "      list: A list of augmented text strings.\n",
    "    '''\n",
    "    # If using another tool, e.g. morphology analyzer, initialize it accordingly.\n",
    "    # db = MorphologyDB.builtin_db()\n",
    "    # analyzer = Analyzer(db)\n",
    "    \n",
    "    words = text.split()\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            synonyms_result = evaluate_synonyms(word, 3)\n",
    "            if synonyms_result:\n",
    "                synonym_candidates = [syn[0] for syn in synonyms_result if syn and syn[0] != word]\n",
    "                if synonym_candidates:\n",
    "                    new_word = random.choice(synonym_candidates)\n",
    "                else:\n",
    "                    new_word = word\n",
    "            else:\n",
    "                new_word = word\n",
    "            new_words.append(new_word)\n",
    "        augmented_texts.append(' '.join(new_words))\n",
    "    \n",
    "    return augmented_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> find a way to do data synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.25 Data Augmenetation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> find a way to do data augmentation.\n",
    "\n",
    "Links:\n",
    "1. https://medium.com/@Mustafa77/data-augmentation-using-transformers-and-similarity-measures-2812c4853ed3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> Translate Arabic to English and perform natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handling Very Common Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_common_words(df: pd.DataFrame, mode='remove'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handling Very Rare Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_rare_words(df: pd.DataFrame, mode='remove'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handling Numbers and Special Characters in Arabic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_numbers_and_special_chars(text, mode='remove'):\n",
    "    \"\"\"\n",
    "    Process Arabic text by either removing or normalizing numbers and special characters.\n",
    "\n",
    "    This function handles numbers and special characters in an Arabic text in one of two ways:\n",
    "      - 'remove': Eliminates all characters that are not Arabic letters (Unicode range \\u0600-\\u06FF) or whitespace.\n",
    "      - 'normalize': Converts Arabic digits (٠١٢٣٤٥٦٧٨٩) to their corresponding Western numeral characters (0-9)\n",
    "                     while leaving other characters unchanged.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input Arabic text, which may include numbers and special characters.\n",
    "        mode (str, optional): The mode of processing, either 'remove' or 'normalize'. \n",
    "                              Default is 'remove'.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after applying the specified operation.\n",
    "\n",
    "    Examples:\n",
    "        >>> handle_numbers_and_special_chars(\"اللغة ١٢٣ جميلة!\", mode='remove')\n",
    "        'اللغة جميلة'\n",
    "        >>> handle_numbers_and_special_chars(\"اللغة ١٢٣ جميلة!\", mode='normalize')\n",
    "        'اللغة 123 جميلة!'\n",
    "    \"\"\"\n",
    "    if mode == 'remove':\n",
    "        # Remove any character that is not an Arabic letter (or whitespace)\n",
    "        return re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    elif mode == 'normalize':\n",
    "        # Normalize Hindi numbers to Arabic numerals\n",
    "        number_map = {\n",
    "            '٠': '0', '١': '1', '٢': '2', '٣': '3', '٤': '4',\n",
    "            '٥': '5', '٦': '6', '٧': '7', '٨': '8', '٩': '9'\n",
    "        }\n",
    "        for arabic, western in number_map.items():\n",
    "            text = text.replace(arabic, western)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Mode = remove\n",
      "Input:     اللغة ١٢٣ جميلة!\n",
      "Expected:  'اللغة جميلة'\n",
      "Result:    'اللغة ١٢٣ جميلة'\n",
      "Pass: False\n",
      "----------------------------------------\n",
      "Test Case 2: Mode = normalize\n",
      "Input:     اللغة ١٢٣ جميلة!\n",
      "Expected:  'اللغة 123 جميلة!'\n",
      "Result:    'اللغة 123 جميلة!'\n",
      "Pass: True\n",
      "----------------------------------------\n",
      "Test Case 3: Mode = remove\n",
      "Input:     هذا نص مع رموز مثل #، و ٤٥٦!\n",
      "Expected:  'هذا نص مع رموز مثل  و '\n",
      "Result:    'هذا نص مع رموز مثل ، و ٤٥٦'\n",
      "Pass: False\n",
      "----------------------------------------\n",
      "Test Case 4: Mode = normalize\n",
      "Input:     هذا نص مع رموز مثل #، و ٤٥٦!\n",
      "Expected:  'هذا نص مع رموز مثل #، و 456!'\n",
      "Result:    'هذا نص مع رموز مثل #، و 456!'\n",
      "Pass: True\n",
      "----------------------------------------\n",
      "Test Case 5: Mode = remove\n",
      "Input:     \n",
      "Expected:  ''\n",
      "Result:    ''\n",
      "Pass: True\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"اللغة ١٢٣ جميلة!\",\n",
    "        \"mode\": \"remove\",\n",
    "        \"expected\": \"اللغة جميلة\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"اللغة ١٢٣ جميلة!\",\n",
    "        \"mode\": \"normalize\",\n",
    "        \"expected\": \"اللغة 123 جميلة!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"هذا نص مع رموز مثل #، و ٤٥٦!\",\n",
    "        \"mode\": \"remove\",\n",
    "        \"expected\": \"هذا نص مع رموز مثل  و \"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"هذا نص مع رموز مثل #، و ٤٥٦!\",\n",
    "        \"mode\": \"normalize\",\n",
    "        \"expected\": \"هذا نص مع رموز مثل #، و 456!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\",\n",
    "        \"mode\": \"remove\",\n",
    "        \"expected\": \"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "for idx, case in enumerate(examples, 1):\n",
    "    result = handle_numbers_and_special_chars(case[\"input\"], mode=case[\"mode\"])\n",
    "    print(f\"Test Case {idx}: Mode = {case['mode']}\")\n",
    "    print(\"Input:    \", case[\"input\"])\n",
    "    print(\"Expected: \", repr(case[\"expected\"]))\n",
    "    print(\"Result:   \", repr(result))\n",
    "    print(\"Pass:\", result == case[\"expected\"])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> fix `handle_numbers_and_special_chars` method as it sometimes provides incorrect output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Preporcessing</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Text Classification:_**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_arabic_text(text: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Classify Arabic text using the specified pretrained model from Hugging Face.\n",
    "    \n",
    "    Parameters:\n",
    "      text (str): The Arabic text to classify.\n",
    "      model_name (str): The Hugging Face model name for sequence classification.\n",
    "      \n",
    "    Returns:\n",
    "      list: A list of probabilities corresponding to each class.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "    processed_text = arabert_prep.preprocess(text)\n",
    "\n",
    "    inputs = tokenizer(processed_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return predictions.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2025-02-15 06:19:28,990 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: هذا النص رائع ومفيد جداً\n",
      "Classification probabilities: [0.8355841636657715, 0.1644158959388733]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"هذا النص رائع ومفيد جداً\"\n",
    "classification = classify_arabic_text(text, model_name=\"aubmindlab/bert-base-arabertv2\")\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Classification probabilities: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> finetune model to perform text classification across multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Sentiment Analysis:_**</span> is identifying whether a text is classified as positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_arabic_sentiment(text: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of an Arabic text using a pretrained sentiment analysis model.\n",
    "\n",
    "    This function uses the Hugging Face Transformers sentiment analysis pipeline with the\n",
    "    model \"CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\" to classify the sentiment\n",
    "    of the input Arabic text. It returns a tuple containing the sentiment label (e.g., \"POSITIVE\"\n",
    "    or \"NEGATIVE\") and the associated confidence score.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The Arabic text to be analyzed for sentiment.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (label, score) where:\n",
    "            - label (str): The predicted sentiment label.\n",
    "            - score (float): The confidence score (between 0 and 1) for the predicted sentiment.\n",
    "\n",
    "    Example:\n",
    "        >>> label, score = analyze_arabic_sentiment(\"هذا النص رائع ومفيد جداً\")\n",
    "        >>> print(f\"Sentiment: {label} (confidence: {score:.2f})\")\n",
    "\n",
    "    \"\"\"\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\")\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    return result['label'], result['score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: أنا سعيد جداً بهذا المنتج!\n",
      "Sentiment: positive, Score: 0.9928115010261536\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"أنا سعيد جداً بهذا المنتج!\"\n",
    "sentiment, score = analyze_arabic_sentiment(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Sentiment: {sentiment}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: أنت مالك ياض !\n",
      "Sentiment: negative, Score: 0.9415218234062195\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"أنت مالك ياض !\"\n",
    "sentiment, score = analyze_arabic_sentiment(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Sentiment: {sentiment}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Word Embedding:_**</span> is a way of representing words as dense, continuous vectors in a high-dimensional space. These vectors capture semantic relationships between words so that words with similar meanings are mapped to nearby points in the vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_word_embedding(text: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute the word embedding for an Arabic word and retrieve its most similar terms.\n",
    "\n",
    "    This function loads a pretrained Word2Vec model, cleans and normalizes the input Arabic text,\n",
    "    and then retrieves the word embedding vector for the cleaned word. Additionally, it finds and \n",
    "    prints the most similar words based on cosine similarity within the embedding space.\n",
    "\n",
    "    The cleaning process includes:\n",
    "      - Replacing various Arabic characters with normalized forms.\n",
    "      - Removing diacritics (tashkeel) and repeated character elongations.\n",
    "      - Trimming whitespace and handling specific punctuation or symbols.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The Arabic word or phrase to process.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - word_vector: The vector representation of the cleaned word (typically a NumPy array).\n",
    "            - most_similar: A list of tuples, where each tuple consists of a similar word (str) \n",
    "              and its similarity score (float).\n",
    "\n",
    "    Example:\n",
    "        >>> vector, similar = arabic_word_embedding(\"القاهرة\")\n",
    "        Most similar words (and their similarity scores) are printed, and 'vector' holds the embedding for the cleaned word.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the pretrained Word2Vec model.\n",
    "    model = gensim.models.Word2Vec.load('./models/tweet_cbow_300/tweets_cbow_300')\n",
    "\n",
    "    # Clean/Normalize Arabic Text\n",
    "    def clean_str(text):\n",
    "        search = [\"أ\", \"إ\", \"آ\", \"ة\", \"_\", \"-\", \"/\", \".\", \"،\", \" و \", \" يا \", '\"', \"ـ\", \"'\", \"ى\", \"\\\\\", '\\n', '\\t', '&quot;', '?', '؟', '!']\n",
    "        replace = [\"ا\", \"ا\", \"ا\", \"ه\", \" \", \" \", \"\", \"\", \"\", \" و\", \" يا\", \"\", \"\", \"\", \"ي\", \"\", \" \", \" \", \" ? \", \" ؟ \", \" ! \"]\n",
    "        \n",
    "        # Remove tashkeel (diacritics)\n",
    "        p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "        text = re.sub(p_tashkeel, \"\", text)\n",
    "        \n",
    "        # Remove longation (repeated characters)\n",
    "        p_longation = re.compile(r'(.)\\1+')\n",
    "        subst = r\"\\1\\1\"\n",
    "        text = re.sub(p_longation, subst, text)\n",
    "        \n",
    "        text = text.replace('وو', 'و')\n",
    "        text = text.replace('يي', 'ي')\n",
    "        text = text.replace('اا', 'ا')\n",
    "        \n",
    "        for i in range(len(search)):\n",
    "            text = text.replace(search[i], replace[i])\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    # Clean the input text\n",
    "    word = clean_str(text)\n",
    "\n",
    "    # Retrieve and print the most similar terms to the cleaned word\n",
    "    most_similar = model.wv.most_similar(word)\n",
    "    print(\"Most similar terms to '{}':\".format(word))\n",
    "    for term, score in most_similar:\n",
    "        print(term, score)\n",
    "        \n",
    "    # Retrieve the word vector\n",
    "    word_vector = model.wv[word]\n",
    "\n",
    "    return word_vector, most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext is not working on Windows\n",
    "# import fasttext\n",
    "\n",
    "def word2vec_fasttext(text: str):\n",
    "    # create word embedding model\n",
    "    model = fasttext.train_unsupervised('xxx.txt', epoch=25)\n",
    "\n",
    "    # get word embeddings for words in text\n",
    "    word_embeddings = model.get_word_vector(text)\n",
    "\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الاسكندريه 0.822142481803894\n",
      "اسوان 0.7448597550392151\n",
      "الجيزه 0.7406017184257507\n",
      "المنصوره 0.7375915050506592\n",
      "الاسماعيليه 0.7310688495635986\n",
      "بورسعيد 0.7242382168769836\n",
      "الاقصر 0.7210926413536072\n",
      "حلوان 0.7209565043449402\n",
      "دمياط 0.7096551060676575\n",
      "طنطا 0.7094075679779053\n",
      "Word vector for 'القاهرة': [-0.36143455  0.32451856  0.14601593  0.32183495 -2.4407325   2.3771033\n",
      " -0.02987524 -0.36511382 -1.9445266   2.0458498  -0.4462689   0.8169745\n",
      "  0.57143867 -0.14586152 -2.7012775   1.5832865   1.6561981   2.3886893\n",
      " -0.7477331  -2.2364702  -0.3022785  -0.44031492  1.0667934  -1.2664819\n",
      "  0.5260765   0.87624025  0.58786726 -0.59008116 -2.0730557   1.4947067\n",
      "  0.61162     4.520309    0.03703779 -0.02008293 -1.1760161  -0.907512\n",
      " -0.6775007  -1.4298267   0.43027702 -0.3751945   2.1304162   1.6183015\n",
      " -0.7221879   0.3284036  -1.151335    1.0218043  -0.19037294 -0.25370607\n",
      "  0.55373436  0.2848552  -2.660286    1.0729909   0.5107816  -2.057838\n",
      " -1.3162624  -0.8008683  -2.131203    1.3305361  -0.08949289 -0.20919245\n",
      "  0.6519448  -0.42523125 -2.1237717   1.7458177   0.9424573  -1.6369351\n",
      " -0.4509722  -0.7313934   0.373175    2.1887953   1.2610693   1.3537819\n",
      " -1.8460715  -0.42807204 -0.5282065  -1.7234492   1.5000864   0.04967218\n",
      " -2.7448952   1.9553512  -0.3313362  -1.8806909   1.7006743  -2.7383993\n",
      " -0.6716011  -2.0714629   0.5913716  -0.6824405   0.30297148 -0.90492505\n",
      " -0.7712346   0.5803537   0.1335388   2.7877033  -1.5064228  -1.8370237\n",
      "  0.2697579   2.3486383   2.103636    1.521233   -1.4001724   0.82219344\n",
      "  0.8240768  -0.15086019  0.2961153  -1.0394131   0.5568691   0.28836966\n",
      "  0.6295702   1.7095642  -0.85605335 -0.6515357   1.4827238   1.002474\n",
      "  0.19940409  1.070932   -0.5550646   2.214102    0.37053385 -0.79781234\n",
      " -1.6374146  -0.60718566 -1.9219145  -2.0604632  -1.6528308   0.33980826\n",
      " -3.5135572   0.41424054  1.9958695  -0.35256767  2.6131797  -3.3231125\n",
      " -0.12307548 -0.9431562   1.1456573  -0.430937    1.071695   -0.6364221\n",
      " -1.233112    0.33730644  1.1635537   1.5573068  -1.2561119   0.15221852\n",
      "  0.4580792  -1.0473022   0.32258037 -2.594469   -0.5203601  -3.1735766\n",
      " -0.19842161 -1.7157494  -1.4219034  -0.5446474   0.4559874   1.8500901\n",
      " -1.9773417   0.5568433  -1.7871313  -0.71735185  1.132678   -0.837672\n",
      " -0.9265186  -0.277091   -0.4498865  -0.65538085 -0.29954773  0.162286\n",
      " -0.5294098   0.83181196  0.73085546 -2.3702266   1.0934252   2.0041482\n",
      "  2.1957505  -0.07741582 -2.0670712   1.0325543  -2.668172    1.6843041\n",
      "  0.3038795  -0.65110064 -0.80047244  2.6929338   2.8901029  -0.34811088\n",
      "  0.99073356  1.696957    1.4957255  -0.33842573  1.4915138   0.58355224\n",
      " -2.7133086   2.4071438   1.3807678  -0.01793567 -1.3098323   1.5640415\n",
      " -0.15761942  3.5385716  -0.941214   -0.7848572  -1.0996732  -0.24231693\n",
      " -2.0031528   0.07146657  0.14217159 -0.26462936 -0.27889195 -0.3159959\n",
      "  0.48282027  1.7386041   1.9550546  -1.6407006  -0.18940933  0.56804603\n",
      "  0.23227286 -3.058908    0.15069857  0.83813035 -1.6782109   0.16098635\n",
      " -2.5502133  -0.23519206  0.77975184  3.4241571  -0.4207713  -0.11863837\n",
      " -1.1716788   0.75626653 -1.7658116  -1.3981905  -0.1646244   3.7715173\n",
      " -0.2594515   0.8704249   2.596015   -0.14752974 -2.328448   -1.4490464\n",
      " -1.2590245  -0.17023873  0.3576156   1.7758955   1.4414039  -1.2650518\n",
      "  0.21300997  0.65599275 -1.1683582  -0.38035727  1.3608054  -0.6101739\n",
      "  0.47013474  6.905608   -0.95340157 -0.6166638  -2.072214    0.19559622\n",
      " -1.0854616   3.545438   -2.2825742  -1.2395469  -0.01394261 -0.6912439\n",
      " -0.68033797  0.38063502 -0.34611645  0.15759547 -0.6383159   1.1406134\n",
      "  2.8847592  -0.50538653  0.6145402   0.11379281 -1.8095436   2.7481453\n",
      "  0.91495186 -0.39614347 -1.0227388   1.1418179  -1.3758924  -0.26808223\n",
      "  0.44675693 -1.7718596  -0.22452602 -0.39037687 -0.4203093  -0.57415867\n",
      "  1.2875202   1.020447    0.12890834  0.4915232   0.9719728  -0.70402974\n",
      " -3.5051332   0.2558523  -1.3064789  -1.0550935   1.9999434  -1.8610594 ]\n"
     ]
    }
   ],
   "source": [
    "vector, similar = arabic_word_embedding(\"القاهرة\")\n",
    "print(\"Word vector for 'القاهرة':\", vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Label Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Topic Modeling:_**</span> is an unsupervised machine learning technique for finding abstract topics in a large collection of documents. It helps in organizing, understanding and summarizing large collections of textual information and discovering the latent topics that vary among documents in a given corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet allocation (LDA) and Non-Negative Matrix Fatorization (NMF) are two of the most popular topic modeling techniques. LDA uses a probabilistic approach whereas NMF uses matrix factorization approach, however, new techniques that are based on BERT for topic modeling do exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://colab.research.google.com/drive/1OT_wcYKpKS73uR6y7IVYjJVxaP-C1H3k?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Translate to English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> Translate Arabic to English and perform natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detecting Sarcasm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://medium.com/@rehabreda/unraveling-sarcasm-in-arabic-with-arabert-a-comprehensive-guide-from-data-preprocessing-to-a4dc7e30b39d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Pipeline Execution</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Visualize Data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Resources</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BERT for Arabic Topic Modeling: An Experimental Study on BERTopic Technique (https://github.com/iwan-rg/Arabic-Topic-Modeling?tab=readme-ov-file)\n",
    "2. NYU ABU DHABI (https://nyuad.nyu.edu/en/research/faculty-labs-and-projects/computational-approaches-to-modeling-language-lab/research/arabic-natural-language-processing.html)\n",
    "3. CAMeL Tools (https://camel-tools.readthedocs.io/en/latest/api.html)\n",
    "4. Comprehensive Arabic NLP Data Processing and Cleaning Guide (https://github.com/h9-tect/Arabic_nlp_preprocessing)\n",
    "5. PyArabic (https://pyarabic.readthedocs.io/ar/latest/)\n",
    "6. AUB MIND LAB (https://huggingface.co/aubmindlab)\n",
    "7. AraBERT (https://github.com/aub-mind/arabert/tree/master)\n",
    "8. Awesome Resources for Arabic NLP Repo (https://github.com/Curated-Awesome-Lists/awesome-arabic-nlp?tab=readme-ov-file)\n",
    "9. Arabic Dialect Identification Models (https://github.com/Lafifi-24/arabic-dialect-identification?tab=readme-ov-file)\n",
    "10. AraVec (https://github.com/bakrianoo/aravec/blob/master/AraVec%202.0/README.md)\n",
    "11. Text Classifier (https://github.com/mustaphakamil/Arabic-text-classification/blob/master/Text%20Classifier%20NLP.ipynb)\n",
    "12. BERT for Arabic Topic Modeling (https://colab.research.google.com/drive/1OT_wcYKpKS73uR6y7IVYjJVxaP-C1H3k?usp=sharing#scrollTo=SNa-KtKDRnus)\n",
    "13. Dialect Identification (https://medium.com/@kmelad43/arabic-dialect-identification-774de9315140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
