{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Setup</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import typing\n",
    "import random\n",
    "import emoji\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(file_path: str) -> list:\n",
    "    '''\n",
    "    process youtube/podcast documents\n",
    "\n",
    "    @param file_path: a string represent file path\n",
    "    '''\n",
    "\n",
    "    with open(file_path, 'r', errors=\"ignore\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Remove numbers followed by ':'\n",
    "    text = re.sub(r'\\d+.*\\d*\\s*:', '', text)\n",
    "\n",
    "    # Define sentence delimiters for Arabic\n",
    "    sentence_endings = r'(?<=[.!ÿüÿõÿå])\\s+'\n",
    "\n",
    "    # Split sentences while preserving dependencies\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 1] # Remove empty strings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_df(sentences: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    convert a list of sentences to a dataframe\n",
    "\n",
    "    @param sentences: a list of sentences\n",
    "    '''\n",
    "\n",
    "    return pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path: str = \"./data\") -> pd.DataFrame:\n",
    "    '''\n",
    "    Load dataset from a directory\n",
    "\n",
    "    :params: **file_path**: a string representing file path to dataset.\n",
    "    '''\n",
    "    output_df = []\n",
    "\n",
    "    data_folder = \"./data\"\n",
    "\n",
    "    for file_name in os.listdir(data_folder):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(data_folder, file_name)\n",
    "            tmp_df = sentences_to_df(extract_sentences(file_path))\n",
    "            output_df.append(tmp_df)\n",
    "\n",
    "    output_df = pd.concat(output_df, ignore_index=True)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                      0\n",
       "0                ŸÑŸÉŸÑ ÿ¥ÿÆÿµ ŸÅŸä ÿ¢ÿÆÿ± ÿßŸÑÿ¥Ÿáÿ±ÿå\n",
       "1               ŸÖÿß ŸÖÿπÿßŸáŸàÿ¥ ÿ∫Ÿäÿ± 50 ÿ¨ŸÜŸäŸáÿå\n",
       "2           ŸàŸÖÿ≠ÿ™ÿßÿ¨ ŸäÿßŸÉŸÑ ÿ£ŸÉŸÑÿ© ÿ™ÿ¥ÿ®ŸëÿπŸá...\n",
       "3                    ŸÑŸÉŸÑ Ÿàÿßÿ≠ÿØ \"ŸÅŸàÿ±ŸÖÿ©\"ÿå\n",
       "4              ŸÖÿ≠ÿ™ÿßÿ¨ ÿ£ŸÉŸÑÿ© ÿ≥ÿ±Ÿäÿπÿ© ÿßŸÑŸáÿ∂ŸÖÿå\n",
       "..                                 ...\n",
       "733          ŸÑÿßÿ≤ŸÖ ÿßŸÑÿ≥ŸÜÿØŸàÿ™ÿ¥ ŸÖÿπÿßŸá ÿ®ÿ∑ÿßÿ∑ÿ≥.\n",
       "734              Ÿàÿ£ŸÇŸàŸÖ ŸÖÿ∑ŸÑŸëÿπ ÿ®ÿ∑ÿßÿ∑ÿ≥ÿßŸäÿ©ÿå\n",
       "735          Ÿàÿ£ŸÇŸàŸÖ ÿ≠ÿßÿ∑ÿ∑Ÿáÿß ŸÅŸä ÿßŸÑÿ≥ŸÜÿØŸàÿ™ÿ¥!\n",
       "736  ÿØÿß ÿ£ŸÜÿß ÿ£ÿ≠ŸäÿßŸÜŸãÿß ÿ®ÿ¨Ÿäÿ® ÿ≥ŸÜÿØŸàÿ™ÿ¥ ÿ®ÿ∑ÿßÿ∑ÿ≥ÿå\n",
       "737    ŸÖŸÜ \"ÿßŸÑÿ≠ÿ±ŸÖŸäŸÜ\" ÿßŸÑŸÑŸä ŸÅŸä \"ÿßŸÑÿ≠Ÿèÿµÿ±Ÿä\".\n",
       "\n",
       "[738 rows x 1 columns]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./data/ÿßŸÑÿ®ÿ∑ÿßÿ∑ÿ≥  ÿßŸÑÿØÿ≠Ÿäÿ≠.txt\"\n",
    "df = load_dataset(file_path)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Libraries & Models</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ghalatawi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ghalatawi:** an Arabic Autocorrect library ŸÖŸÉÿ™ÿ®ÿ© ŸÑŸÑÿ™ÿµÿ≠Ÿäÿ≠ ÿßŸÑÿ™ŸÑŸÇÿßÿ¶Ÿä ŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\n",
    "\n",
    "Source: https://github.com/linuxscout/ghalatawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regex': True, 'wordlist': True, 'punct': True, 'typo': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ghalatawi.autocorrector import AutoCorrector\n",
    "\n",
    "autoco = AutoCorrector()\n",
    "\n",
    "autoco.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_Note:_**</span> The library allow for fixing spelling, adjusting punctuations, typos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyArabic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyArabic:** a Arabic language library for Python, provides basic functions to manipulate Arabic letters and text, like detecting Arabic letters, Arabic letters groups and characteristics, remove diacritics etc.\n",
    "\n",
    "Source: https://github.com/linuxscout/pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number\n",
    "\n",
    "autoco = AutoCorrector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CAMeL Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CAMeL Bert:** a collection of BERT models pre-trained on Arabic texts with different sizes and variants.\n",
    "\n",
    "Source: https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name CAMeL-Lab/bert-base-arabic-camelbert-msa. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# Load Arabic SBERT Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model = SentenceTransformer(\"CAMeL-Lab/bert-base-arabic-camelbert-msa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DSAraby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DSAraby:** is a library that aims to transliterate text which is to write a word using the closest corresponding letters of a different alphabet or language.\n",
    "\n",
    "Source: https://github.com/saobou/DSAraby/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsaraby import DSAraby\n",
    "\n",
    "ds = DSAraby()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tashaphyne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tashaphyne:** is an Arabic light stemmer and segmentor. It mainly supports light stemming (removng prefixes and suffixes) and gives all possible segmentations. it uses a modified finite state automation, which allows it to generate all segmentations.\n",
    "\n",
    "Source: https://github.com/linuxscout/tashaphyne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from tashaphyne.arabicstopwords import STOPWORDS as TASHAPHYNE_STOPWORDS\n",
    "\n",
    "tashaphyne_stemmer = ArabicLightStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CaMeL Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CaMeL Tools:** is suite of Arabic natural language processing tools developed by the CAMeL Lab at New York University Abu Dhabi.\n",
    "\n",
    "Source: https://github.com/CAMeL-Lab/camel_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mazen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\camel_tools\\cli\\camel_data.py\n"
     ]
    }
   ],
   "source": [
    "import camel_tools\n",
    "\n",
    "from camel_tools.data import downloader\n",
    "from camel_tools.morphology import analyzer\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "# from camel_tools.disambig import CamelDisambiguator\n",
    "# from camel_tools.dialectid import DialectIdentifier\n",
    "from camel_tools.morphology.analyzer import Analyzer\n",
    "from camel_tools.tagger.default import DefaultTagger\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "# from camel_tools.segmenters.word import MaxLikelihoodProbabilityModel\n",
    "# from camel_tools.ner import NERecognizer, STOPWORDS as CAMEL_STOPWORDS\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "\n",
    "camel_data_path = os.path.join(os.path.dirname(camel_tools.__file__), 'cli', 'camel_data.py')\n",
    "print(camel_data_path)\n",
    "\n",
    "downloader.DownloaderError(\"calima-msa-r13\")\n",
    "\n",
    "morph_db = MorphologyDB.builtin_db(flags = 'r')\n",
    "analyzer = Analyzer(morph_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Farasa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Farasa:** is the state-of-the-art library for dealing with Arabic Language Processing. It has been developed by Arabic Language Technologies Group at Qatar Computing Research Institute (QCRI).\n",
    "\n",
    "Source: https://github.com/MagedSaeed/farasapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farasa.stemmer import FarasaStemmer\n",
    "\n",
    "farasa_stemmer = FarasaStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tnkeeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tnkeeh:** is an Arabic preprocessing library for python. it was designe dusing `re` for creating quick replacement expressions for several examples such as Quick cleaning, Segmentation, Normalization and Data splitting.\n",
    "\n",
    "Source: https://github.com/ARBML/tnkeeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tnkeeh as tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK:** a leading platform for building Python programs to work with human language data.\n",
    "\n",
    "Source: https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "NLTK_STOPWORDS = set(stopwords.words('arabic'))\n",
    "\n",
    "nltk_stemmer = ISRIStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SinaTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SinaTools:** an Open-Source Toolkit for Arabic NLP and NLU developed by SinaLab at Birzeit University.\n",
    "\n",
    "Models:\n",
    "- morph: https://sina.birzeit.edu/lemmas_dic.pickle,\n",
    "- ner: https://sina.birzeit.edu/Wj27012000.tar.gz,\n",
    "- wsd_model: https://sina.birzeit.edu/bert-base-arabertv02_22_May_2021_00h_allglosses_unused01.zip,\n",
    "- wsd_tokenizer: https://sina.birzeit.edu/bert-base-arabertv02.zip,\n",
    "- one_gram: https://sina.birzeit.edu/one_gram.pickle,\n",
    "- five_grams: https://sina.birzeit.edu/five_grams.pickle,\n",
    "- four_grams: https://sina.birzeit.edu/four_grams.pickle,\n",
    "- three_grams: https://sina.birzeit.edu/three_grams.pickle,\n",
    "- two_grams: https://sina.birzeit.edu/two_grams.pickle,\n",
    "- graph_l2: https://sina.birzeit.edu/graph_l2.pkl,\n",
    "- graph_l3: https://sina.birzeit.edu/graph_l3.pkl,\n",
    "- relation: https://sina.birzeit.edu/relation_model.zip\n",
    "\n",
    "\n",
    "Source: https://github.com/SinaLab/SinaTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinatools.morphology import morph_analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ü§ó Transformers:** a library that provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.\n",
    "\n",
    "Source: https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-Learn**: a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license.\n",
    "\n",
    "Source: https://github.com/scikit-learn/scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Cleaning</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tidying Up Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Orthographic mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Spelling inconsistencies (Text Correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_correct(dataset: list) -> list:\n",
    "    '''\n",
    "    A method that that fixes typos, punctuation and spelling mistakes.\n",
    "    '''\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for text in dataset:\n",
    "        output = autoco.spell(text)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Unknown characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Repeated letters and with spaces in the words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Reshape Text\n",
    "\n",
    "https://pypi.org/project/arabic-reshaper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems in text collected from youtube/podcast is that their is no true sentence structure is made that we split text upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_sentence_segmentation(paragraph: str) -> str:\n",
    "    '''\n",
    "    A method that segmente arabic pargaraphs to meaningful sentences\n",
    "\n",
    "    @param paragraph: a bunch of sentences that are segmented to meaningful sentences.\n",
    "    '''\n",
    "\n",
    "    # Compute sentence embeddings\n",
    "    embeddings = sbert_model.encode(paragraph)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    # Find semantic breakpoints (low similarity)\n",
    "    threshold = 0.5  # Adjust this based on experimentation\n",
    "    split_points = [i for i in range(len(paragraph) - 1) if sim_matrix[i, i+1] < threshold]\n",
    "\n",
    "    # Generate semantic splits\n",
    "    segments = []\n",
    "    start = 0\n",
    "    for split in split_points:\n",
    "        segments.append(\" \".join(paragraph[start:split+1]))\n",
    "        start = split + 1\n",
    "\n",
    "    segments.append(\" \".join(paragraph[start:]))\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> handle text that contains both english and arabic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Arabizi to Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabizi_to_arabic(text: str) -> str:\n",
    "    '''\n",
    "    A method that gives the possible words in Arabic based on a given word in Latin by mapping\n",
    "    the Latin letters to Arabic ones, then takes the most frequent word existing in a corpus.\n",
    "    \n",
    "    @param text: a sentence containing english (ÿπŸÖŸä) that need to be converted to arabic.\n",
    "    '''\n",
    "\n",
    "    return ds.transliterate(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m arabizi_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmar7aba, kayf 7alak?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m arabic_text \u001b[38;5;241m=\u001b[39m \u001b[43marabizi_to_arabic\u001b[49m\u001b[43m(\u001b[49m\u001b[43marabizi_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArabizi:\u001b[39m\u001b[38;5;124m\"\u001b[39m, arabizi_text)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArabic:\u001b[39m\u001b[38;5;124m\"\u001b[39m, arabic_text)\n",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36marabizi_to_arabic\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marabizi_to_arabic\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    A method that gives the possible words in Arabic based on a given word in Latin by mapping\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    the Latin letters to Arabic ones, then takes the most frequent word existing in a corpus.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    @param text: a sentence containing english (ÿπŸÖŸä) that need to be converted to arabic.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mtransliterate(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "arabizi_text = \"mar7aba, kayf 7alak?\"\n",
    "arabic_text = arabizi_to_arabic(arabizi_text)\n",
    "print(\"Arabizi:\", arabizi_text)\n",
    "print(\"Arabic:\", arabic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Stemming:_**</span> is the process of reducing a word to its root/lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_stemming(text: str, tool: str) -> str:\n",
    "    '''\n",
    "    A method that perform arabic text stemming\n",
    "\n",
    "    @param text: A sentence that requires stemming\n",
    "    '''\n",
    "    zen = TextBlob(text) # check for alternatives\n",
    "    words = zen.words\n",
    "    \n",
    "    if tool == 'camel':\n",
    "        return ' '.join([analyzer.analyze(word)['stem'] for word in words])\n",
    "    elif tool == 'farasa':\n",
    "        return farasa_stemmer.stem(text)\n",
    "    elif tool == \"light\":\n",
    "        return ' '.join([tashaphyne_stemmer.light_stem(word) for word in words])\n",
    "    else:\n",
    "        return ' '.join([nltk_stemmer.stem(word) for word in words])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_Note:_**</span> ISRI Stemmer is a stemming process that is based on algorithm (Arabic Stemming without a root dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Lemmatization:_**</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_lemmatization(text: str) -> str:\n",
    "    '''\n",
    "    A method that perform arabic text lemmatization\n",
    "\n",
    "    @param text: A sentence that requires lemmatization\n",
    "    '''\n",
    "    zen = TextBlob(text) # check for alternatives\n",
    "    words = zen.words\n",
    "\n",
    "    analyzed_text = morph_analyzer.analyze(text)\n",
    "\n",
    "    return ' '.join([word[\"lemma\"] for word in analyzed_text])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Stopwords:_**</span> are most common terms in an Arabic language such as ÿ≠ÿ±ŸàŸÅ ÿßŸÑÿ¨ÿ±."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_stopwords(text: str, custom_stopwords: bool=None, use_nltk: bool=True, use_camel: bool=True, use_tashaphyne: bool=True) -> str:\n",
    "    '''\n",
    "    A method that remove stopwords in text\n",
    "\n",
    "    @param text: a sentence that requires removing stopwords.\n",
    "    '''\n",
    "\n",
    "    # Get Arabic stopwords\n",
    "    stopwords = set()\n",
    "\n",
    "    if use_nltk:\n",
    "        stopwords.update(NLTK_STOPWORDS)\n",
    "    if use_camel:\n",
    "        stopwords.update(CAMEL_STOPWORDS)\n",
    "    if use_tashaphyne:\n",
    "        stopwords.update(TASHAPHYNE_STOPWORDS)\n",
    "    if custom_stopwords:\n",
    "        stopwords.update(custom_stopwords)\n",
    "\n",
    "    stopwords_comp = {\"ÿå\",\"ÿ¢ÿ∂\",\"ÿ¢ŸÖŸäŸÜŸé\",\"ÿ¢Ÿá\",\"ÿ¢ŸáÿßŸã\",\"ÿ¢Ÿä\",\"ÿ£\",\"ÿ£ÿ®\",\"ÿ£ÿ¨ŸÑ\",\"ÿ£ÿ¨ŸÖÿπ\",\"ÿ£ÿÆ\",\n",
    "                    \"ÿ£ÿÆÿ∞\",\"ÿ£ÿµÿ®ÿ≠\",\"ÿ£ÿ∂ÿ≠Ÿâ\",\"ÿ£ŸÇÿ®ŸÑ\",\"ÿ£ŸÇŸÑ\",\"ÿ£ŸÉÿ´ÿ±\",\"ÿ£ŸÑÿß\",\"ÿ£ŸÖ\",\"ÿ£ŸÖÿß\",\n",
    "                    \"ÿ£ŸÖÿßŸÖŸÉ\",\"ÿ£ŸÖÿßŸÖŸÉŸé\",\"ÿ£ŸÖÿ≥Ÿâ\",\"ÿ£ŸÖŸëÿß\",\"ÿ£ŸÜ\",\"ÿ£ŸÜÿß\",\"ÿ£ŸÜÿ™\",\"ÿ£ŸÜÿ™ŸÖ\",\n",
    "                    \"ÿ£ŸÜÿ™ŸÖÿß\",\"ÿ£ŸÜÿ™ŸÜ\",\"ÿ£ŸÜÿ™Ÿê\",\"ÿ£ŸÜÿ¥ÿ£\",\"ÿ£ŸÜŸëŸâ\",\"ÿ£Ÿà\",\"ÿ£Ÿàÿ¥ŸÉ\",\"ÿ£ŸàŸÑÿ¶ŸÉ\",\n",
    "                    \"ÿ£ŸàŸÑÿ¶ŸÉŸÖ\",\"ÿ£ŸàŸÑÿßÿ°\",\"ÿ£ŸàŸÑÿßŸÑŸÉ\",\"ÿ£ŸàŸëŸáŸí\",\"ÿ£Ÿä\",\"ÿ£Ÿäÿß\",\"ÿ£ŸäŸÜ\",\"ÿ£ŸäŸÜŸÖÿß\",\n",
    "                    \"ÿ£ŸäŸë\",\"ÿ£ŸéŸÜŸëŸé\",\"ÿ£ŸéŸéŸäŸëŸè\",\"ÿ£ŸèŸÅŸëŸç\",\"ÿ•ÿ∞\",\"ÿ•ÿ∞ÿß\",\"ÿ•ÿ∞ÿßŸã\",\"ÿ•ÿ∞ŸÖÿß\",\"ÿ•ÿ∞ŸÜ\",\"ÿ•ŸÑŸâ\",\n",
    "                    \"ÿ•ŸÑŸäŸÉŸÖ\",\"ÿ•ŸÑŸäŸÉŸÖÿß\",\"ÿ•ŸÑŸäŸÉŸÜŸë\",\"ÿ•ŸÑŸäŸÉŸé\",\"ÿ•ŸÑŸéŸäŸíŸÉŸé\",\"ÿ•ŸÑŸëÿß\",\"ÿ•ŸÖŸëÿß\",\"ÿ•ŸÜ\",\n",
    "                    \"ÿ•ŸÜŸëŸÖÿß\",\"ÿ•Ÿä\",\"ÿ•ŸäÿßŸÉ\",\"ÿ•ŸäÿßŸÉŸÖ\",\"ÿ•ŸäÿßŸÉŸÖÿß\",\"ÿ•ŸäÿßŸÉŸÜ\",\"ÿ•ŸäÿßŸÜÿß\",\"ÿ•ŸäÿßŸá\",\n",
    "                    \"ÿ•ŸäÿßŸáÿß\",\"ÿ•ŸäÿßŸáŸÖ\",\"ÿ•ŸäÿßŸáŸÖÿß\",\"ÿ•ŸäÿßŸáŸÜ\",\"ÿ•ŸäÿßŸä\",\"ÿ•ŸäŸáŸç\",\"ÿ•ŸêŸÜŸëŸé\",\"ÿß\",\n",
    "                    \"ÿßÿ®ÿ™ÿØÿ£\",\"ÿßÿ´ÿ±\",\"ÿßÿ¨ŸÑ\",\"ÿßÿ≠ÿØ\",\"ÿßÿÆÿ±Ÿâ\",\"ÿßÿÆŸÑŸàŸÑŸÇ\",\"ÿßÿ∞ÿß\",\"ÿßÿ±ÿ®ÿπÿ©\",\n",
    "                    \"ÿßÿ±ÿ™ÿØŸë\",\"ÿßÿ≥ÿ™ÿ≠ÿßŸÑ\",\"ÿßÿ∑ÿßÿ±\",\"ÿßÿπÿßÿØÿ©\",\"ÿßÿπŸÑŸÜÿ™\",\"ÿßŸÅ\",\"ÿßŸÉÿ´ÿ±\",\"ÿßŸÉÿØ\",\n",
    "                    \"ÿßŸÑÿ£ŸÑÿßÿ°\",\"ÿßŸÑÿ£ŸÑŸâ\",\"ÿßŸÑÿß\",\"ÿßŸÑÿßÿÆŸäÿ±ÿ©\",\"ÿßŸÑÿßŸÜ\",\"ÿßŸÑÿßŸàŸÑ\",\"ÿßŸÑÿßŸàŸÑŸâ\",\"ÿßŸÑÿ™Ÿâ\",\n",
    "                    \"ÿßŸÑÿ™Ÿä\",\"ÿßŸÑÿ´ÿßŸÜŸä\",\"ÿßŸÑÿ´ÿßŸÜŸäÿ©\",\"ÿßŸÑÿ∞ÿßÿ™Ÿä\",\"ÿßŸÑÿ∞Ÿâ\",\"ÿßŸÑÿ∞Ÿä\",\"ÿßŸÑÿ∞ŸäŸÜ\",\n",
    "                    \"ÿßŸÑÿ≥ÿßÿ®ŸÇ\",\"ÿßŸÑŸÅ\",\"ÿßŸÑŸÑÿßÿ¶Ÿä\",\"ÿßŸÑŸÑÿßÿ™Ÿä\",\"ÿßŸÑŸÑÿ™ÿßŸÜ\",\"ÿßŸÑŸÑÿ™Ÿäÿß\",\"ÿßŸÑŸÑÿ™ŸäŸÜ\",\n",
    "                    \"ÿßŸÑŸÑÿ∞ÿßŸÜ\",\"ÿßŸÑŸÑÿ∞ŸäŸÜ\",\"ÿßŸÑŸÑŸàÿßÿ™Ÿä\",\"ÿßŸÑŸÖÿßÿ∂Ÿä\",\"ÿßŸÑŸÖŸÇÿ®ŸÑ\",\"ÿßŸÑŸàŸÇÿ™\",\n",
    "                    \"ÿßŸÑŸâ\",\"ÿßŸÑŸäŸàŸÖ\",\"ÿßŸÖÿß\",\"ÿßŸÖÿßŸÖ\",\"ÿßŸÖÿ≥\",\"ÿßŸÜ\",\"ÿßŸÜÿ®ÿ±Ÿâ\",\"ÿßŸÜŸÇŸÑÿ®\",\n",
    "                    \"ÿßŸÜŸá\",\"ÿßŸÜŸáÿß\",\"ÿßŸà\",\"ÿßŸàŸÑ\",\"ÿßŸä\",\"ÿßŸäÿßÿ±\",\"ÿßŸäÿßŸÖ\",\"ÿßŸäÿ∂ÿß\",\"ÿ®\",\n",
    "                    \"ÿ®ÿßÿ™\",\"ÿ®ÿßÿ≥ŸÖ\",\"ÿ®ÿßŸÜ\",\"ÿ®ÿÆŸç\",\"ÿ®ÿ±ÿ≥\",\"ÿ®ÿ≥ÿ®ÿ®\",\"ÿ®ÿ≥Ÿë\",\"ÿ®ÿ¥ŸÉŸÑ\",\"ÿ®ÿ∂ÿπ\",\n",
    "                    \"ÿ®ÿ∑ÿ¢ŸÜ\",\"ÿ®ÿπÿØ\",\"ÿ®ÿπÿ∂\",\"ÿ®ŸÉ\",\"ÿ®ŸÉŸÖ\",\"ÿ®ŸÉŸÖÿß\",\"ÿ®ŸÉŸÜ\",\"ÿ®ŸÑ\",\"ÿ®ŸÑŸâ\",\n",
    "                    \"ÿ®ŸÖÿß\",\"ÿ®ŸÖÿßÿ∞ÿß\",\"ÿ®ŸÖŸÜ\",\"ÿ®ŸÜ\",\"ÿ®ŸÜÿß\",\"ÿ®Ÿá\",\"ÿ®Ÿáÿß\",\"ÿ®Ÿä\",\"ÿ®ŸäÿØ\",\n",
    "                    \"ÿ®ŸäŸÜ\",\"ÿ®Ÿéÿ≥Ÿí\",\"ÿ®ŸéŸÑŸíŸáŸé\",\"ÿ®Ÿêÿ¶Ÿíÿ≥Ÿé\",\"ÿ™ÿßŸÜŸê\",\"ÿ™ÿßŸÜŸêŸÉ\",\"ÿ™ÿ®ÿØŸëŸÑ\",\"ÿ™ÿ¨ÿßŸá\",\"ÿ™ÿ≠ŸàŸëŸÑ\",\n",
    "                    \"ÿ™ŸÑŸÇÿßÿ°\",\"ÿ™ŸÑŸÉ\",\"ÿ™ŸÑŸÉŸÖ\",\"ÿ™ŸÑŸÉŸÖÿß\",\"ÿ™ŸÖ\",\"ÿ™ŸäŸÜŸÉ\",\"ÿ™ŸéŸäŸíŸÜŸê\",\"ÿ™ŸêŸá\",\"ÿ™ŸêŸä\",\n",
    "                    \"ÿ´ŸÑÿßÿ´ÿ©\",\"ÿ´ŸÖ\",\"ÿ´ŸÖŸë\",\"ÿ´ŸÖŸëÿ©\",\"ÿ´ŸèŸÖŸëŸé\",\"ÿ¨ÿπŸÑ\",\"ÿ¨ŸÑŸÑ\",\"ÿ¨ŸÖŸäÿπ\",\"ÿ¨Ÿäÿ±\",\"ÿ≠ÿßÿ±\",\n",
    "                    \"ÿ≠ÿßÿ¥ÿß\",\"ÿ≠ÿßŸÑŸäÿß\",\"ÿ≠ÿßŸä\",\"ÿ≠ÿ™Ÿâ\",\"ÿ≠ÿ±Ÿâ\",\"ÿ≠ÿ≥ÿ®\",\"ÿ≠ŸÖ\",\"ÿ≠ŸàÿßŸÑŸâ\",\"ÿ≠ŸàŸÑ\",\n",
    "                    \"ÿ≠Ÿäÿ´\",\"ÿ≠Ÿäÿ´ŸÖÿß\",\"ÿ≠ŸäŸÜ\",\"ÿ≠ŸäŸëŸé\",\"ÿ≠Ÿéÿ®ŸëŸéÿ∞Ÿéÿß\",\"ÿ≠Ÿéÿ™ŸëŸéŸâ\",\"ÿ≠Ÿéÿ∞ÿßÿ±Ÿê\",\"ÿÆŸÑÿß\",\"ÿÆŸÑÿßŸÑ\",\n",
    "                    \"ÿØŸàŸÜ\",\"ÿØŸàŸÜŸÉ\",\"ÿ∞ÿß\",\"ÿ∞ÿßÿ™\",\"ÿ∞ÿßŸÉ\",\"ÿ∞ÿßŸÜŸÉ\",\"ÿ∞ÿßŸÜŸê\",\"ÿ∞ŸÑŸÉ\",\"ÿ∞ŸÑŸÉŸÖ\",\n",
    "                    \"ÿ∞ŸÑŸÉŸÖÿß\",\"ÿ∞ŸÑŸÉŸÜ\",\"ÿ∞Ÿà\",\"ÿ∞Ÿàÿß\",\"ÿ∞Ÿàÿßÿ™ÿß\",\"ÿ∞Ÿàÿßÿ™Ÿä\",\"ÿ∞Ÿäÿ™\",\"ÿ∞ŸäŸÜŸÉ\",\n",
    "                    \"ÿ∞ŸéŸäŸíŸÜŸê\",\"ÿ∞ŸêŸá\",\"ÿ∞ŸêŸä\",\"ÿ±ÿßÿ≠\",\"ÿ±ÿ¨ÿπ\",\"ÿ±ŸàŸäÿØŸÉ\",\"ÿ±Ÿäÿ´\",\"ÿ±Ÿèÿ®ŸëŸé\",\"ÿ≤Ÿäÿßÿ±ÿ©\",\n",
    "                    \"ÿ≥ÿ®ÿ≠ÿßŸÜ\",\"ÿ≥ÿ±ÿπÿßŸÜ\",\"ÿ≥ŸÜÿ©\",\"ÿ≥ŸÜŸàÿßÿ™\",\"ÿ≥ŸàŸÅ\",\"ÿ≥ŸàŸâ\",\"ÿ≥Ÿéÿßÿ°Ÿé\",\"ÿ≥Ÿéÿßÿ°ŸéŸÖŸéÿß\",\n",
    "                    \"ÿ¥ÿ®Ÿá\",\"ÿ¥ÿÆÿµÿß\",\"ÿ¥ÿ±ÿπ\",\"ÿ¥Ÿéÿ™ŸëŸéÿßŸÜŸé\",\"ÿµÿßÿ±\",\"ÿµÿ®ÿßÿ≠\",\"ÿµŸÅÿ±\",\"ÿµŸáŸç\",\"ÿµŸáŸí\",\n",
    "                    \"ÿ∂ÿØ\",\"ÿ∂ŸÖŸÜ\",\"ÿ∑ÿßŸÇ\",\"ÿ∑ÿßŸÑŸÖÿß\",\"ÿ∑ŸÅŸÇ\",\"ÿ∑ŸéŸÇ\",\"ÿ∏ŸÑŸë\",\"ÿπÿßÿØ\",\"ÿπÿßŸÖ\",\n",
    "                    \"ÿπÿßŸÖÿß\",\"ÿπÿßŸÖÿ©\",\"ÿπÿØÿß\",\"ÿπÿØÿ©\",\"ÿπÿØÿØ\",\"ÿπÿØŸÖ\",\"ÿπÿ≥Ÿâ\",\"ÿπÿ¥ÿ±\",\"ÿπÿ¥ÿ±ÿ©\",\n",
    "                    \"ÿπŸÑŸÇ\",\"ÿπŸÑŸâ\",\"ÿπŸÑŸäŸÉ\",\"ÿπŸÑŸäŸá\",\"ÿπŸÑŸäŸáÿß\",\"ÿπŸÑŸëŸã\",\"ÿπŸÜ\",\"ÿπŸÜÿØ\",\"ÿπŸÜÿØŸÖÿß\",\n",
    "                    \"ÿπŸàÿ∂\",\"ÿπŸäŸÜ\",\"ÿπŸéÿØŸéÿ≥Ÿí\",\"ÿπŸéŸÖŸëŸéÿß\",\"ÿ∫ÿØÿß\",\"ÿ∫Ÿäÿ±\",\"ŸÄ\",\"ŸÅ\",\"ŸÅÿßŸÜ\",\"ŸÅŸÑÿßŸÜ\",\n",
    "                    \"ŸÅŸà\",\"ŸÅŸâ\",\"ŸÅŸä\",\"ŸÅŸäŸÖ\",\"ŸÅŸäŸÖÿß\",\"ŸÅŸäŸá\",\"ŸÅŸäŸáÿß\",\"ŸÇÿßŸÑ\",\"ŸÇÿßŸÖ\",\"ŸÇÿ®ŸÑ\",\n",
    "                    \"ŸÇÿØ\",\"ŸÇÿ∑Ÿë\",\"ŸÇŸÑŸÖÿß\",\"ŸÇŸàÿ©\",\"ŸÉÿ£ŸÜŸëŸÖÿß\",\"ŸÉÿ£ŸäŸÜ\",\"ŸÉÿ£ŸäŸë\",\"ŸÉÿ£ŸäŸëŸÜ\",\"ŸÉÿßÿØ\",\n",
    "                    \"ŸÉÿßŸÜ\",\"ŸÉÿßŸÜÿ™\",\"ŸÉÿ∞ÿß\",\"ŸÉÿ∞ŸÑŸÉ\",\"ŸÉÿ±ÿ®\",\"ŸÉŸÑ\",\"ŸÉŸÑÿß\",\"ŸÉŸÑÿßŸáŸÖÿß\",\"ŸÉŸÑÿ™ÿß\",\n",
    "                    \"ŸÉŸÑŸÖ\",\"ŸÉŸÑŸäŸÉŸÖÿß\",\"ŸÉŸÑŸäŸáŸÖÿß\",\"ŸÉŸÑŸëŸÖÿß\",\"ŸÉŸÑŸëŸéÿß\",\"ŸÉŸÖ\",\"ŸÉŸÖÿß\",\"ŸÉŸä\",\"ŸÉŸäÿ™\",\n",
    "                    \"ŸÉŸäŸÅ\",\"ŸÉŸäŸÅŸÖÿß\",\"ŸÉŸéÿ£ŸéŸÜŸëŸé\",\"ŸÉŸêÿÆ\",\"ŸÑÿ¶ŸÜ\",\"ŸÑÿß\",\"ŸÑÿßÿ™\",\"ŸÑÿßÿ≥ŸäŸÖÿß\",\"ŸÑÿØŸÜ\",\"ŸÑÿØŸâ\",\n",
    "                    \"ŸÑÿπŸÖÿ±\",\"ŸÑŸÇÿßÿ°\",\"ŸÑŸÉ\",\"ŸÑŸÉŸÖ\",\"ŸÑŸÉŸÖÿß\",\"ŸÑŸÉŸÜ\",\"ŸÑŸÉŸÜŸëŸéŸÖÿß\",\"ŸÑŸÉŸä\",\"ŸÑŸÉŸäŸÑÿß\",\n",
    "                    \"ŸÑŸÑÿßŸÖŸÖ\",\"ŸÑŸÖ\",\"ŸÑŸÖÿß\",\"ŸÑŸÖŸëÿß\",\"ŸÑŸÜ\",\"ŸÑŸÜÿß\",\"ŸÑŸá\",\"ŸÑŸáÿß\",\"ŸÑŸà\",\"ŸÑŸàŸÉÿßŸÑÿ©\",\n",
    "                    \"ŸÑŸàŸÑÿß\",\"ŸÑŸàŸÖÿß\",\"ŸÑŸä\",\"ŸÑŸéÿ≥Ÿíÿ™Ÿé\",\"ŸÑŸéÿ≥Ÿíÿ™Ÿè\",\"ŸÑŸéÿ≥Ÿíÿ™ŸèŸÖ\",\"ŸÑŸéÿ≥Ÿíÿ™ŸèŸÖŸéÿß\",\"ŸÑŸéÿ≥Ÿíÿ™ŸèŸÜŸëŸé\",\"ŸÑŸéÿ≥Ÿíÿ™Ÿê\",\n",
    "                    \"ŸÑŸéÿ≥ŸíŸÜŸé\",\"ŸÑŸéÿπŸéŸÑŸëŸé\",\"ŸÑŸéŸÉŸêŸÜŸëŸé\",\"ŸÑŸéŸäŸíÿ™Ÿé\",\"ŸÑŸéŸäŸíÿ≥Ÿé\",\"ŸÑŸéŸäŸíÿ≥Ÿéÿß\",\"ŸÑŸéŸäŸíÿ≥Ÿéÿ™Ÿéÿß\",\"ŸÑŸéŸäŸíÿ≥Ÿéÿ™Ÿí\",\"ŸÑŸéŸäŸíÿ≥ŸèŸàÿß\",\n",
    "                    \"ŸÑŸéŸêÿ≥ŸíŸÜŸéÿß\",\"ŸÖÿß\",\"ŸÖÿßÿßŸÜŸÅŸÉ\",\"ŸÖÿßÿ®ÿ±ÿ≠\",\"ŸÖÿßÿØÿßŸÖ\",\"ŸÖÿßÿ∞ÿß\",\"ŸÖÿßÿ≤ÿßŸÑ\",\"ŸÖÿßŸÅÿ™ÿ¶\",\n",
    "                    \"ŸÖÿßŸäŸà\",\"ŸÖÿ™Ÿâ\",\"ŸÖÿ´ŸÑ\",\"ŸÖÿ∞\",\"ŸÖÿ≥ÿßÿ°\",\"ŸÖÿπ\",\"ŸÖÿπÿßÿ∞\",\"ŸÖŸÇÿßÿ®ŸÑ\",\"ŸÖŸÉÿßŸÜŸÉŸÖ\",\n",
    "                    \"ŸÖŸÉÿßŸÜŸÉŸÖÿß\",\"ŸÖŸÉÿßŸÜŸÉŸÜŸë\",\"ŸÖŸÉÿßŸÜŸéŸÉ\",\"ŸÖŸÑŸäÿßÿ±\",\"ŸÖŸÑŸäŸàŸÜ\",\"ŸÖŸÖÿß\",\"ŸÖŸÖŸÜ\",\"ŸÖŸÜ\",\n",
    "                    \"ŸÖŸÜÿ∞\",\"ŸÖŸÜŸáÿß\",\"ŸÖŸá\",\"ŸÖŸáŸÖÿß\",\"ŸÖŸéŸÜŸí\",\"ŸÖŸêŸÜ\",\"ŸÜÿ≠ŸÜ\",\"ŸÜÿ≠Ÿà\",\"ŸÜÿπŸÖ\",\"ŸÜŸÅÿ≥\",\n",
    "                    \"ŸÜŸÅÿ≥Ÿá\",\"ŸÜŸáÿßŸäÿ©\",\"ŸÜŸéÿÆŸí\",\"ŸÜŸêÿπŸêŸÖŸëÿß\",\"ŸÜŸêÿπŸíŸÖŸé\",\"Ÿáÿß\",\"Ÿáÿßÿ§ŸÖ\",\"ŸáÿßŸÉŸé\",\"ŸáÿßŸáŸÜÿß\",\n",
    "                    \"Ÿáÿ®Ÿë\",\"Ÿáÿ∞ÿß\",\"Ÿáÿ∞Ÿá\",\"ŸáŸÉÿ∞ÿß\",\"ŸáŸÑ\",\"ŸáŸÑŸÖŸëŸé\",\"ŸáŸÑŸëÿß\",\"ŸáŸÖ\",\"ŸáŸÖÿß\",\"ŸáŸÜ\",\n",
    "                    \"ŸáŸÜÿß\",\"ŸáŸÜÿßŸÉ\",\"ŸáŸÜÿßŸÑŸÉ\",\"ŸáŸà\",\"ŸáŸä\",\"ŸáŸäÿß\",\"ŸáŸäÿ™\",\"ŸáŸäŸëÿß\",\"ŸáŸéÿ§ŸÑÿßÿ°\",\n",
    "                    \"ŸáŸéÿßÿ™ÿßŸÜŸê\",\"ŸáŸéÿßÿ™ŸéŸäŸíŸÜŸê\",\"ŸáŸéÿßÿ™ŸêŸá\",\"ŸáŸéÿßÿ™ŸêŸä\",\"ŸáŸéÿ¨Ÿí\",\"ŸáŸéÿ∞ÿß\",\"ŸáŸéÿ∞ÿßŸÜŸê\",\"ŸáŸéÿ∞ŸéŸäŸíŸÜŸê\",\n",
    "                    \"ŸáŸéÿ∞ŸêŸá\",\"ŸáŸéÿ∞ŸêŸä\",\"ŸáŸéŸäŸíŸáŸéÿßÿ™Ÿé\",\"Ÿà\",\"Ÿàÿß\",\"Ÿàÿßÿ≠ÿØ\",\"Ÿàÿßÿ∂ÿßŸÅ\",\"Ÿàÿßÿ∂ÿßŸÅÿ™\",\"ŸàÿßŸÉÿØ\",\n",
    "                    \"ŸàÿßŸÜ\",\"ŸàÿßŸáÿßŸã\",\"ŸàÿßŸàÿ∂ÿ≠\",\"Ÿàÿ±ÿßÿ°ŸéŸÉ\",\"ŸàŸÅŸä\",\"ŸàŸÇÿßŸÑ\",\"ŸàŸÇÿßŸÑÿ™\",\"ŸàŸÇÿØ\",\n",
    "                    \"ŸàŸÇŸÅ\",\"ŸàŸÉÿßŸÜ\",\"ŸàŸÉÿßŸÜÿ™\",\"ŸàŸÑÿß\",\"ŸàŸÑŸÖ\",\"ŸàŸÖŸÜ\",\"ŸÖŸéŸÜ\",\"ŸàŸáŸà\",\"ŸàŸáŸä\",\n",
    "                    \"ŸàŸäŸÉÿ£ŸÜŸë\",\"ŸàŸéŸäŸí\",\"ŸàŸèÿ¥ŸíŸÉŸéÿßŸÜŸéŸé\",\"ŸäŸÉŸàŸÜ\",\"ŸäŸÖŸÉŸÜ\",\"ŸäŸàŸÖ\",\"Ÿëÿ£ŸäŸëÿßŸÜ\"}\n",
    "\n",
    "    zen = TextBlob(text) # TODO: check for alternatives\n",
    "    words = zen.words\n",
    "\n",
    "    return \" \".join([w for w in words if not w in stopwords and not w in stopwords_comp and len(w) >= 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CAMEL_STOPWORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŸáÿ∞ÿß\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŸÖÿ´ÿßŸÑ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÿπŸÑŸâ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÿ•ÿ≤ÿßŸÑÿ©\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŸÉŸÑŸÖÿßÿ™\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÿßŸÑÿ™ŸàŸÇŸÅ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÿ®ÿ¥ŸÉŸÑ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŸÖÿ™ŸÇÿØŸÖ\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m custom_stopwords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŸÖÿ™ŸÇÿØŸÖ\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mremove_arabic_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_stopwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_stopwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered_tokens)\n",
      "Cell \u001b[1;32mIn[54], line 14\u001b[0m, in \u001b[0;36mremove_arabic_stopwords\u001b[1;34m(text, custom_stopwords, use_nltk, use_camel, use_tashaphyne)\u001b[0m\n\u001b[0;32m     12\u001b[0m     stopwords\u001b[38;5;241m.\u001b[39mupdate(NLTK_STOPWORDS)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_camel:\n\u001b[1;32m---> 14\u001b[0m     stopwords\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mCAMEL_STOPWORDS\u001b[49m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_tashaphyne:\n\u001b[0;32m     16\u001b[0m     stopwords\u001b[38;5;241m.\u001b[39mupdate(TASHAPHYNE_STOPWORDS)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CAMEL_STOPWORDS' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "tokens = [\"Ÿáÿ∞ÿß\", \"ŸÖÿ´ÿßŸÑ\", \"ÿπŸÑŸâ\", \"ÿ•ÿ≤ÿßŸÑÿ©\", \"ŸÉŸÑŸÖÿßÿ™\", \"ÿßŸÑÿ™ŸàŸÇŸÅ\", \"ÿ®ÿ¥ŸÉŸÑ\", \"ŸÖÿ™ŸÇÿØŸÖ\"]\n",
    "custom_stopwords = [\"ŸÖÿ™ŸÇÿØŸÖ\"]\n",
    "filtered_tokens = remove_arabic_stopwords(tokens, custom_stopwords=custom_stopwords)\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Handling Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Purpose:* The idea is that arabic text can sometimes contains hashtags as for example \"ŸÖÿ®ÿßÿ±ŸÉ_ÿπŸÑŸäŸÉŸÖ_ÿßŸÑÿ¥Ÿáÿ± ÿ±ÿ®Ÿä ÿßÿ¨ÿπŸÑ ÿ¥Ÿáÿ± ÿ±ŸÖÿ∂ÿßŸÜ ŸÅÿßÿ™ÿ≠ÿ© ÿÆŸäÿ± ŸÑŸÜÿß Ÿàÿ®ÿØÿßŸäÿ© ÿ£ÿ¨ŸÖŸÑ ÿ£ŸÇÿØÿßÿ±ŸÜÿß Ÿàÿ≠ŸÇŸÇ ŸÑŸÜÿß ŸÖÿß ŸÜÿ™ŸÖŸÜŸâ Ÿäÿß ŸÉÿ±ŸäŸÖ#\" which need to be converted to \" ŸÖÿ®ÿßÿ±ŸÉ ÿπŸÑŸäŸÉŸÖ ÿßŸÑÿ¥Ÿáÿ± ÿ±ÿ®Ÿä ÿßÿ¨ÿπŸÑ ÿ¥Ÿáÿ± ÿ±ŸÖÿ∂ÿßŸÜ ŸÅÿßÿ™ÿ≠ÿ©ÿÆŸäÿ± ŸÑŸÜÿß Ÿàÿ®ÿØÿßŸäÿ© ÿ£ÿ¨ŸÖŸÑ ÿ£ŸÇÿØÿßÿ±ŸÜÿß\n",
    "Ÿàÿ≠ŸÇŸÇ ŸÑŸÜÿß ŸÖÿß ŸÜÿ™ŸÖŸÜŸâ Ÿäÿß ŸÉÿ±ŸäŸÖ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_with_hashtag(word: str) -> bool:\n",
    "    '''\n",
    "    A method that checks whether a word starts with a hashtag\n",
    "\n",
    "    @param word: a single word\n",
    "    '''\n",
    "\n",
    "    if word.startswith(\"#\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def split_hashtag_to_words(tag: str) -> list:\n",
    "    '''\n",
    "    A method that convert a hashtag to list of words\n",
    "\n",
    "    @param tag: a hashtag\n",
    "    '''\n",
    "\n",
    "    tag = tag.replace('#', '')\n",
    "    tags = tag.split('_')\n",
    "\n",
    "    if len(tags) > 1:\n",
    "        return tags\n",
    "    \n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def extract_hashtag(text: str) -> list:\n",
    "    '''\n",
    "    A method that removes hashtags from tags in a sentence\n",
    "\n",
    "    @param text: a sentence that contains a hashtag\n",
    "    '''\n",
    "    \n",
    "    hash_list = ([re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\")])\n",
    "    word_list = []\n",
    "\n",
    "    for word in hash_list:\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def clean_arabic_hashtag(text: str) -> str:\n",
    "    '''\n",
    "    A method that replace each tag within a text with it equivalent text separated format\n",
    "\n",
    "    @param text: a sentence that contains a hashtag\n",
    "    '''\n",
    "    \n",
    "    words = text.split()\n",
    "    text = list()\n",
    "\n",
    "    for word in words:\n",
    "        if start_with_hashtag(word):\n",
    "            text.extend(extract_hashtag(word))\n",
    "        else:\n",
    "            text.append(word)\n",
    "    \n",
    "    return \" \".join(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Handling Emojis ü§™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_emojis(text: str, mode: str = 'remove') -> str:\n",
    "    '''\n",
    "    A method that handles emojis.\n",
    "    '''\n",
    "    if mode == 'remove':\n",
    "        return emoji.replace_emoji(text, '')\n",
    "    elif mode == 'description':\n",
    "        return emoji.demojize(text, language='ar')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text_with_emoji = \"ÿ£ŸÜÿß ÿ£ÿ≠ÿ® ÿßŸÑŸÇÿ±ÿßÿ°ÿ© üìö Ÿàÿ£ÿ≥ÿ™ŸÖÿ™ÿπ ÿ®Ÿáÿß ŸÉÿ´Ÿäÿ±ÿßŸã üòä\"\n",
    "text_without_emoji = handle_emojis(text_with_emoji, 'remove')\n",
    "text_with_descriptions = handle_emojis(text_with_emoji, 'description')\n",
    "\n",
    "print(\"Original:\", text_with_emoji)\n",
    "print(\"Without emojis:\", text_without_emoji)\n",
    "print(\"With emoji descriptions:\", text_with_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> Search on how to extract meaning from emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Normalization:_**</span> match digits that have the same writing but different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text: str, tool: str) -> str:\n",
    "    '''\n",
    "    A method that match digits that have same writing but different encodings\n",
    "\n",
    "    @param text: a sentence that requires normalizing its text.\n",
    "    @param tool: determining which library name to utilize in normalizing text.\n",
    "    '''\n",
    "    \n",
    "    if tool == \"tnkeeh\":\n",
    "        normalizer = tn.Tnkeeh(normalize=True)\n",
    "        output = normalizer.clean_raw_text(text)\n",
    "        return output\n",
    "    elif tool == \"camel\":\n",
    "        return normalize_unicode(text)\n",
    "    else:\n",
    "        text = text.strip()\n",
    "        text = re.sub(\"[ÿ•ÿ£Ÿ±ÿ¢ÿß]\", \"ÿß\", text)\n",
    "        text = re.sub(\"Ÿâ\", \"Ÿä\", text)\n",
    "        text = re.sub(\"ÿ§\", \"ÿ°\", text)\n",
    "        text = re.sub(\"ÿ¶\", \"ÿ°\", text)\n",
    "        text = re.sub(\"ÿ©\", \"Ÿá\", text)\n",
    "        text = re.sub(\"⁄Ø\", \"ŸÉ\", text)\n",
    "        text = re.sub(\"⁄§\", \"ŸÅ\", text)\n",
    "        text = re.sub(\"⁄Ü\", \"ÿ¨\", text)\n",
    "        text = re.sub(\"Ÿæ\", \"ÿ®\", text)\n",
    "        text = re.sub(\"⁄ú\", \"ÿ¥\", text)\n",
    "        text = re.sub(\"⁄™\", \"ŸÉ\", text)\n",
    "        text = re.sub(\"⁄ß\", \"ŸÇ\", text)\n",
    "        text = re.sub(\"Ÿ±\", \"ÿß\", text)\n",
    "        noise = re.compile(\"\"\" Ÿë    | # Tashdid\n",
    "                                Ÿé    | # Fatha\n",
    "                                Ÿã    | # Tanwin Fath\n",
    "                                Ÿè    | # Damma\n",
    "                                Ÿå    | # Tanwin Damm\n",
    "                                Ÿê    | # Kasra\n",
    "                                Ÿç    | # Tanwin Kasr\n",
    "                                Ÿí    | # Sukun\n",
    "                                ŸÄ     # Tatwil/Kashida\n",
    "                            \"\"\", re.VERBOSE)\n",
    "        text = re.sub(noise, '', text)\n",
    "        text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Convert repeated characters to single occurrence\n",
    "        return araby.strip_tashkeel(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ÿáÿ∞ÿß ŸÜÿµ ÿ™ÿ¨ÿ±Ÿäÿ®Ÿä Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸä ÿßÿ≠ÿ±ŸÅ ŸÖÿÆÿ™ŸÑŸÅŸá ŸÖÿ´ŸÑ ÿß Ÿà ÿß Ÿà ÿß Ÿà Ÿä Ÿà ŸÅ Ÿà ÿ¨\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "raw_text = \"Ÿáÿ∞ÿß ŸÜÿµ ÿ™ÿ¨ÿ±Ÿäÿ®Ÿä Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ£ÿ≠ÿ±ŸÅ ŸÖÿÆÿ™ŸÑŸÅÿ© ŸÖÿ´ŸÑ ÿ• Ÿà ÿ£ Ÿà ÿ¢ Ÿà Ÿâ Ÿà ⁄§ Ÿà ⁄Ü\"\n",
    "normalized_text = normalize_arabic(raw_text)\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Specific Noise Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Noise Removal:_**</span> extend noise removal to handle more cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_noise(text: str) -> str:\n",
    "    '''\n",
    "    A method that remove specific noise in text such as tatweel, html tags, etc.\n",
    "\n",
    "    :params: **text**: a sentence to be processed\n",
    "    '''\n",
    "\n",
    "    # Remove tatweel\n",
    "    text = re.sub(r'\\u0640', '', text)\n",
    "\n",
    "    # Remove non-Arabic characters\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ÿáÿ∞ÿß ŸÜÿµ ÿ™ÿ¨ÿ±Ÿäÿ®Ÿä ŸÖÿπ ŸÖÿ≥ÿßŸÅÿßÿ™ ÿ≤ÿßÿ¶ÿØÿ©\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "noisy_text = \"ŸáŸéŸÄŸÄŸÄÿ∞ÿß ŸÜŸéŸÄŸÄŸÄÿµŸë <b>ÿ™Ÿéÿ¨ŸíŸÄŸÄÿ±ŸêŸäŸÄŸÄÿ®ŸÄŸêŸÄŸäŸë</b> ŸÖÿπ   ŸÖÿ≥ÿßŸÅÿßÿ™  ÿ≤ÿßÿ¶ÿØÿ©\"\n",
    "clean_text = remove_arabic_noise(noisy_text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Tokenization:_**</span> is the process of breaking a sequence of text into smaller units called tokens, such as words, phrases, symbols, and other elements. For the Arabic language, tokenization is a complex task due to the differences between the written and spoken forms of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_arabic(text: str, method='simple'):\n",
    "    '''\n",
    "    A method that tokenize a sentence.\n",
    "\n",
    "    :params: **text**: a sentence to be tokenized\n",
    "    :params: **method**: either `simple` or `morphological`\n",
    "    '''\n",
    "\n",
    "    if method == 'simple':\n",
    "        return simple_word_tokenize(text)\n",
    "    elif method == 'morphological':\n",
    "        disambiguator = MLEDisambiguator.pretrained() # Load a pre-trained disambiguator\n",
    "        tokenizer = MorphologicalTokenizer(disambiguator) # Create a tokenize\n",
    "        words = tokenizer.tokenize(text) # Tokenize text\n",
    "        return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MorphologicalTokenizer.__init__() missing 1 required positional argument: 'scheme'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŸáÿ∞ÿß ŸÖÿ´ÿßŸÑ ÿπŸÑŸâ ÿ™ŸÇÿ∑Ÿäÿπ ÿßŸÑŸÜÿµ ÿßŸÑÿπÿ±ÿ®Ÿä ÿ®ÿ∑ÿ±ŸäŸÇÿ© ŸÖÿ™ŸÇÿØŸÖÿ©.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m simple_tokens \u001b[38;5;241m=\u001b[39m tokenize_arabic(text, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m morphological_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_arabic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmorphological\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimple tokenization:\u001b[39m\u001b[38;5;124m\"\u001b[39m, simple_tokens)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMorphological tokenization:\u001b[39m\u001b[38;5;124m\"\u001b[39m, morphological_tokens)\n",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m, in \u001b[0;36mtokenize_arabic\u001b[1;34m(text, method)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmorphological\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     19\u001b[0m     disambiguator \u001b[38;5;241m=\u001b[39m MLEDisambiguator\u001b[38;5;241m.\u001b[39mpretrained() \u001b[38;5;66;03m# Load a pre-trained disambiguator\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMorphologicalTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisambiguator\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Create a tokenize\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     words \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text) \u001b[38;5;66;03m# Tokenize text\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words\n",
      "\u001b[1;31mTypeError\u001b[0m: MorphologicalTokenizer.__init__() missing 1 required positional argument: 'scheme'"
     ]
    }
   ],
   "source": [
    "text = \"Ÿáÿ∞ÿß ŸÖÿ´ÿßŸÑ ÿπŸÑŸâ ÿ™ŸÇÿ∑Ÿäÿπ ÿßŸÑŸÜÿµ ÿßŸÑÿπÿ±ÿ®Ÿä ÿ®ÿ∑ÿ±ŸäŸÇÿ© ŸÖÿ™ŸÇÿØŸÖÿ©.\"\n",
    "simple_tokens = tokenize_arabic(text, 'simple')\n",
    "morphological_tokens = tokenize_arabic(text, 'morphological')\n",
    "\n",
    "print(\"Simple tokenization:\", simple_tokens)\n",
    "print(\"Morphological tokenization:\", morphological_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Dediacritization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Dediacritization:_**</span> Dediacritization is the process of removing Arabic diacritical marks. Diacritics increase data sparsity and so most Arabic NLP techniques ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_dediacrition(text: str, method='remove', tool='pyarabic') -> str:\n",
    "    '''\n",
    "    A method that remove arabic diacritical marks.\n",
    "\n",
    "    @param text: a sentence that requires dediacritizating.\n",
    "    @param method: options - 'remove', 'normalize' and 'keep'\n",
    "    @param tool: options - 'pyarabic' or 'camel'\n",
    "    '''\n",
    "\n",
    "    if method == 'remove':\n",
    "        if tool == 'pyarabic':\n",
    "            return araby.strip_diacritics(text)\n",
    "        elif tool == 'camel':\n",
    "            return dediac_ar(text)\n",
    "    elif method == 'normalize':\n",
    "        return araby.normalize_hamza(araby.strip_shadda(text))\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text_with_diacritics = \"ÿßŸÑŸÑŸèŸëÿ∫Ÿéÿ©Ÿè ÿßŸÑÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©Ÿè ÿ¨ŸéŸÖŸêŸäŸÑŸéÿ©Ÿå\"\n",
    "removed_diacritics = arabic_dediacrition(text_with_diacritics, 'remove')\n",
    "normalized_diacritics = arabic_dediacrition(text_with_diacritics, 'normalize')\n",
    "\n",
    "print(\"Original:\", text_with_diacritics)\n",
    "print(\"Removed diacritics:\", removed_diacritics)\n",
    "print(\"Normalized diacritics:\", normalized_diacritics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 Dialect Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Dialects:_**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_dialect(text: str, target: str) -> list:\n",
    "    '''\n",
    "    A method that identifies which city, country or region does a text comes from.\n",
    "\n",
    "    :params: **target**: a sentences that requires identifying its dialect.\n",
    "    '''\n",
    "\n",
    "    did = DialectIdentifier.pretrained() # a pretrained dialect identification system that can distinguish between 25 city dialects as well as Modern Standard Arabic.\n",
    "    # In addition to city dialects, the model provides the results aggregated by region and by country. \n",
    "    # While these agregated results are less fine-grained, they tend to be more accurate.\n",
    "\n",
    "    if target == \"city\":\n",
    "        return did.predict(text, \"city\")\n",
    "    elif target == \"country\":\n",
    "        return did.predict(text, \"country\")\n",
    "    elif target == \"region\":\n",
    "        return did.predit(text, \"region\")\n",
    "    else:\n",
    "        dialect = did.predict(text)\n",
    "        return dialect\n",
    "\n",
    "def normalize_dialect(text, target_dialect='MSA'):\n",
    "    # This is a placeholder function. In practice, you would use more sophisticated\n",
    "    # methods to normalize dialects, which is an active area of research.\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text = \"ÿ¥ŸÑŸàŸÜŸÉ ÿ≠ÿ®Ÿäÿ®Ÿäÿü ÿ¥ÿÆÿ®ÿßÿ±ŸÉ ÿßŸÑŸäŸàŸÖÿü\"\n",
    "dialect = identify_dialect(text)\n",
    "normalized_text = normalize_dialect(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Identified dialect:\", dialect)\n",
    "print(\"Normalized to MSA:\", normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13 Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Punctuation:_**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_punctuations(text: str) -> str:\n",
    "    '''\n",
    "    A method that removes punctuations\n",
    "\n",
    "    :params: text: a sentence to processed.\n",
    "    '''\n",
    "\n",
    "    return re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,ÿå-./:;<=>ÿü?@[\\]^_`{|}~\"\"\"), ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14 Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Named entity recognition:_**</span> find and label named entities like proper nouns, organisations, places, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token in an input sentence, `NERecognizer` outputs a label that indicates the type of named-entity.The system outputs one of the following labels for each token: `'B-LOC'`, `'B-ORG'`, `'B-PERS'`, `'B-MISC'`, `'I-LOC'`, `'I-ORG'`, `'I-PERS'`, `'I-MISC'`, `'O'`.\n",
    "Named-entites can either be a `LOC` (location), `ORG` (organization), `PERS` (person), or `MISC` (miscallaneous).\n",
    "\n",
    "Labels beginning with `B` indicate that their corresponding tokens are the begininging of a multi-word named-entity or is a single-token named-entity'. Those begining with `I` indicate that their corresponding tokens are continuations of a multi-word named-entity. Words that aren't named-entities are given the `'O'` label.\n",
    "\n",
    "The example below illustrates how `NERecognizer` can be used to label named-entities in a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_arabic_entities(text: str):\n",
    "    '''\n",
    "    A method that provides ner for a text such that for each token in an input sentence, the method outputs a label indicates the type of named-entity.\n",
    "    The system outputs one of the following lables for each token `B-LOC`, `B-ORG`, `B-PERS`, `B-MISC`, `I-LOC`, `I-ORG`, `I-PERS`, `I-MISC`, `O`.\n",
    "\n",
    "    Named-entities can either be a `LOC` (location), `ORG` (organization), `PERS` (person), or `MISC` (miscallaneous)\n",
    "\n",
    "    Labels beginning with `B` indicate that their corresponding tokens are the beginning of a multi-word name-entity or is a single token named-entity.\n",
    "    Those begining with `I` indicate that their corresponding tokens are continuations of a multi-word name-entity. Words that aren't named-entities are given the label `O` label.\n",
    "\n",
    "    :params:\n",
    "    text: a sentence that requires named-entity recognition\n",
    "    '''\n",
    "\n",
    "    ner = NERecognizer.pretrained()\n",
    "    labels = ner.predict_sentence(text)\n",
    "    words = simple_word_tokenize(text)\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "    \n",
    "    for word, label in zip(words, labels):\n",
    "        if label.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity), current_label))\n",
    "                current_entity = []\n",
    "            current_entity.append(word)\n",
    "            current_label = label[2:]\n",
    "        elif label.startswith('I-') and current_entity:\n",
    "            current_entity.append(word)\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append((' '.join(current_entity), current_label))\n",
    "                current_entity = []\n",
    "                current_label = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append((' '.join(current_entity), current_label))\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text = \"ŸäÿπŸäÿ¥ ŸÖÿ≠ŸÖÿØ ŸÅŸä ÿßŸÑŸÇÿßŸáÿ±ÿ© ŸàŸäÿπŸÖŸÑ ŸÅŸä ÿ¥ÿ±ŸÉÿ© ÿ¨Ÿàÿ¨ŸÑ.\"\n",
    "entities = recognize_arabic_entities(text)\n",
    "print(\"Text:\", text)\n",
    "print(\"Recognized entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.15 Morphological Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Morphological Analysis:_**</span> is the process of generating all possible readings (analyses) of a given word out of context. All analyses are generated from the undiacritized form of the input word. Each of these analyses is defined by a set lexical and morphological features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_morph_analysis(text: str):\n",
    "    '''\n",
    "    A method that generates all possible readings of a given word out of context.\n",
    "\n",
    "    :params: text: a sentence to be processed.\n",
    "    '''\n",
    "    # First, we need to load a morphological database.\n",
    "    # Here, we load the default database which is used for analyzing\n",
    "    # Modern Standard Arabic. \n",
    "    db = MorphologyDB.builtin_db()\n",
    "\n",
    "    analyzer = Analyzer(db)\n",
    "\n",
    "    analyses = analyzer.analyze(text)\n",
    "\n",
    "    return analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.16 Word Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Word Segmentation:_**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_word_segmentation(text: str) -> str:\n",
    "    '''\n",
    "    A method that segment connected words into a sentence with proper spaces between words.\n",
    "\n",
    "    @param text: a sentence requires segmentation\n",
    "    '''\n",
    "    mlp_model = MaxLikelihoodProbabilityModel.pretrained()\n",
    "    segmented = mlp_model.segment(text)\n",
    "    return ' '.join(segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MaxLikelihoodProbabilityModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŸàŸÇÿßŸÑŸÖÿµÿØÿ±ÿ•ŸÜŸáŸÜÿßŸÉÿ™ÿ≠ÿ≥ŸÜÿßŸÅŸäÿßŸÑŸàÿ∂ÿπ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m segmented_text \u001b[38;5;241m=\u001b[39m \u001b[43marabic_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal:\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmented:\u001b[39m\u001b[38;5;124m\"\u001b[39m, segmented_text)\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36marabic_segmentation\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marabic_segmentation\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    A method that segment connected words into a sentence with proper spaces between words.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    @param text: a sentence requires segmentation\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     mlp_model \u001b[38;5;241m=\u001b[39m \u001b[43mMaxLikelihoodProbabilityModel\u001b[49m\u001b[38;5;241m.\u001b[39mpretrained()\n\u001b[0;32m      8\u001b[0m     segmented \u001b[38;5;241m=\u001b[39m mlp_model\u001b[38;5;241m.\u001b[39msegment(text)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(segmented)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MaxLikelihoodProbabilityModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"ŸàŸÇÿßŸÑŸÖÿµÿØÿ±ÿ•ŸÜŸáŸÜÿßŸÉÿ™ÿ≠ÿ≥ŸÜÿßŸÅŸäÿßŸÑŸàÿ∂ÿπ\"\n",
    "segmented_text = arabic_segmentation(text)\n",
    "print(\"Original:\", text)\n",
    "print(\"Segmented:\", segmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.17 Part-of-speech tagging (POS tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Part-of-speech tagging:_**</span> is the process of determining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_pos_tagging(text: str) -> list:\n",
    "    '''\n",
    "    A method that provides part-of-speech (pos) tagging for arabic text\n",
    "\n",
    "    :params: text: a sentence to be processed\n",
    "    '''\n",
    "\n",
    "    mle = MLEDisambiguator.pretrained()\n",
    "    tagger = DefaultTagger(mle, 'pos')\n",
    "\n",
    "    # The tagger expects pre-tokenized text\n",
    "    sentence = simple_word_tokenize(text)\n",
    "\n",
    "    pos_tags = tagger.tag(sentence)\n",
    "\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.18 Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Disambiguation:_**</span> is the process of determining what is the most likely analysis of a word in a given context. Disambiguation is the backbone for many Arabic NLP tasks such as diacritization, POS tagging and morphological tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_disambiguation(text: str, model: str=\"calima\"):\n",
    "    '''\n",
    "    A method that determines what is the most likely analysis of a word.\n",
    "\n",
    "    :params: **text**: a sentence to be processed\n",
    "    :params: **model**: the name of the model either `calima` or default (Maximum Likelihood Estimation Model)\n",
    "    '''\n",
    "\n",
    "    if model == \"calima\":\n",
    "        disambiguator = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "        disambiguated = disambiguator.disambiguate(text.split())\n",
    "        return [d.analyses[0].analysis['lex'] for d in disambiguated]\n",
    "    else:\n",
    "        mle = MLEDisambiguator.pretrained()\n",
    "        disambig = mle.disambiguate(text)\n",
    "\n",
    "        # For each disambiguated word d in disambig, d.analyses is a list of analyses\n",
    "        # sorted from most likely to least likely. Therefore, d.analyses[0] would\n",
    "        # be the most likely analysis for a given word. Below we extract different\n",
    "        # features from the top analysis of each disambiguated word into seperate lists.\n",
    "        diacritized = [d.analyses[0].analysis['diac'] for d in disambig]\n",
    "        pos_tags = [d.analyses[0].analysis['pos'] for d in disambig]\n",
    "        lemmas = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "\n",
    "        return diacritized, pos_tags, lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text = \"ÿ∞Ÿáÿ® ÿßŸÑÿ±ÿ¨ŸÑ ÿ•ŸÑŸâ ÿßŸÑÿ®ŸÜŸÉ\"\n",
    "diacritized, pos_tags, lemmas = arabic_disambiguation(text)\n",
    "print(\"Original:\", text)\n",
    "print(\"Diacritized:\", diacritized)\n",
    "print(\"POS tags:\", pos_tags)\n",
    "print(\"Lemmas:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.19 Elongated Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Elongated Words:_**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_elongated_words(text: str) -> str:\n",
    "    '''\n",
    "    A method that removes word elongation.\n",
    "\n",
    "    :params: **text**: a sentence to be processed\n",
    "    '''\n",
    "\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "elongated_text = \"Ÿäÿßÿßÿßÿßÿß ÿ≥ŸÑÿßÿßÿßÿßŸÖ ÿπŸÑŸâ Ÿáÿ∞ÿß ÿßŸÑÿ®ÿ±ŸÜÿßÿßÿßŸÖÿ¨ ÿßŸÑÿ±ÿßÿßÿßÿßÿ¶ÿπ\"\n",
    "normalized_text = normalize_elongated_words(elongated_text)\n",
    "print(\"Elongated:\", elongated_text)\n",
    "print(\"Normalized:\", normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.20 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Data Augmentation:_**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_arabic_data(text: str, num_augmentations: int=1) -> str:\n",
    "    '''\n",
    "    A method that changes the form of each word in text.\n",
    "\n",
    "    :params: **text**: a string to be process\n",
    "    :params: **num_augmentations**: number of times to perform augmentations\n",
    "    '''\n",
    "    morph = analyzer.pretrained_analyzer()\n",
    "    words = text.split()\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            analysis = morph.analyze(word)\n",
    "            if analysis:\n",
    "                # Randomly choose a different form of the word\n",
    "                new_word = random.choice(analysis).inflected\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        augmented_texts.append(' '.join(new_words))\n",
    "\n",
    "    return augmented_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "original_text = \"ÿßŸÑŸÉÿ™ÿßÿ® ŸÖŸÅŸäÿØ ŸÑŸÑŸÇÿ±ÿßÿ°ÿ©\"\n",
    "augmented_data = augment_arabic_data(original_text, num_augmentations=3)\n",
    "\n",
    "print(\"Original:\", original_text)\n",
    "print(\"Augmented data:\")\n",
    "for i, text in enumerate(augmented_data, 1):\n",
    "    print(f\"{i}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.21 Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Generation:_**</span> is the process of inflecting a lemma for a set of morphological features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_word_generation(word: str, pos: str = 'noun', gen: str = 'm', num: str = 'p'):\n",
    "    '''\n",
    "    A method that inflects the lemmma of a word for a set of morphological features\n",
    "    '''\n",
    "\n",
    "    # We need to indicate that the database we are loading will be\n",
    "    # used for generation.\n",
    "    db = MorphologyDB.builtin_db(flags='g')\n",
    "\n",
    "    generator = Generator(db)\n",
    "\n",
    "    # get lemma of a word\n",
    "    lemma = word\n",
    "    features = {\n",
    "        'pos': pos,\n",
    "        'gen': gen,\n",
    "        'num': num\n",
    "    }\n",
    "\n",
    "    analyses = generator.generate(lemma, features)\n",
    "\n",
    "    # Extract and print unique diacritizations from generated analyses\n",
    "    return set([a['diac'] for a in analyses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_Note:_**</span> `'pos'` is the only *required* feature that needs to be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.22 Reinflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Reinflection:_**</span> is the process of converting a given word in any form to a different form (i.e. tense, gender, etc). The CAMeL Tools reinflector works similar to the generator except that the word doesn't have to be a lemma and it is not have to be restricted to a specific `'pos'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_reinflection(word: str, num: str = 'd', prc1: str = 'bi_prep') -> set: \n",
    "    '''\n",
    "    A method that converts a given word in any different form (i.e. tense, gender, etc).\n",
    "    '''\n",
    "\n",
    "    # We need to indicate that the database we are loading will be\n",
    "    # used for reinflection.\n",
    "    db = MorphologyDB.builtin_db(flags='r')\n",
    "\n",
    "    reinflector = Reinflector(db)\n",
    "\n",
    "    features = {\n",
    "        'num': num,\n",
    "        'prc1': prc1\n",
    "    }\n",
    "\n",
    "    analyses = reinflector.reinflect(word, features)\n",
    "\n",
    "    # Extract and print unique diacritizations from reinflected analyses\n",
    "    return set(a['diac'] for a in analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handling Very Common Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_common_words(df: pd.DataFrame, mode='remove'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handling Very Rare Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_rare_words(df: pd.DataFrame, mode='remove'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handling Numbers and Special Characters in Arabic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_numbers_and_special_chars(text, mode='remove'):\n",
    "    '''\n",
    "    A method that either replace arabic numbers to hindi numbers or remove them.\n",
    "    '''\n",
    "    if mode == 'remove':\n",
    "        # Remove numbers and special characters\n",
    "        return re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    elif mode == 'normalize':\n",
    "        # Normalize Arabic numbers to Hindi numbers\n",
    "        number_map = {\n",
    "            'Ÿ†': '0', 'Ÿ°': '1', 'Ÿ¢': '2', 'Ÿ£': '3', 'Ÿ§': '4',\n",
    "            'Ÿ•': '5', 'Ÿ¶': '6', 'Ÿß': '7', 'Ÿ®': '8', 'Ÿ©': '9'\n",
    "        }\n",
    "        \n",
    "        for arabic, hindi in number_map.items():\n",
    "            text = text.replace(arabic, hindi)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text = \"ŸäŸàÿ¨ÿØ Ÿ£ ÿ™ŸÅÿßÿ≠ÿßÿ™ ŸàŸ• ÿ®ÿ±ÿ™ŸÇÿßŸÑÿßÿ™ ŸÅŸä ÿßŸÑÿ≥ŸÑÿ©!\"\n",
    "removed_numbers = handle_numbers_and_special_chars(text, 'remove')\n",
    "normalized_numbers = handle_numbers_and_special_chars(text, 'normalize')\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Removed numbers and special chars:\", removed_numbers)\n",
    "print(\"Normalized numbers:\", normalized_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Feature Engineering</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Preporcessing</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_arabic_text(text: str, model_name: str =\"aubmindlab/bert-base-arabertv2\"):\n",
    "    '''\n",
    "    A method that classify text\n",
    "\n",
    "    :params: **text**:\n",
    "    :params: **model_name**:\n",
    "    '''\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return predictions.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mŸáÿ∞ÿß ÿßŸÑŸÜÿµ ÿ±ÿßÿ¶ÿπ ŸàŸÖŸÅŸäÿØ ÿ¨ÿØÿßŸã\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m classification \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_arabic_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification probabilities: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassification\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mclassify_arabic_text\u001b[1;34m(text, model_name)\u001b[0m\n\u001b[0;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m----> 7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Ÿáÿ∞ÿß ÿßŸÑŸÜÿµ ÿ±ÿßÿ¶ÿπ ŸàŸÖŸÅŸäÿØ ÿ¨ÿØÿßŸã\"\n",
    "classification = classify_arabic_text(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Classification probabilities: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_arabic_sentiment(text):\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\")\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    return result['label'], result['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analyze_arabic_sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÿ£ŸÜÿß ÿ≥ÿπŸäÿØ ÿ¨ÿØÿßŸã ÿ®Ÿáÿ∞ÿß ÿßŸÑŸÖŸÜÿ™ÿ¨!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m sentiment, score \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_arabic_sentiment\u001b[49m(text)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'analyze_arabic_sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"ÿ£ŸÜÿß ÿ≥ÿπŸäÿØ ÿ¨ÿØÿßŸã ÿ®Ÿáÿ∞ÿß ÿßŸÑŸÖŸÜÿ™ÿ¨!\"\n",
    "sentiment, score = analyze_arabic_sentiment(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Sentiment: {sentiment}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Word Embedding:_**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_word_embedding(text: str):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Label Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Topic Modeling:_**</span> is an unsupervised machine learning technique for finding abstract topics in a large collection of documents. It helps in organizing, understanding and summarizing large collections of textual information and discovering the latent topics that vary among documents in a given corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet allocation (LDA) and Non-Negative Matrix Fatorization (NMF) are two of the most popular topic modeling techniques. LDA uses a probabilistic approach whereas NMF uses matrix factorization approach, however, new techniques that are based on BERT for topic modeling do exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Translate to English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> Translate Arabic to English and perform natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Visualize Data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ## Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,ÿå-./:;<=>ÿü?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuation\n",
    "    \n",
    "    ## Remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    ## Remove Emojis\n",
    "    text = remove_emoji(text)\n",
    "\n",
    "    ## Convert text to lowercases\n",
    "    text = text.lower()\n",
    "\n",
    "    ## Arabisy the text\n",
    "    text = to_arabic(text)\n",
    "\n",
    "    ## Remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "\n",
    "    ## Remove numbers\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "\n",
    "    ## Remove Tashkeel\n",
    "    text = normalizeArabic(text)\n",
    "\n",
    "    #text = re.sub('\\W+', ' ', text)\n",
    "    text = re.sub('[A-Za-z]+',' ',text)\n",
    "    text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+',' ',text)\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    #Stemming\n",
    "    #text = stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Resources</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BERT for Arabic Topic Modeling: An Experimental Study on BERTopic Technique (https://github.com/iwan-rg/Arabic-Topic-Modeling?tab=readme-ov-file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
