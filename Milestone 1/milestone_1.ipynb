{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Setup</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import typing\n",
    "import random\n",
    "import emoji\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(file_path: str) -> list:\n",
    "    '''\n",
    "    process youtube/podcast documents\n",
    "\n",
    "    @param file_path: a string represent file path\n",
    "    '''\n",
    "\n",
    "    with open(file_path, 'r', errors=\"ignore\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Remove numbers followed by ':'\n",
    "    text = re.sub(r'\\d+.*\\d*\\s*:', '', text)\n",
    "\n",
    "    # Define sentence delimiters for Arabic\n",
    "    sentence_endings = r'(?<=[.!ØŸØ›ØŒ])\\s+'\n",
    "\n",
    "    # Split sentences while preserving dependencies\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 1] # Remove empty strings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_df(sentences: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    convert a list of sentences to a dataframe\n",
    "\n",
    "    @param sentences: a list of sentences\n",
    "    '''\n",
    "\n",
    "    return pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path: str = \"./data\") -> pd.DataFrame:\n",
    "    '''\n",
    "    Load dataset from a directory\n",
    "\n",
    "    :params: **file_path**: a string representing file path to dataset.\n",
    "    '''\n",
    "    output_df = []\n",
    "\n",
    "    data_folder = \"./data\"\n",
    "\n",
    "    for file_name in os.listdir(data_folder):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(data_folder, file_name)\n",
    "            tmp_df = sentences_to_df(extract_sentences(file_path))\n",
    "            output_df.append(tmp_df)\n",
    "\n",
    "    output_df = pd.concat(output_df, ignore_index=True)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                      0\n",
       "0                Ù„ÙƒÙ„ Ø´Ø®Øµ ÙÙŠ Ø¢Ø®Ø± Ø§Ù„Ø´Ù‡Ø±ØŒ\n",
       "1               Ù…Ø§ Ù…Ø¹Ø§Ù‡ÙˆØ´ ØºÙŠØ± 50 Ø¬Ù†ÙŠÙ‡ØŒ\n",
       "2           ÙˆÙ…Ø­ØªØ§Ø¬ ÙŠØ§ÙƒÙ„ Ø£ÙƒÙ„Ø© ØªØ´Ø¨Ù‘Ø¹Ù‡...\n",
       "3                    Ù„ÙƒÙ„ ÙˆØ§Ø­Ø¯ \"ÙÙˆØ±Ù…Ø©\"ØŒ\n",
       "4              Ù…Ø­ØªØ§Ø¬ Ø£ÙƒÙ„Ø© Ø³Ø±ÙŠØ¹Ø© Ø§Ù„Ù‡Ø¶Ù…ØŒ\n",
       "..                                 ...\n",
       "733          Ù„Ø§Ø²Ù… Ø§Ù„Ø³Ù†Ø¯ÙˆØªØ´ Ù…Ø¹Ø§Ù‡ Ø¨Ø·Ø§Ø·Ø³.\n",
       "734              ÙˆØ£Ù‚ÙˆÙ… Ù…Ø·Ù„Ù‘Ø¹ Ø¨Ø·Ø§Ø·Ø³Ø§ÙŠØ©ØŒ\n",
       "735          ÙˆØ£Ù‚ÙˆÙ… Ø­Ø§Ø·Ø·Ù‡Ø§ ÙÙŠ Ø§Ù„Ø³Ù†Ø¯ÙˆØªØ´!\n",
       "736  Ø¯Ø§ Ø£Ù†Ø§ Ø£Ø­ÙŠØ§Ù†Ù‹Ø§ Ø¨Ø¬ÙŠØ¨ Ø³Ù†Ø¯ÙˆØªØ´ Ø¨Ø·Ø§Ø·Ø³ØŒ\n",
       "737    Ù…Ù† \"Ø§Ù„Ø­Ø±Ù…ÙŠÙ†\" Ø§Ù„Ù„ÙŠ ÙÙŠ \"Ø§Ù„Ø­ÙØµØ±ÙŠ\".\n",
       "\n",
       "[738 rows x 1 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./data/Ø§Ù„Ø¨Ø·Ø§Ø·Ø³  Ø§Ù„Ø¯Ø­ÙŠØ­.txt\"\n",
    "df = load_dataset(file_path)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Libraries & Models</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ghalatawi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ghalatawi:** an Arabic Autocorrect library Ù…ÙƒØªØ¨Ø© Ù„Ù„ØªØµØ­ÙŠØ­ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n",
    "\n",
    "Source: https://github.com/linuxscout/ghalatawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regex': True, 'wordlist': True, 'punct': True, 'typo': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ghalatawi.autocorrector import AutoCorrector\n",
    "\n",
    "autoco = AutoCorrector()\n",
    "\n",
    "autoco.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_Note:_**</span> The library allow for fixing spelling, adjusting punctuations, typos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyArabic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyArabic:** a Arabic language library for Python, provides basic functions to manipulate Arabic letters and text, like detecting Arabic letters, Arabic letters groups and characteristics, remove diacritics etc.\n",
    "\n",
    "Source: https://github.com/linuxscout/pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CAMeL Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CAMeL Bert:** a collection of BERT models pre-trained on Arabic texts with different sizes and variants.\n",
    "\n",
    "Source: https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-msa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name CAMeL-Lab/bert-base-arabic-camelbert-msa. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# Load Arabic SBERT Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model = SentenceTransformer(\"CAMeL-Lab/bert-base-arabic-camelbert-msa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DSAraby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DSAraby:** is a library that aims to transliterate text which is to write a word using the closest corresponding letters of a different alphabet or language.\n",
    "\n",
    "Source: https://github.com/saobou/DSAraby/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsaraby import DSAraby\n",
    "\n",
    "ds = DSAraby()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tashaphyne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tashaphyne:** is an Arabic light stemmer and segmentor. It mainly supports light stemming (removng prefixes and suffixes) and gives all possible segmentations. it uses a modified finite state automation, which allows it to generate all segmentations.\n",
    "\n",
    "Source: https://github.com/linuxscout/tashaphyne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from tashaphyne.arabicstopwords import STOPWORDS as TASHAPHYNE_STOPWORDS\n",
    "\n",
    "tashaphyne_stemmer = ArabicLightStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CaMeL Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CaMeL Tools:** is suite of Arabic natural language processing tools developed by the CAMeL Lab at New York University Abu Dhabi.\n",
    "\n",
    "Source: https://github.com/CAMeL-Lab/camel_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mazen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\camel_tools\\cli\\camel_data.py\n"
     ]
    }
   ],
   "source": [
    "import camel_tools\n",
    "\n",
    "from camel_tools.data import downloader\n",
    "from camel_tools.ner import NERecognizer\n",
    "from camel_tools.morphology import analyzer\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "# from camel_tools.dialectid import DIDPred\n",
    "from camel_tools.morphology.analyzer import Analyzer\n",
    "from camel_tools.tagger.default import DefaultTagger\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "from camel_tools.morphology.reinflector import Reinflector\n",
    "from camel_tools.morphology.generator import Generator\n",
    "\n",
    "camel_data_path = os.path.join(os.path.dirname(camel_tools.__file__), 'cli', 'camel_data.py')\n",
    "print(camel_data_path)\n",
    "\n",
    "downloader.DownloaderError(\"calima-msa-r13\")\n",
    "\n",
    "morph_db = MorphologyDB.builtin_db(flags = 'r')\n",
    "analyzer = Analyzer(morph_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Farasa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Farasa:** is the state-of-the-art library for dealing with Arabic Language Processing. It has been developed by Arabic Language Technologies Group at Qatar Computing Research Institute (QCRI).\n",
    "\n",
    "Source: https://github.com/MagedSaeed/farasapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farasa.stemmer import FarasaStemmer\n",
    "\n",
    "farasa_stemmer = FarasaStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tnkeeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tnkeeh:** is an Arabic preprocessing library for python. it was designe dusing `re` for creating quick replacement expressions for several examples such as Quick cleaning, Segmentation, Normalization and Data splitting.\n",
    "\n",
    "Source: https://github.com/ARBML/tnkeeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tnkeeh as tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK:** a leading platform for building Python programs to work with human language data.\n",
    "\n",
    "Source: https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\mazen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\mazen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mazen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "NLTK_STOPWORDS = set(stopwords.words('arabic'))\n",
    "nltk_stemmer = ISRIStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SinaTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SinaTools:** an Open-Source Toolkit for Arabic NLP and NLU developed by SinaLab at Birzeit University.\n",
    "\n",
    "Models:\n",
    "- morph: https://sina.birzeit.edu/lemmas_dic.pickle,\n",
    "- ner: https://sina.birzeit.edu/Wj27012000.tar.gz,\n",
    "- wsd_model: https://sina.birzeit.edu/bert-base-arabertv02_22_May_2021_00h_allglosses_unused01.zip,\n",
    "- wsd_tokenizer: https://sina.birzeit.edu/bert-base-arabertv02.zip,\n",
    "- one_gram: https://sina.birzeit.edu/one_gram.pickle,\n",
    "- five_grams: https://sina.birzeit.edu/five_grams.pickle,\n",
    "- four_grams: https://sina.birzeit.edu/four_grams.pickle,\n",
    "- three_grams: https://sina.birzeit.edu/three_grams.pickle,\n",
    "- two_grams: https://sina.birzeit.edu/two_grams.pickle,\n",
    "- graph_l2: https://sina.birzeit.edu/graph_l2.pkl,\n",
    "- graph_l3: https://sina.birzeit.edu/graph_l3.pkl,\n",
    "- relation: https://sina.birzeit.edu/relation_model.zip\n",
    "\n",
    "\n",
    "Source: https://github.com/SinaLab/SinaTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinatools.morphology import morph_analyzer\n",
    "from sinatools.utils import text_transliteration\n",
    "from sinatools.synonyms.synonyms_generator import evaluate_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use different python versions (other than 3.12)\n",
    "# from sinatools.ner.entity_extractor import extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ¤— Transformers:** a library that provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.\n",
    "\n",
    "Source: https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-Learn**: a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license.\n",
    "\n",
    "Source: https://github.com/scikit-learn/scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Arabert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arabert**: is an Arabic pretrained language model based on Google's BERT architecture.\n",
    "\n",
    "Source: https://huggingface.co/aubmindlab/bert-base-arabertv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arabert.preprocess import ArabertPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Cleaning</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tidying Up Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Orthographic mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Spelling inconsistencies (Text Correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_correct(text: str) -> str:\n",
    "    '''\n",
    "    A method that that fixes typos, punctuation and spelling mistakes.\n",
    "    '''\n",
    "    autoco = AutoCorrector()\n",
    "\n",
    "    autoco.show_config()\n",
    "\n",
    "    output = autoco.spell(text)\n",
    "\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ø§Ø³ØªØ¹Ø§Ø±Ø© ÙƒØªØ§Ø¨ØŒ Ø§Ø°Ù‡Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø£Ùˆ Ø§Ù„Ø§Ø¯Ø§Ø±Ø© ÙÙŠ Ø§Ù„Ø¸Ù‡ÙŠØ±Ø©.\n"
     ]
    }
   ],
   "source": [
    "text = \"Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ø¥Ø³ØªØ¹Ø§Ø±Ø© ÙƒØªØ§Ø¨ØŒ Ø§Ø°Ù‡Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø£Ùˆ Ø§Ù„Ø§Ø¯Ø§Ø±Ø© ÙÙŠ Ø§Ù„Ø¶Ù‡ÙŠØ±Ø©.\"\n",
    "auto_correct(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø³Ù†Ù‚Ø± Ù„Ø§ ØªØ³Ø±Ùƒ\n"
     ]
    }
   ],
   "source": [
    "text = 'Ø³Ù†Ù‚Ø± Ù„Ø§ ØªØ³Ø±Ùƒ'\n",
    "auto_correct(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_OBSERVATION:_**</span> The library does not always fix typos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Unknown characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Repeated letters and with spaces in the words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Reshape Text\n",
    "\n",
    "https://pypi.org/project/arabic-reshaper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_NOTE:_**</span> One of the problems in text collected from youtube/podcast is that their is no true sentence structure is made that we split text upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_sentence_segmentation(paragraph: str) -> str:\n",
    "    '''\n",
    "    A method that segmente arabic pargaraphs to meaningful sentences\n",
    "\n",
    "    @param paragraph: a bunch of sentences that are segmented to meaningful sentences.\n",
    "    '''\n",
    "\n",
    "    # Compute sentence embeddings\n",
    "    embeddings = sbert_model.encode(paragraph)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    # Find semantic breakpoints (low similarity)\n",
    "    threshold = 0.5  # Adjust this based on experimentation\n",
    "    split_points = [i for i in range(len(paragraph) - 1) if sim_matrix[i, i+1] < threshold]\n",
    "\n",
    "    # Generate semantic splits\n",
    "    segments = []\n",
    "    start = 0\n",
    "    for split in split_points:\n",
    "        segments.append(\" \".join(paragraph[start:split+1]))\n",
    "        start = split + 1\n",
    "\n",
    "    segments.append(\" \".join(paragraph[start:]))\n",
    "\n",
    "    # Remove empty strings\n",
    "    segments = [seg for seg in segments if seg.strip()]\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nÙ„ÙƒÙ„ Ø´Ø®Øµ ÙÙŠ Ø¢Ø®Ø± Ø§Ù„Ø´Ù‡Ø±ØŒ', 'Ù…Ø§ Ù…Ø¹Ø§Ù‡ÙˆØ´ ØºÙŠØ± 50 Ø¬Ù†ÙŠÙ‡ØŒ', 'Ù„ÙƒÙ„ ÙˆØ§Ø­Ø¯ Ù…Ø´ Ø¹Ø§Ø±Ù ÙŠØ§ÙƒÙ„ØŒ', 'ÙˆÙ†ÙØ³Ù‡ ÙÙŠ Ø£ÙƒÙ„Ø© Ø¬Ø§Ù†Ø¨ÙŠØ© Ù…Ø¹ Ø§Ù„Ø£ÙƒÙ„ØŒ', 'Ø§Ù„Ù‚Ø§Ù‡Ø±Ù‡ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±ØŒ', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nÙ„ÙƒÙ„ Ø´Ø®Øµ ÙÙŠ Ø¢Ø®Ø± Ø§Ù„Ø´Ù‡Ø±ØŒ Ù…Ø§ Ù…Ø¹Ø§Ù‡ÙˆØ´ ØºÙŠØ± 50 Ø¬Ù†ÙŠÙ‡ØŒ Ù„ÙƒÙ„ ÙˆØ§Ø­Ø¯ Ù…Ø´ Ø¹Ø§Ø±Ù ÙŠØ§ÙƒÙ„ØŒ ÙˆÙ†ÙØ³Ù‡ ÙÙŠ Ø£ÙƒÙ„Ø© Ø¬Ø§Ù†Ø¨ÙŠØ© Ù…Ø¹ Ø§Ù„Ø£ÙƒÙ„ØŒ',\n",
       " 'Ø§Ù„Ù‚Ø§Ù‡Ø±Ù‡ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±ØŒ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "Ù„ÙƒÙ„ Ø´Ø®Øµ ÙÙŠ Ø¢Ø®Ø± Ø§Ù„Ø´Ù‡Ø±ØŒ\n",
    "Ù…Ø§ Ù…Ø¹Ø§Ù‡ÙˆØ´ ØºÙŠØ± 50 Ø¬Ù†ÙŠÙ‡ØŒ\n",
    "Ù„ÙƒÙ„ ÙˆØ§Ø­Ø¯ Ù…Ø´ Ø¹Ø§Ø±Ù ÙŠØ§ÙƒÙ„ØŒ\n",
    "ÙˆÙ†ÙØ³Ù‡ ÙÙŠ Ø£ÙƒÙ„Ø© Ø¬Ø§Ù†Ø¨ÙŠØ© Ù…Ø¹ Ø§Ù„Ø£ÙƒÙ„ØŒ\n",
    "Ø§Ù„Ù‚Ø§Ù‡Ø±Ù‡ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±ØŒ\n",
    "\"\"\"\n",
    "\n",
    "# Define sentence delimiters for Arabic\n",
    "sentence_endings = r'(?<=[.!ØŸØ›ØŒ])\\s+'\n",
    "\n",
    "# Split sentences while preserving dependencies\n",
    "sentences = re.split(sentence_endings, paragraph)\n",
    "print(sentences)\n",
    "\n",
    "arabic_sentence_segmentation(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> handle text that contains both english and arabic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Arabizi to Arabic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Arabizi:_**</span> is a sentence that contain Latin words (3ami)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabizi_to_arabic(text: str) -> tuple:\n",
    "    '''\n",
    "    A method that gives the possible words in Arabic based on a given word in Latin by mapping\n",
    "    the Latin letters to Arabic ones, then takes the most frequent word existing in a corpus.\n",
    "    \n",
    "    :param text: A sentence containing Arabizi (e.g., \"3ami\") that needs to be converted to Arabic.\n",
    "\n",
    "    :return: A tuple of two values: \n",
    "        - The transliterated text based on the given schema. \n",
    "        - A boolean flag indicating whether all characters in the input text were successfully transliterated or not.\n",
    "    '''\n",
    "\n",
    "    transliterate_text = text_transliteration.perform_transliteration(text, \"bw2ar\")[0]\n",
    "\n",
    "    conversion_dict = {\n",
    "        'a': 'Ø§', 'b': 'Ø¨', 't': 'Øª', 'th': 'Ø«', 'g': 'Ø¬', '7': 'Ø­', 'kh': 'Ø®',\n",
    "        'd': 'Ø¯', 'dh': 'Ø°', 'r': 'Ø±', 'z': 'Ø²', 's': 'Ø³', 'sh': 'Ø´', '9': 'Øµ',\n",
    "        '6': 'Ø·', '3': 'Ø¹', 'gh': 'Øº', 'f': 'Ù', 'q': 'Ù‚', 'k': 'Ùƒ', 'l': 'Ù„',\n",
    "        'm': 'Ù…', 'n': 'Ù†', 'h': 'Ù‡', 'w': 'Ùˆ', 'y': 'ÙŠ', '?': \"ØŸ\"\n",
    "    }\n",
    "    \n",
    "    for latin, arabic in conversion_dict.items():\n",
    "        transliterate_text = transliterate_text.replace(latin, arabic)\n",
    "\n",
    "    transliterate_text = ds.transliterate(transliterate_text)\n",
    "\n",
    "    # Check if all characters in the result are Arabic\n",
    "    valid_arabic_regex = re.compile(r'^[\\u0600-\\u06FF\\s.,ØŒØŸ!Ø›]+$')\n",
    "    transliterate_successed = all(valid_arabic_regex.match(char) for char in transliterate_text)\n",
    "\n",
    "    return transliterate_text, transliterate_successed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabizi: mar7aba, kayf 7alak?\n",
      "Arabic: ('Ù…ÙŽØ±Ø­ÙŽØ¨ÙŽÛ¥ ÙƒÙŽÙŠÙ Ø­ÙŽÙ„ÙŽÙƒØŸ', True)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "arabizi_text = \"mar7aba, kayf 7alak?\"\n",
    "arabic_text = arabizi_to_arabic(arabizi_text)\n",
    "\n",
    "print(\"Arabizi:\", arabizi_text)\n",
    "print(\"Arabic:\", arabic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Stemming:_**</span> is the process of reducing a word to its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_stemming(text: str, tool: str) -> str:\n",
    "    '''\n",
    "    A method that perform arabic text stemming\n",
    "\n",
    "    @param text: A sentence that requires stemming\n",
    "    '''\n",
    "    zen = TextBlob(text) # check for alternatives\n",
    "    words = zen.words\n",
    "    \n",
    "    if tool == 'camel':\n",
    "        return ' '.join([analyzer.analyze(word)[0]['stem'] for word in words])\n",
    "    elif tool == 'farasa':\n",
    "        return farasa_stemmer.stem(text)\n",
    "    elif tool == \"light\":\n",
    "        return ' '.join([tashaphyne_stemmer.light_stem(word) for word in words])\n",
    "    else:\n",
    "        return ' '.join([nltk_stemmer.stem(word) for word in words])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_Note:_**</span> ISRI Stemmer is a stemming process that is based on algorithm (Arabic Stemming without a root dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø°ÙŽÙ‡Ù‘ÙØ¨ Ø·ÙÙ„Ù‘Ø§Ø¨ Ø¢Ù„ Ù…ÙØ¯ÙŽØ±Ù‘ÙØ³ ØµÙØ¨Ø§Ø­ Ø¹ÙÙˆØ¯ ÙÙÙŠ Ù…ÙŽØ³Ø§Ø¡\n",
      "Ø°Ù‡Ø¨ Ø·Ø§Ù„Ø¨ Ø¥Ù„Ù‰ Ù…Ø¯Ø±Ø³Ø© ØµØ¨Ø§Ø­ Ø¹Ø§Ø¯ ÙÙŠ Ù…Ø³Ø§Ø¡ .\n",
      "Ø°Ù‡Ø¨ Ø·Ù„Ø§Ø¨ Ø¥Ù„Ù‰ Ù…Ø¯Ø±Ø³ ØµØ¨Ø§Ø­Ø§ Ø¹ÙˆØ¯ ÙÙŠ Ù…Ø³Ø§Ø¡\n",
      "Ø°Ù‡Ø¨ Ø·Ù„Ø¨ Ø§Ù„Ù‰ Ø¯Ø±Ø³ ØµØ¨Ø­ ÙŠØ¹Ø¯ ÙÙŠ Ø³Ø§Ø¡\n"
     ]
    }
   ],
   "source": [
    "text = \"ÙŠØ°Ù‡Ø¨ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ØµØ¨Ø§Ø­Ù‹Ø§ ÙˆÙŠØ¹ÙˆØ¯ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø¡.\"\n",
    "print(arabic_stemming(text, \"camel\"))\n",
    "print(arabic_stemming(text, \"farasa\"))\n",
    "print(arabic_stemming(text, \"light\"))\n",
    "print(arabic_stemming(text, \"nltk\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_Question:_**</span> How to determine which of the models is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Lemmatization:_**</span> is the process of reducing the different forms of a word to one single form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_lemmatization(text: str) -> str:\n",
    "    '''\n",
    "    A method that perform arabic text lemmatization\n",
    "\n",
    "    @param text: A sentence that requires lemmatization\n",
    "    '''\n",
    "    words = simple_word_tokenize(text) # check for alternatives\n",
    "\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        lemma = morph_analyzer.analyze(word)[0][\"lemma\"].split(\"|\")[0]\n",
    "\n",
    "        # Remove any character that is not in the Arabic Unicode range\n",
    "        clean_lemma = re.sub(r'[^\\u0600-\\u06FF]', '', lemma)\n",
    "        if clean_lemma:\n",
    "            lemmatized_words.append(clean_lemma)\n",
    "    \n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 1:  Ø°ÙŽÙ‡ÙŽØ¨ÙŽ Ø·ÙŽØ§Ù„ÙØ¨ÙŒ Ø¥ÙÙ„ÙŽÙ‰ Ù…ÙŽØ¯Ù’Ø±ÙŽØ³ÙŽØ©ÙŒ ØµÙŽØ¨Ø§Ø­ÙŒ Ø¹ÙŽØ§Ø¯ÙŽ ÙÙÙŠ Ù…ÙŽØ³ÙŽØ§Ø¡ÙŒ\n",
      "example 2: Ø±ÙŽØ¬ÙÙ„ÙŒ Ø£ÙŽØ­ÙŽØ¨Ù‘ÙŽ Ø·ÙÙÙ’Ù„ÙŒ Ù†ÙØ³ÙŽØ§Ø¡ÙŒ Ù‚ÙŽØ±ÙŽØ£ÙŽ ÙƒÙØªÙŽØ§Ø¨ÙŒ\n"
     ]
    }
   ],
   "source": [
    "text = \"ÙŠØ°Ù‡Ø¨ Ø§Ù„Ø·Ù„Ø§Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ØµØ¨Ø§Ø­Ù‹Ø§ ÙˆÙŠØ¹ÙˆØ¯ÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø¡.\"\n",
    "print(\"example 1: \", arabic_lemmatization(text))\n",
    "\n",
    "text = \"Ø§Ù„Ø±Ø¬Ø§Ù„ ÙŠØ­Ø¨ÙˆÙ† Ø§Ù„Ø£Ø·ÙØ§Ù„ ÙˆØ§Ù„Ù†Ø³Ø§Ø¡ ÙŠÙ‚Ø±Ø£Ù† Ø§Ù„ÙƒØªØ¨.\"\n",
    "print(\"example 2:\", arabic_lemmatization(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Observation:_**</span> The `arabic_lemmatization` method produce lemmatized text with diacritics (tashkel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Stopwords:_**</span> are most common terms in an Arabic language such as Ø­Ø±ÙˆÙ Ø§Ù„Ø¬Ø±."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_stopwords(text: str, custom_stopwords: bool=None, use_nltk: bool=True, use_tashaphyne: bool=True) -> str:\n",
    "    '''\n",
    "    A method that remove stopwords in text\n",
    "\n",
    "    @param text: a sentence that requires removing stopwords.\n",
    "    '''\n",
    "\n",
    "    # Get Arabic stopwords\n",
    "    stopwords = set()\n",
    "\n",
    "    if use_nltk:\n",
    "        stopwords.update(NLTK_STOPWORDS)\n",
    "    if use_tashaphyne:\n",
    "        stopwords.update(TASHAPHYNE_STOPWORDS)\n",
    "    if custom_stopwords:\n",
    "        stopwords.update(custom_stopwords)\n",
    "\n",
    "    stopwords_comp = {\"ØŒ\",\"Ø¢Ø¶\",\"Ø¢Ù…ÙŠÙ†ÙŽ\",\"Ø¢Ù‡\",\"Ø¢Ù‡Ø§Ù‹\",\"Ø¢ÙŠ\",\"Ø£\",\"Ø£Ø¨\",\"Ø£Ø¬Ù„\",\"Ø£Ø¬Ù…Ø¹\",\"Ø£Ø®\",\n",
    "                    \"Ø£Ø®Ø°\",\"Ø£ØµØ¨Ø­\",\"Ø£Ø¶Ø­Ù‰\",\"Ø£Ù‚Ø¨Ù„\",\"Ø£Ù‚Ù„\",\"Ø£ÙƒØ«Ø±\",\"Ø£Ù„Ø§\",\"Ø£Ù…\",\"Ø£Ù…Ø§\",\n",
    "                    \"Ø£Ù…Ø§Ù…Ùƒ\",\"Ø£Ù…Ø§Ù…ÙƒÙŽ\",\"Ø£Ù…Ø³Ù‰\",\"Ø£Ù…Ù‘Ø§\",\"Ø£Ù†\",\"Ø£Ù†Ø§\",\"Ø£Ù†Øª\",\"Ø£Ù†ØªÙ…\",\n",
    "                    \"Ø£Ù†ØªÙ…Ø§\",\"Ø£Ù†ØªÙ†\",\"Ø£Ù†ØªÙ\",\"Ø£Ù†Ø´Ø£\",\"Ø£Ù†Ù‘Ù‰\",\"Ø£Ùˆ\",\"Ø£ÙˆØ´Ùƒ\",\"Ø£ÙˆÙ„Ø¦Ùƒ\",\n",
    "                    \"Ø£ÙˆÙ„Ø¦ÙƒÙ…\",\"Ø£ÙˆÙ„Ø§Ø¡\",\"Ø£ÙˆÙ„Ø§Ù„Ùƒ\",\"Ø£ÙˆÙ‘Ù‡Ù’\",\"Ø£ÙŠ\",\"Ø£ÙŠØ§\",\"Ø£ÙŠÙ†\",\"Ø£ÙŠÙ†Ù…Ø§\",\n",
    "                    \"Ø£ÙŠÙ‘\",\"Ø£ÙŽÙ†Ù‘ÙŽ\",\"Ø£ÙŽÙŽÙŠÙ‘Ù\",\"Ø£ÙÙÙ‘Ù\",\"Ø¥Ø°\",\"Ø¥Ø°Ø§\",\"Ø¥Ø°Ø§Ù‹\",\"Ø¥Ø°Ù…Ø§\",\"Ø¥Ø°Ù†\",\"Ø¥Ù„Ù‰\",\n",
    "                    \"Ø¥Ù„ÙŠÙƒÙ…\",\"Ø¥Ù„ÙŠÙƒÙ…Ø§\",\"Ø¥Ù„ÙŠÙƒÙ†Ù‘\",\"Ø¥Ù„ÙŠÙƒÙŽ\",\"Ø¥Ù„ÙŽÙŠÙ’ÙƒÙŽ\",\"Ø¥Ù„Ù‘Ø§\",\"Ø¥Ù…Ù‘Ø§\",\"Ø¥Ù†\",\n",
    "                    \"Ø¥Ù†Ù‘Ù…Ø§\",\"Ø¥ÙŠ\",\"Ø¥ÙŠØ§Ùƒ\",\"Ø¥ÙŠØ§ÙƒÙ…\",\"Ø¥ÙŠØ§ÙƒÙ…Ø§\",\"Ø¥ÙŠØ§ÙƒÙ†\",\"Ø¥ÙŠØ§Ù†Ø§\",\"Ø¥ÙŠØ§Ù‡\",\n",
    "                    \"Ø¥ÙŠØ§Ù‡Ø§\",\"Ø¥ÙŠØ§Ù‡Ù…\",\"Ø¥ÙŠØ§Ù‡Ù…Ø§\",\"Ø¥ÙŠØ§Ù‡Ù†\",\"Ø¥ÙŠØ§ÙŠ\",\"Ø¥ÙŠÙ‡Ù\",\"Ø¥ÙÙ†Ù‘ÙŽ\",\"Ø§\",\n",
    "                    \"Ø§Ø¨ØªØ¯Ø£\",\"Ø§Ø«Ø±\",\"Ø§Ø¬Ù„\",\"Ø§Ø­Ø¯\",\"Ø§Ø®Ø±Ù‰\",\"Ø§Ø®Ù„ÙˆÙ„Ù‚\",\"Ø§Ø°Ø§\",\"Ø§Ø±Ø¨Ø¹Ø©\",\n",
    "                    \"Ø§Ø±ØªØ¯Ù‘\",\"Ø§Ø³ØªØ­Ø§Ù„\",\"Ø§Ø·Ø§Ø±\",\"Ø§Ø¹Ø§Ø¯Ø©\",\"Ø§Ø¹Ù„Ù†Øª\",\"Ø§Ù\",\"Ø§ÙƒØ«Ø±\",\"Ø§ÙƒØ¯\",\n",
    "                    \"Ø§Ù„Ø£Ù„Ø§Ø¡\",\"Ø§Ù„Ø£Ù„Ù‰\",\"Ø§Ù„Ø§\",\"Ø§Ù„Ø§Ø®ÙŠØ±Ø©\",\"Ø§Ù„Ø§Ù†\",\"Ø§Ù„Ø§ÙˆÙ„\",\"Ø§Ù„Ø§ÙˆÙ„Ù‰\",\"Ø§Ù„ØªÙ‰\",\n",
    "                    \"Ø§Ù„ØªÙŠ\",\"Ø§Ù„Ø«Ø§Ù†ÙŠ\",\"Ø§Ù„Ø«Ø§Ù†ÙŠØ©\",\"Ø§Ù„Ø°Ø§ØªÙŠ\",\"Ø§Ù„Ø°Ù‰\",\"Ø§Ù„Ø°ÙŠ\",\"Ø§Ù„Ø°ÙŠÙ†\",\n",
    "                    \"Ø§Ù„Ø³Ø§Ø¨Ù‚\",\"Ø§Ù„Ù\",\"Ø§Ù„Ù„Ø§Ø¦ÙŠ\",\"Ø§Ù„Ù„Ø§ØªÙŠ\",\"Ø§Ù„Ù„ØªØ§Ù†\",\"Ø§Ù„Ù„ØªÙŠØ§\",\"Ø§Ù„Ù„ØªÙŠÙ†\",\n",
    "                    \"Ø§Ù„Ù„Ø°Ø§Ù†\",\"Ø§Ù„Ù„Ø°ÙŠÙ†\",\"Ø§Ù„Ù„ÙˆØ§ØªÙŠ\",\"Ø§Ù„Ù…Ø§Ø¶ÙŠ\",\"Ø§Ù„Ù…Ù‚Ø¨Ù„\",\"Ø§Ù„ÙˆÙ‚Øª\",\n",
    "                    \"Ø§Ù„Ù‰\",\"Ø§Ù„ÙŠÙˆÙ…\",\"Ø§Ù…Ø§\",\"Ø§Ù…Ø§Ù…\",\"Ø§Ù…Ø³\",\"Ø§Ù†\",\"Ø§Ù†Ø¨Ø±Ù‰\",\"Ø§Ù†Ù‚Ù„Ø¨\",\n",
    "                    \"Ø§Ù†Ù‡\",\"Ø§Ù†Ù‡Ø§\",\"Ø§Ùˆ\",\"Ø§ÙˆÙ„\",\"Ø§ÙŠ\",\"Ø§ÙŠØ§Ø±\",\"Ø§ÙŠØ§Ù…\",\"Ø§ÙŠØ¶Ø§\",\"Ø¨\",\n",
    "                    \"Ø¨Ø§Øª\",\"Ø¨Ø§Ø³Ù…\",\"Ø¨Ø§Ù†\",\"Ø¨Ø®Ù\",\"Ø¨Ø±Ø³\",\"Ø¨Ø³Ø¨Ø¨\",\"Ø¨Ø³Ù‘\",\"Ø¨Ø´ÙƒÙ„\",\"Ø¨Ø¶Ø¹\",\n",
    "                    \"Ø¨Ø·Ø¢Ù†\",\"Ø¨Ø¹Ø¯\",\"Ø¨Ø¹Ø¶\",\"Ø¨Ùƒ\",\"Ø¨ÙƒÙ…\",\"Ø¨ÙƒÙ…Ø§\",\"Ø¨ÙƒÙ†\",\"Ø¨Ù„\",\"Ø¨Ù„Ù‰\",\n",
    "                    \"Ø¨Ù…Ø§\",\"Ø¨Ù…Ø§Ø°Ø§\",\"Ø¨Ù…Ù†\",\"Ø¨Ù†\",\"Ø¨Ù†Ø§\",\"Ø¨Ù‡\",\"Ø¨Ù‡Ø§\",\"Ø¨ÙŠ\",\"Ø¨ÙŠØ¯\",\n",
    "                    \"Ø¨ÙŠÙ†\",\"Ø¨ÙŽØ³Ù’\",\"Ø¨ÙŽÙ„Ù’Ù‡ÙŽ\",\"Ø¨ÙØ¦Ù’Ø³ÙŽ\",\"ØªØ§Ù†Ù\",\"ØªØ§Ù†ÙÙƒ\",\"ØªØ¨Ø¯Ù‘Ù„\",\"ØªØ¬Ø§Ù‡\",\"ØªØ­ÙˆÙ‘Ù„\",\n",
    "                    \"ØªÙ„Ù‚Ø§Ø¡\",\"ØªÙ„Ùƒ\",\"ØªÙ„ÙƒÙ…\",\"ØªÙ„ÙƒÙ…Ø§\",\"ØªÙ…\",\"ØªÙŠÙ†Ùƒ\",\"ØªÙŽÙŠÙ’Ù†Ù\",\"ØªÙÙ‡\",\"ØªÙÙŠ\",\n",
    "                    \"Ø«Ù„Ø§Ø«Ø©\",\"Ø«Ù…\",\"Ø«Ù…Ù‘\",\"Ø«Ù…Ù‘Ø©\",\"Ø«ÙÙ…Ù‘ÙŽ\",\"Ø¬Ø¹Ù„\",\"Ø¬Ù„Ù„\",\"Ø¬Ù…ÙŠØ¹\",\"Ø¬ÙŠØ±\",\"Ø­Ø§Ø±\",\n",
    "                    \"Ø­Ø§Ø´Ø§\",\"Ø­Ø§Ù„ÙŠØ§\",\"Ø­Ø§ÙŠ\",\"Ø­ØªÙ‰\",\"Ø­Ø±Ù‰\",\"Ø­Ø³Ø¨\",\"Ø­Ù…\",\"Ø­ÙˆØ§Ù„Ù‰\",\"Ø­ÙˆÙ„\",\n",
    "                    \"Ø­ÙŠØ«\",\"Ø­ÙŠØ«Ù…Ø§\",\"Ø­ÙŠÙ†\",\"Ø­ÙŠÙ‘ÙŽ\",\"Ø­ÙŽØ¨Ù‘ÙŽØ°ÙŽØ§\",\"Ø­ÙŽØªÙ‘ÙŽÙ‰\",\"Ø­ÙŽØ°Ø§Ø±Ù\",\"Ø®Ù„Ø§\",\"Ø®Ù„Ø§Ù„\",\n",
    "                    \"Ø¯ÙˆÙ†\",\"Ø¯ÙˆÙ†Ùƒ\",\"Ø°Ø§\",\"Ø°Ø§Øª\",\"Ø°Ø§Ùƒ\",\"Ø°Ø§Ù†Ùƒ\",\"Ø°Ø§Ù†Ù\",\"Ø°Ù„Ùƒ\",\"Ø°Ù„ÙƒÙ…\",\n",
    "                    \"Ø°Ù„ÙƒÙ…Ø§\",\"Ø°Ù„ÙƒÙ†\",\"Ø°Ùˆ\",\"Ø°ÙˆØ§\",\"Ø°ÙˆØ§ØªØ§\",\"Ø°ÙˆØ§ØªÙŠ\",\"Ø°ÙŠØª\",\"Ø°ÙŠÙ†Ùƒ\",\n",
    "                    \"Ø°ÙŽÙŠÙ’Ù†Ù\",\"Ø°ÙÙ‡\",\"Ø°ÙÙŠ\",\"Ø±Ø§Ø­\",\"Ø±Ø¬Ø¹\",\"Ø±ÙˆÙŠØ¯Ùƒ\",\"Ø±ÙŠØ«\",\"Ø±ÙØ¨Ù‘ÙŽ\",\"Ø²ÙŠØ§Ø±Ø©\",\n",
    "                    \"Ø³Ø¨Ø­Ø§Ù†\",\"Ø³Ø±Ø¹Ø§Ù†\",\"Ø³Ù†Ø©\",\"Ø³Ù†ÙˆØ§Øª\",\"Ø³ÙˆÙ\",\"Ø³ÙˆÙ‰\",\"Ø³ÙŽØ§Ø¡ÙŽ\",\"Ø³ÙŽØ§Ø¡ÙŽÙ…ÙŽØ§\",\n",
    "                    \"Ø´Ø¨Ù‡\",\"Ø´Ø®ØµØ§\",\"Ø´Ø±Ø¹\",\"Ø´ÙŽØªÙ‘ÙŽØ§Ù†ÙŽ\",\"ØµØ§Ø±\",\"ØµØ¨Ø§Ø­\",\"ØµÙØ±\",\"ØµÙ‡Ù\",\"ØµÙ‡Ù’\",\n",
    "                    \"Ø¶Ø¯\",\"Ø¶Ù…Ù†\",\"Ø·Ø§Ù‚\",\"Ø·Ø§Ù„Ù…Ø§\",\"Ø·ÙÙ‚\",\"Ø·ÙŽÙ‚\",\"Ø¸Ù„Ù‘\",\"Ø¹Ø§Ø¯\",\"Ø¹Ø§Ù…\",\n",
    "                    \"Ø¹Ø§Ù…Ø§\",\"Ø¹Ø§Ù…Ø©\",\"Ø¹Ø¯Ø§\",\"Ø¹Ø¯Ø©\",\"Ø¹Ø¯Ø¯\",\"Ø¹Ø¯Ù…\",\"Ø¹Ø³Ù‰\",\"Ø¹Ø´Ø±\",\"Ø¹Ø´Ø±Ø©\",\n",
    "                    \"Ø¹Ù„Ù‚\",\"Ø¹Ù„Ù‰\",\"Ø¹Ù„ÙŠÙƒ\",\"Ø¹Ù„ÙŠÙ‡\",\"Ø¹Ù„ÙŠÙ‡Ø§\",\"Ø¹Ù„Ù‘Ù‹\",\"Ø¹Ù†\",\"Ø¹Ù†Ø¯\",\"Ø¹Ù†Ø¯Ù…Ø§\",\n",
    "                    \"Ø¹ÙˆØ¶\",\"Ø¹ÙŠÙ†\",\"Ø¹ÙŽØ¯ÙŽØ³Ù’\",\"Ø¹ÙŽÙ…Ù‘ÙŽØ§\",\"ØºØ¯Ø§\",\"ØºÙŠØ±\",\"Ù€\",\"Ù\",\"ÙØ§Ù†\",\"ÙÙ„Ø§Ù†\",\n",
    "                    \"ÙÙˆ\",\"ÙÙ‰\",\"ÙÙŠ\",\"ÙÙŠÙ…\",\"ÙÙŠÙ…Ø§\",\"ÙÙŠÙ‡\",\"ÙÙŠÙ‡Ø§\",\"Ù‚Ø§Ù„\",\"Ù‚Ø§Ù…\",\"Ù‚Ø¨Ù„\",\n",
    "                    \"Ù‚Ø¯\",\"Ù‚Ø·Ù‘\",\"Ù‚Ù„Ù…Ø§\",\"Ù‚ÙˆØ©\",\"ÙƒØ£Ù†Ù‘Ù…Ø§\",\"ÙƒØ£ÙŠÙ†\",\"ÙƒØ£ÙŠÙ‘\",\"ÙƒØ£ÙŠÙ‘Ù†\",\"ÙƒØ§Ø¯\",\n",
    "                    \"ÙƒØ§Ù†\",\"ÙƒØ§Ù†Øª\",\"ÙƒØ°Ø§\",\"ÙƒØ°Ù„Ùƒ\",\"ÙƒØ±Ø¨\",\"ÙƒÙ„\",\"ÙƒÙ„Ø§\",\"ÙƒÙ„Ø§Ù‡Ù…Ø§\",\"ÙƒÙ„ØªØ§\",\n",
    "                    \"ÙƒÙ„Ù…\",\"ÙƒÙ„ÙŠÙƒÙ…Ø§\",\"ÙƒÙ„ÙŠÙ‡Ù…Ø§\",\"ÙƒÙ„Ù‘Ù…Ø§\",\"ÙƒÙ„Ù‘ÙŽØ§\",\"ÙƒÙ…\",\"ÙƒÙ…Ø§\",\"ÙƒÙŠ\",\"ÙƒÙŠØª\",\n",
    "                    \"ÙƒÙŠÙ\",\"ÙƒÙŠÙÙ…Ø§\",\"ÙƒÙŽØ£ÙŽÙ†Ù‘ÙŽ\",\"ÙƒÙØ®\",\"Ù„Ø¦Ù†\",\"Ù„Ø§\",\"Ù„Ø§Øª\",\"Ù„Ø§Ø³ÙŠÙ…Ø§\",\"Ù„Ø¯Ù†\",\"Ù„Ø¯Ù‰\",\n",
    "                    \"Ù„Ø¹Ù…Ø±\",\"Ù„Ù‚Ø§Ø¡\",\"Ù„Ùƒ\",\"Ù„ÙƒÙ…\",\"Ù„ÙƒÙ…Ø§\",\"Ù„ÙƒÙ†\",\"Ù„ÙƒÙ†Ù‘ÙŽÙ…Ø§\",\"Ù„ÙƒÙŠ\",\"Ù„ÙƒÙŠÙ„Ø§\",\n",
    "                    \"Ù„Ù„Ø§Ù…Ù…\",\"Ù„Ù…\",\"Ù„Ù…Ø§\",\"Ù„Ù…Ù‘Ø§\",\"Ù„Ù†\",\"Ù„Ù†Ø§\",\"Ù„Ù‡\",\"Ù„Ù‡Ø§\",\"Ù„Ùˆ\",\"Ù„ÙˆÙƒØ§Ù„Ø©\",\n",
    "                    \"Ù„ÙˆÙ„Ø§\",\"Ù„ÙˆÙ…Ø§\",\"Ù„ÙŠ\",\"Ù„ÙŽØ³Ù’ØªÙŽ\",\"Ù„ÙŽØ³Ù’ØªÙ\",\"Ù„ÙŽØ³Ù’ØªÙÙ…\",\"Ù„ÙŽØ³Ù’ØªÙÙ…ÙŽØ§\",\"Ù„ÙŽØ³Ù’ØªÙÙ†Ù‘ÙŽ\",\"Ù„ÙŽØ³Ù’ØªÙ\",\n",
    "                    \"Ù„ÙŽØ³Ù’Ù†ÙŽ\",\"Ù„ÙŽØ¹ÙŽÙ„Ù‘ÙŽ\",\"Ù„ÙŽÙƒÙÙ†Ù‘ÙŽ\",\"Ù„ÙŽÙŠÙ’ØªÙŽ\",\"Ù„ÙŽÙŠÙ’Ø³ÙŽ\",\"Ù„ÙŽÙŠÙ’Ø³ÙŽØ§\",\"Ù„ÙŽÙŠÙ’Ø³ÙŽØªÙŽØ§\",\"Ù„ÙŽÙŠÙ’Ø³ÙŽØªÙ’\",\"Ù„ÙŽÙŠÙ’Ø³ÙÙˆØ§\",\n",
    "                    \"Ù„ÙŽÙØ³Ù’Ù†ÙŽØ§\",\"Ù…Ø§\",\"Ù…Ø§Ø§Ù†ÙÙƒ\",\"Ù…Ø§Ø¨Ø±Ø­\",\"Ù…Ø§Ø¯Ø§Ù…\",\"Ù…Ø§Ø°Ø§\",\"Ù…Ø§Ø²Ø§Ù„\",\"Ù…Ø§ÙØªØ¦\",\n",
    "                    \"Ù…Ø§ÙŠÙˆ\",\"Ù…ØªÙ‰\",\"Ù…Ø«Ù„\",\"Ù…Ø°\",\"Ù…Ø³Ø§Ø¡\",\"Ù…Ø¹\",\"Ù…Ø¹Ø§Ø°\",\"Ù…Ù‚Ø§Ø¨Ù„\",\"Ù…ÙƒØ§Ù†ÙƒÙ…\",\n",
    "                    \"Ù…ÙƒØ§Ù†ÙƒÙ…Ø§\",\"Ù…ÙƒØ§Ù†ÙƒÙ†Ù‘\",\"Ù…ÙƒØ§Ù†ÙŽÙƒ\",\"Ù…Ù„ÙŠØ§Ø±\",\"Ù…Ù„ÙŠÙˆÙ†\",\"Ù…Ù…Ø§\",\"Ù…Ù…Ù†\",\"Ù…Ù†\",\n",
    "                    \"Ù…Ù†Ø°\",\"Ù…Ù†Ù‡Ø§\",\"Ù…Ù‡\",\"Ù…Ù‡Ù…Ø§\",\"Ù…ÙŽÙ†Ù’\",\"Ù…ÙÙ†\",\"Ù†Ø­Ù†\",\"Ù†Ø­Ùˆ\",\"Ù†Ø¹Ù…\",\"Ù†ÙØ³\",\n",
    "                    \"Ù†ÙØ³Ù‡\",\"Ù†Ù‡Ø§ÙŠØ©\",\"Ù†ÙŽØ®Ù’\",\"Ù†ÙØ¹ÙÙ…Ù‘Ø§\",\"Ù†ÙØ¹Ù’Ù…ÙŽ\",\"Ù‡Ø§\",\"Ù‡Ø§Ø¤Ù…\",\"Ù‡Ø§ÙƒÙŽ\",\"Ù‡Ø§Ù‡Ù†Ø§\",\n",
    "                    \"Ù‡Ø¨Ù‘\",\"Ù‡Ø°Ø§\",\"Ù‡Ø°Ù‡\",\"Ù‡ÙƒØ°Ø§\",\"Ù‡Ù„\",\"Ù‡Ù„Ù…Ù‘ÙŽ\",\"Ù‡Ù„Ù‘Ø§\",\"Ù‡Ù…\",\"Ù‡Ù…Ø§\",\"Ù‡Ù†\",\n",
    "                    \"Ù‡Ù†Ø§\",\"Ù‡Ù†Ø§Ùƒ\",\"Ù‡Ù†Ø§Ù„Ùƒ\",\"Ù‡Ùˆ\",\"Ù‡ÙŠ\",\"Ù‡ÙŠØ§\",\"Ù‡ÙŠØª\",\"Ù‡ÙŠÙ‘Ø§\",\"Ù‡ÙŽØ¤Ù„Ø§Ø¡\",\n",
    "                    \"Ù‡ÙŽØ§ØªØ§Ù†Ù\",\"Ù‡ÙŽØ§ØªÙŽÙŠÙ’Ù†Ù\",\"Ù‡ÙŽØ§ØªÙÙ‡\",\"Ù‡ÙŽØ§ØªÙÙŠ\",\"Ù‡ÙŽØ¬Ù’\",\"Ù‡ÙŽØ°Ø§\",\"Ù‡ÙŽØ°Ø§Ù†Ù\",\"Ù‡ÙŽØ°ÙŽÙŠÙ’Ù†Ù\",\n",
    "                    \"Ù‡ÙŽØ°ÙÙ‡\",\"Ù‡ÙŽØ°ÙÙŠ\",\"Ù‡ÙŽÙŠÙ’Ù‡ÙŽØ§ØªÙŽ\",\"Ùˆ\",\"ÙˆØ§\",\"ÙˆØ§Ø­Ø¯\",\"ÙˆØ§Ø¶Ø§Ù\",\"ÙˆØ§Ø¶Ø§ÙØª\",\"ÙˆØ§ÙƒØ¯\",\n",
    "                    \"ÙˆØ§Ù†\",\"ÙˆØ§Ù‡Ø§Ù‹\",\"ÙˆØ§ÙˆØ¶Ø­\",\"ÙˆØ±Ø§Ø¡ÙŽÙƒ\",\"ÙˆÙÙŠ\",\"ÙˆÙ‚Ø§Ù„\",\"ÙˆÙ‚Ø§Ù„Øª\",\"ÙˆÙ‚Ø¯\",\n",
    "                    \"ÙˆÙ‚Ù\",\"ÙˆÙƒØ§Ù†\",\"ÙˆÙƒØ§Ù†Øª\",\"ÙˆÙ„Ø§\",\"ÙˆÙ„Ù…\",\"ÙˆÙ…Ù†\",\"Ù…ÙŽÙ†\",\"ÙˆÙ‡Ùˆ\",\"ÙˆÙ‡ÙŠ\",\n",
    "                    \"ÙˆÙŠÙƒØ£Ù†Ù‘\",\"ÙˆÙŽÙŠÙ’\",\"ÙˆÙØ´Ù’ÙƒÙŽØ§Ù†ÙŽÙŽ\",\"ÙŠÙƒÙˆÙ†\",\"ÙŠÙ…ÙƒÙ†\",\"ÙŠÙˆÙ…\",\"Ù‘Ø£ÙŠÙ‘Ø§Ù†\"}\n",
    "\n",
    "    words = simple_word_tokenize(text)\n",
    "\n",
    "    return \" \".join([w for w in words if not w in stopwords and not w in stopwords_comp and len(w) >= 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Original: Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„ØªÙØ§Ø­ ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­\n",
      "Filtered: Ø£Ø­Ø¨ Ø§Ù„ØªÙØ§Ø­ Ø§Ù„ØµØ¨Ø§Ø­\n",
      "\n",
      "Example 2\n",
      "Original: Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù…Ø¬ØªÙ‡Ø¯ ÙŠØ¯Ø±Ø³ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©\n",
      "Filtered: Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù…Ø¬ØªÙ‡Ø¯ ÙŠØ¯Ø±Ø³ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©\n",
      "\n",
      "Example 3\n",
      "Original: Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù…Ø¬ØªÙ‡Ø¯ ÙŠØ¯Ø±Ø³ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©\n",
      "Filtered with custom stopwords: Ø§Ù„Ø·Ø§Ù„Ø¨ ÙŠØ¯Ø±Ø³ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple sentence with a few common stopwords.\n",
    "text1 = \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„ØªÙØ§Ø­ ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­\"\n",
    "# Expected behavior: Words like \"Ø£Ù†Ø§\" and \"ÙÙŠ\" (if included in our stopword sets) should be removed.\n",
    "print(\"Example 1\")\n",
    "print(\"Original:\", text1)\n",
    "print(\"Filtered:\", remove_arabic_stopwords(text1))\n",
    "print()\n",
    "\n",
    "# Example 2: Sentence with additional stopwords from stopwords_comp.\n",
    "text2 = \"Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù…Ø¬ØªÙ‡Ø¯ ÙŠØ¯Ø±Ø³ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©\"\n",
    "# Expected behavior: Words that are common stopwords (e.g., \"ÙÙŠ\") should be removed.\n",
    "print(\"Example 2\")\n",
    "print(\"Original:\", text2)\n",
    "print(\"Filtered:\", remove_arabic_stopwords(text2))\n",
    "print()\n",
    "\n",
    "# Example 3: Using custom stopwords to remove an additional word.\n",
    "custom_stops = {\"Ø§Ù„Ù…Ø¬ØªÙ‡Ø¯\"}\n",
    "text3 = \"Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù…Ø¬ØªÙ‡Ø¯ ÙŠØ¯Ø±Ø³ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©\"\n",
    "# Expected behavior: In addition to the default stopwords, \"Ø§Ù„Ù…Ø¬ØªÙ‡Ø¯\" should be removed.\n",
    "print(\"Example 3\")\n",
    "print(\"Original:\", text3)\n",
    "print(\"Filtered with custom stopwords:\", remove_arabic_stopwords(text3, custom_stopwords=custom_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Handling Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Purpose:* The idea is that arabic text can sometimes contains hashtags as for example \"Ù…Ø¨Ø§Ø±Ùƒ_Ø¹Ù„ÙŠÙƒÙ…_Ø§Ù„Ø´Ù‡Ø± Ø±Ø¨ÙŠ Ø§Ø¬Ø¹Ù„ Ø´Ù‡Ø± Ø±Ù…Ø¶Ø§Ù† ÙØ§ØªØ­Ø© Ø®ÙŠØ± Ù„Ù†Ø§ ÙˆØ¨Ø¯Ø§ÙŠØ© Ø£Ø¬Ù…Ù„ Ø£Ù‚Ø¯Ø§Ø±Ù†Ø§ ÙˆØ­Ù‚Ù‚ Ù„Ù†Ø§ Ù…Ø§ Ù†ØªÙ…Ù†Ù‰ ÙŠØ§ ÙƒØ±ÙŠÙ…#\" which need to be converted to \" Ù…Ø¨Ø§Ø±Ùƒ Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø´Ù‡Ø± Ø±Ø¨ÙŠ Ø§Ø¬Ø¹Ù„ Ø´Ù‡Ø± Ø±Ù…Ø¶Ø§Ù† ÙØ§ØªØ­Ø©Ø®ÙŠØ± Ù„Ù†Ø§ ÙˆØ¨Ø¯Ø§ÙŠØ© Ø£Ø¬Ù…Ù„ Ø£Ù‚Ø¯Ø§Ø±Ù†Ø§\n",
    "ÙˆØ­Ù‚Ù‚ Ù„Ù†Ø§ Ù…Ø§ Ù†ØªÙ…Ù†Ù‰ ÙŠØ§ ÙƒØ±ÙŠÙ…\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_hashtag(word: str) -> bool:\n",
    "    '''\n",
    "    Checks whether a word starts or ends with a hashtag.\n",
    "    @param word: a single word\n",
    "    @return: True if the word starts or ends with \"#\", False otherwise.\n",
    "    '''\n",
    "    return word.startswith(\"#\") or word.endswith(\"#\")\n",
    "\n",
    "def split_hashtag_to_words(tag: str) -> list:\n",
    "    '''\n",
    "    Converts a hashtag to a list of words.\n",
    "    If the hashtag uses underscores, they are used as delimiters;\n",
    "    otherwise, it applies a camel-case splitting pattern.\n",
    "    \n",
    "    @param tag: a hashtag (e.g., \"#Ù…Ø¨Ø§Ø±Ùƒ_Ø¹Ù„ÙŠÙƒÙ…_Ø§Ù„Ø´Ù‡Ø±\")\n",
    "    @return: a list of words extracted from the hashtag.\n",
    "    '''\n",
    "    tag = tag.replace('#', '')\n",
    "    tags = tag.split('_')\n",
    "    if len(tags) > 1:\n",
    "        return tags\n",
    "    pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "    return pattern.findall(tag)\n",
    "\n",
    "def extract_hashtag(text: str) -> list:\n",
    "    '''\n",
    "    Extracts words from hashtags found in the input text.\n",
    "    It removes trailing punctuation and then splits the hashtag.\n",
    "    \n",
    "    @param text: a sentence that contains one or more hashtags.\n",
    "    @return: a list of words extracted from hashtags.\n",
    "    '''\n",
    "    hash_list = [re.sub(r\"(\\W+)$\", \"\", i) for i in text.split() if i.startswith(\"#\") or i.endswith(\"#\")]\n",
    "    word_list = []\n",
    "    for word in hash_list:\n",
    "        word_list.extend(split_hashtag_to_words(word))\n",
    "    return word_list\n",
    "\n",
    "def clean_arabic_hashtag(text: str) -> str:\n",
    "    '''\n",
    "    Replaces each hashtag in the text with its space-separated equivalent.\n",
    "    Note: Only words starting with \"#\" are processed.\n",
    "    \n",
    "    @param text: a sentence that contains hashtags.\n",
    "    @return: the text with cleaned hashtags.\n",
    "    '''\n",
    "    words = text.split()\n",
    "    output = []\n",
    "    for word in words:\n",
    "        if has_hashtag(word):\n",
    "            output.extend(extract_hashtag(word))\n",
    "        else:\n",
    "            output.append(word)\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "Original: #Ù…Ø¨Ø§Ø±Ùƒ_Ø¹Ù„ÙŠÙƒÙ…_Ø§Ù„Ø´Ù‡Ø± Ø±Ø¨ÙŠ Ø§Ø¬Ø¹Ù„ Ø´Ù‡Ø± Ø±Ù…Ø¶Ø§Ù† ÙØ§ØªØ­Ø© Ø®ÙŠØ± Ù„Ù†Ø§ ÙˆØ¨Ø¯Ø§ÙŠØ© Ø£Ø¬Ù…Ù„ Ø£Ù‚Ø¯Ø§Ø±Ù†Ø§ ÙˆØ­Ù‚Ù‚ Ù„Ù†Ø§ Ù…Ø§ Ù†ØªÙ…Ù†Ù‰ ÙŠØ§ ÙƒØ±ÙŠÙ…#\n",
      "Cleaned:  Ù…Ø¨Ø§Ø±Ùƒ Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø´Ù‡Ø± Ø±Ø¨ÙŠ Ø§Ø¬Ø¹Ù„ Ø´Ù‡Ø± Ø±Ù…Ø¶Ø§Ù† ÙØ§ØªØ­Ø© Ø®ÙŠØ± Ù„Ù†Ø§ ÙˆØ¨Ø¯Ø§ÙŠØ© Ø£Ø¬Ù…Ù„ Ø£Ù‚Ø¯Ø§Ø±Ù†Ø§ ÙˆØ­Ù‚Ù‚ Ù„Ù†Ø§ Ù…Ø§ Ù†ØªÙ…Ù†Ù‰ ÙŠØ§\n"
     ]
    }
   ],
   "source": [
    "print(\"Example:\")\n",
    "# Hashtags at both beginning and end.\n",
    "test3 = \"#Ù…Ø¨Ø§Ø±Ùƒ_Ø¹Ù„ÙŠÙƒÙ…_Ø§Ù„Ø´Ù‡Ø± Ø±Ø¨ÙŠ Ø§Ø¬Ø¹Ù„ Ø´Ù‡Ø± Ø±Ù…Ø¶Ø§Ù† ÙØ§ØªØ­Ø© Ø®ÙŠØ± Ù„Ù†Ø§ ÙˆØ¨Ø¯Ø§ÙŠØ© Ø£Ø¬Ù…Ù„ Ø£Ù‚Ø¯Ø§Ø±Ù†Ø§ ÙˆØ­Ù‚Ù‚ Ù„Ù†Ø§ Ù…Ø§ Ù†ØªÙ…Ù†Ù‰ ÙŠØ§ ÙƒØ±ÙŠÙ…#\"\n",
    "print(\"Original:\", test3)\n",
    "print(\"Cleaned: \", clean_arabic_hashtag(test3))\n",
    "# Expected output:\n",
    "# the first token starts with '#' so it gets converted to \"Ù…Ø¨Ø§Ø±Ùƒ Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø´Ù‡Ø±\"\n",
    "# Thus, output will be: \"Ù…Ø¨Ø§Ø±Ùƒ Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø´Ù‡Ø± Ø±Ø¨ÙŠ Ø§Ø¬Ø¹Ù„ Ø´Ù‡Ø± Ø±Ù…Ø¶Ø§Ù† ÙØ§ØªØ­Ø© Ø®ÙŠØ± Ù„Ù†Ø§ ÙˆØ¨Ø¯Ø§ÙŠØ© Ø£Ø¬Ù…Ù„ Ø£Ù‚Ø¯Ø§Ø±Ù†Ø§ ÙˆØ­Ù‚Ù‚ Ù„Ù†Ø§ Ù…Ø§ Ù†ØªÙ…Ù†Ù‰ ÙŠØ§ ÙƒØ±ÙŠÙ…\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Handling Emojis ðŸ¤ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_emojis(text: str, mode: str = 'remove') -> str:\n",
    "    '''\n",
    "    A method that handles emojis.\n",
    "    '''\n",
    "    if mode == 'remove':\n",
    "        return emoji.replace_emoji(text, '')\n",
    "    elif mode == 'description':\n",
    "        return emoji.demojize(text, language='ar')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ðŸ“š ÙˆØ£Ø³ØªÙ…ØªØ¹ Ø¨Ù‡Ø§ ÙƒØ«ÙŠØ±Ø§Ù‹ ðŸ˜Š\n",
      "Without emojis: Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©  ÙˆØ£Ø³ØªÙ…ØªØ¹ Ø¨Ù‡Ø§ ÙƒØ«ÙŠØ±Ø§Ù‹ \n",
      "With emoji descriptions: Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© :ÙƒØªØ¨: ÙˆØ£Ø³ØªÙ…ØªØ¹ Ø¨Ù‡Ø§ ÙƒØ«ÙŠØ±Ø§Ù‹ :ÙˆØ¬Ù‡_Ø¨Ø§Ø³Ù…_Ø¨Ø¹ÙŠÙ†ÙŠÙ†_Ø¨Ø§Ø³Ù…ØªÙŠÙ†:\n"
     ]
    }
   ],
   "source": [
    "text_with_emoji = \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ðŸ“š ÙˆØ£Ø³ØªÙ…ØªØ¹ Ø¨Ù‡Ø§ ÙƒØ«ÙŠØ±Ø§Ù‹ ðŸ˜Š\"\n",
    "text_without_emoji = handle_emojis(text_with_emoji, 'remove')\n",
    "text_with_descriptions = handle_emojis(text_with_emoji, 'description')\n",
    "\n",
    "print(\"Original:\", text_with_emoji)\n",
    "print(\"Without emojis:\", text_without_emoji)\n",
    "print(\"With emoji descriptions:\", text_with_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> Search on how to extract meaning from emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Normalization:_**</span> match digits that have the same writing but different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text: str, tool: str) -> str:\n",
    "    '''\n",
    "    A method that match digits that have same writing but different encodings\n",
    "\n",
    "    @param text: a sentence that requires normalizing its text.\n",
    "    @param tool: determining which library name to utilize in normalizing text.\n",
    "    '''\n",
    "    \n",
    "    if tool == \"tnkeeh\":\n",
    "        normalizer = tn.Tnkeeh(normalize=True)\n",
    "        output = normalizer.clean_raw_text(text)\n",
    "        return output[0]\n",
    "    elif tool == \"camel\":\n",
    "        return normalize_unicode(text)\n",
    "    else:\n",
    "        text = text.strip()\n",
    "        text = re.sub(\"[Ø¥Ø£Ù±Ø¢Ø§]\", \"Ø§\", text)\n",
    "        text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "        text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "        text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "        text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "        text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
    "        text = re.sub(\"Ú¤\", \"Ù\", text)\n",
    "        text = re.sub(\"Ú†\", \"Ø¬\", text)\n",
    "        text = re.sub(\"Ù¾\", \"Ø¨\", text)\n",
    "        text = re.sub(\"Úœ\", \"Ø´\", text)\n",
    "        text = re.sub(\"Úª\", \"Ùƒ\", text)\n",
    "        text = re.sub(\"Ú§\", \"Ù‚\", text)\n",
    "        text = re.sub(\"Ù±\", \"Ø§\", text)\n",
    "        noise = re.compile(\"\"\" Ù‘    | # Tashdid\n",
    "                                ÙŽ    | # Fatha\n",
    "                                Ù‹    | # Tanwin Fath\n",
    "                                Ù    | # Damma\n",
    "                                ÙŒ    | # Tanwin Damm\n",
    "                                Ù    | # Kasra\n",
    "                                Ù    | # Tanwin Kasr\n",
    "                                Ù’    | # Sukun\n",
    "                                Ù€     # Tatwil/Kashida\n",
    "                            \"\"\", re.VERBOSE)\n",
    "        text = re.sub(noise, '', text)\n",
    "        text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Convert repeated characters to single occurrence\n",
    "        return araby.strip_tashkeel(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Replace various forms of alif with a bare alif.\n",
      "Input:  Ø¥Ø³Ù„Ø§Ù…\n",
      "Excepted:  Ø§Ø³Ù„Ø§Ù…\n",
      "Output:  Ø§Ø³Ù„Ø§Ù…\n",
      "Pass\n",
      "\n",
      "Example 2: Remove diacritics and perform character replacements.\n",
      "Input:  Ù…ÙØ¯ÙŽØ±Ù‘ÙØ³Ø©\n",
      "Excepted:  Ù…Ø¯Ø±Ø³Ù‡\n",
      "Output:  Ù…Ø¯Ø±Ø³Ù‡\n",
      "Pass\n",
      "\n",
      "Example 3: Collapse repeated characters.\n",
      "Input:  Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…\n",
      "Excepted:  Ù…Ù…\n",
      "Output:  Ù…Ù…\n",
      "Pass\n",
      "\n",
      "Example 4: Multiple replacement rules in one sentence.\n",
      "Input:  Ú¯Ù„Ø§Ø¨ Ú†Ø§ÛŒ Ù¾ÙŠØª Ú¤ÙŠØ¯ÙŠÙˆ ÚœÙ‡Ø± ÚªØªØ§Ø¨ Ú§ÙƒØ± Ù±Ù…Ø§Ù†\n",
      "Excepted:  ÙƒÙ„Ø§Ø¨ Ø¬Ø§ÙŠ Ø¨ÙŠØª ÙÙŠØ¯ÙŠÙˆ Ø´Ù‡Ø± ÙƒØªØ§Ø¨ Ù‚ÙƒØ± Ø§Ù…Ø§Ù†\n",
      "Output:  ÙƒÙ„Ø§Ø¨ Ø¬Ø§ÛŒ Ø¨ÙŠØª ÙÙŠØ¯ÙŠÙˆ Ø´Ù‡Ø± ÙƒØªØ§Ø¨ Ù‚ÙƒØ± Ø§Ù…Ø§Ù†\n",
      "Fail (got 'ÙƒÙ„Ø§Ø¨ Ø¬Ø§ÛŒ Ø¨ÙŠØª ÙÙŠØ¯ÙŠÙˆ Ø´Ù‡Ø± ÙƒØªØ§Ø¨ Ù‚ÙƒØ± Ø§Ù…Ø§Ù†', expected 'ÙƒÙ„Ø§Ø¨ Ø¬Ø§ÙŠ Ø¨ÙŠØª ÙÙŠØ¯ÙŠÙˆ Ø´Ù‡Ø± ÙƒØªØ§Ø¨ Ù‚ÙƒØ± Ø§Ù…Ø§Ù†')\n",
      "\n",
      "Example 5: Multiple replacement rules in one sentence with Tnkeeh library.\n",
      "TNKEEH branch output: ÙƒÙ„Ø§Ø¨ Ø¬Ø§ÙŠ Ø¨ÙŠØª ÙÙŠØ¯ÙŠÙˆ Ø´Ù‡Ø± ÙƒØªØ§Ø¨ Ù‚ÙƒØ± Ø§Ù…Ø§Ù†\n",
      "\n",
      "Example 6: Multiple replacement rules in one sentence with CAMEL library.\n",
      "CAMEL branch output: ÙƒÙ„Ø§Ø¨ Ø¬Ø§ÙŠ Ø¨ÙŠØª ÙÙŠØ¯ÙŠÙˆ Ø´Ù‡Ø± ÙƒØªØ§Ø¨ Ù‚ÙƒØ± Ø§Ù…Ø§Ù†\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 1: Replace various forms of alif with a bare alif.\")\n",
    "input_text = \"Ø¥Ø³Ù„Ø§Ù…\"\n",
    "expected = \"Ø§Ø³Ù„Ø§Ù…\"  # \"Ø¥\" replaced with \"Ø§\"\n",
    "result = normalize_arabic(input_text, tool=\"other\")\n",
    "print(\"Input: \", input_text)\n",
    "print(\"Excepted: \", expected)\n",
    "print(\"Output: \", result)\n",
    "print(\"Pass\" if result == expected else f\"Fail (got '{result}', expected '{expected}')\")\n",
    "\n",
    "print(\"\\nExample 2: Remove diacritics and perform character replacements.\")\n",
    "input_text = \"Ù…ÙØ¯ÙŽØ±Ù‘ÙØ³Ø©\"  # Contains diacritics and ends with Ø©\n",
    "expected = \"Ù…Ø¯Ø±Ø³Ù‡\"  # Expected: diacritics removed, Ø© -> Ù‡, then stripped by strip_tashkeel (here our dummy returns unchanged)\n",
    "result = normalize_arabic(input_text, tool=\"other\")\n",
    "print(\"Input: \", input_text)\n",
    "print(\"Excepted: \", expected)\n",
    "print(\"Output: \", result)\n",
    "print(\"Pass\" if result == expected else f\"Fail (got '{result}', expected '{expected}')\")\n",
    "\n",
    "print(\"\\nExample 3: Collapse repeated characters.\")\n",
    "input_text = \"Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…\"  # Many repeated Ù…'s\n",
    "expected = \"Ù…Ù…\"  # Reduced to two occurrences\n",
    "result = normalize_arabic(input_text, tool=\"other\")\n",
    "print(\"Input: \", input_text)\n",
    "print(\"Excepted: \", expected)\n",
    "print(\"Output: \", result)\n",
    "print(\"Pass\" if result == expected else f\"Fail (got '{result}', expected '{expected}')\")\n",
    "\n",
    "print(\"\\nExample 4: Multiple replacement rules in one sentence.\")\n",
    "input_text = \"Ú¯Ù„Ø§Ø¨ Ú†Ø§ÛŒ Ù¾ÙŠØª Ú¤ÙŠØ¯ÙŠÙˆ ÚœÙ‡Ø± ÚªØªØ§Ø¨ Ú§ÙƒØ± Ù±Ù…Ø§Ù†\"\n",
    "# Expected replacements:\n",
    "# Ú¯ -> Ùƒ  => \"Ú¯Ù„Ø§Ø¨\" -> \"ÙƒÙ„Ø§Ø¨\"\n",
    "# Ú† -> Ø¬  => \"Ú†Ø§ÛŒ\" -> \"Ø¬Ø§ÙŠ\"\n",
    "# Ù¾ -> Ø¨  => \"Ù¾ÙŠØª\" -> \"Ø¨ÙŠØª\"\n",
    "# Ú¤ -> Ù  => \"Ú¤ÙŠØ¯ÙŠÙˆ\" -> \"ÙÙŠØ¯ÙŠÙˆ\"\n",
    "# Úœ -> Ø´  => \"ÚœÙ‡Ø±\" -> \"Ø´Ù‡Ø±\"\n",
    "# Úª -> Ùƒ  => \"ÚªØªØ§Ø¨\" -> \"ÙƒØªØ§Ø¨\"\n",
    "# Ú§ -> Ù‚  => \"Ú§ÙƒØ±\" -> \"Ù‚ÙƒØ±\"\n",
    "# Ù± -> Ø§  => \"Ù±Ù…Ø§Ù†\" -> \"Ø§Ù…Ø§Ù†\"\n",
    "expected = \"ÙƒÙ„Ø§Ø¨ Ø¬Ø§ÙŠ Ø¨ÙŠØª ÙÙŠØ¯ÙŠÙˆ Ø´Ù‡Ø± ÙƒØªØ§Ø¨ Ù‚ÙƒØ± Ø§Ù…Ø§Ù†\"\n",
    "result = normalize_arabic(input_text, tool=\"other\")\n",
    "print(\"Input: \", input_text)\n",
    "print(\"Excepted: \", expected)\n",
    "print(\"Output: \", result)\n",
    "print(\"Pass\" if result == expected else f\"Fail (got '{result}', expected '{expected}')\")\n",
    "\n",
    "print(\"\\nExample 5: Multiple replacement rules in one sentence with Tnkeeh library.\")\n",
    "input_text = \"ÙƒÙ„Ø§Ø¨ Ø¬Ø§ÙŠ Ø¨ÙŠØª ÙÙŠØ¯ÙŠÙˆ Ø´Ù‡Ø± ÙƒØªØ§Ø¨ Ù‚ÙƒØ± Ø§Ù…Ø§Ù†\"\n",
    "result = normalize_arabic(input_text, tool=\"tnkeeh\")\n",
    "print(\"TNKEEH branch output:\", result)\n",
    "\n",
    "print(\"\\nExample 6: Multiple replacement rules in one sentence with CAMEL library.\")\n",
    "input_text = \"ÙƒÙ„Ø§Ø¨ Ø¬Ø§ÙŠ Ø¨ÙŠØª ÙÙŠØ¯ÙŠÙˆ Ø´Ù‡Ø± ÙƒØªØ§Ø¨ Ù‚ÙƒØ± Ø§Ù…Ø§Ù†\"\n",
    "result = normalize_arabic(input_text, tool=\"camel\")\n",
    "print(\"CAMEL branch output:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Observation:_**</span> The overall normaliztion process of text is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Specific Noise Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Noise Removal:_**</span> extend noise removal to handle more cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_noise(text: str) -> str:\n",
    "    '''\n",
    "    A method that removes specific noise in text such as tatweel, HTML tags, URLs, etc.\n",
    "\n",
    "    :param text: A sentence to be processed.\n",
    "    :return: Cleaned text containing only Arabic letters and whitespace.\n",
    "    '''\n",
    "    # Remove tatweel (Ù€)\n",
    "    text = re.sub(r'\\u0640', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove non-Arabic characters (keep Arabic Unicode block and whitespace)\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Input:     Ù…Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù…ÙˆÙ‚Ø¹Ù†Ø§ <b>Ø§Ù„Ø±Ø§Ø¦Ø¹</b>!\n",
      "Expected:  Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù…ÙˆÙ‚Ø¹Ù†Ø§ Ø§Ù„Ø±Ø§Ø¦Ø¹\n",
      "Got:       Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù…ÙˆÙ‚Ø¹Ù†Ø§ Ø§Ù„Ø±Ø§Ø¦Ø¹\n",
      "Pass: True\n",
      "\n",
      "Exmaple 2\n",
      "Input:     ØªÙØ¶Ù„ Ø¨Ø²ÙŠØ§Ø±Ø© http://example.com Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.\n",
      "Expected:  ØªÙØ¶Ù„ Ø¨Ø²ÙŠØ§Ø±Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\n",
      "Got:       ØªÙØ¶Ù„ Ø¨Ø²ÙŠØ§Ø±Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\n",
      "Pass: True\n",
      "\n",
      "Example 3\n",
      "Input:     Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù…Ø¹ Ø£Ø­Ø±Ù Ù„Ø§ØªÙŠÙ†ÙŠØ© Ù…Ø«Ù„ ABC ÙˆØ£Ø±Ù‚Ø§Ù… 123 ÙˆØ±Ù…ÙˆØ² @#!.\n",
      "Expected:  Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù…Ø¹ Ø£Ø­Ø±Ù Ù„Ø§ØªÙŠÙ†ÙŠØ© Ù…Ø«Ù„ ÙˆØ±Ù…ÙˆØ²\n",
      "Got:       Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù…Ø¹ Ø£Ø­Ø±Ù Ù„Ø§ØªÙŠÙ†ÙŠØ© Ù…Ø«Ù„ ÙˆØ£Ø±Ù‚Ø§Ù… ÙˆØ±Ù…ÙˆØ²\n",
      "Pass: False\n",
      "\n",
      "Example 4\n",
      "Input:        <div>Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø³Ù„Ø§Ù…</div>   \n",
      "Expected:  Ø³Ù„Ø§Ù…\n",
      "Got:       Ø³Ù„Ø§Ù…\n",
      "Pass: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"Example 1: Remove tatweel and HTML tags.\")\n",
    "example1 = \"Ù…Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù…ÙˆÙ‚Ø¹Ù†Ø§ <b>Ø§Ù„Ø±Ø§Ø¦Ø¹</b>!\"\n",
    "# Expected: \"Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù…ÙˆÙ‚Ø¹Ù†Ø§ Ø§Ù„Ø±Ø§Ø¦Ø¹\"\n",
    "result1 = remove_arabic_noise(example1)\n",
    "print(\"Example 1\")\n",
    "print(\"Input:    \", example1)\n",
    "print(\"Expected: \", \"Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù…ÙˆÙ‚Ø¹Ù†Ø§ Ø§Ù„Ø±Ø§Ø¦Ø¹\")\n",
    "print(\"Got:      \", result1)\n",
    "print(\"Pass:\" , result1 == \"Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù…ÙˆÙ‚Ø¹Ù†Ø§ Ø§Ù„Ø±Ø§Ø¦Ø¹\")\n",
    "print()\n",
    "\n",
    "# Example 2: Remove URLs.\n",
    "example2 = \"ØªÙØ¶Ù„ Ø¨Ø²ÙŠØ§Ø±Ø© http://example.com Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.\"\n",
    "# Expected: \"ØªÙØ¶Ù„ Ø¨Ø²ÙŠØ§Ø±Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\"\n",
    "result2 = remove_arabic_noise(example2)\n",
    "print(\"Exmaple 2\")\n",
    "print(\"Input:    \", example2)\n",
    "print(\"Expected: \", \"ØªÙØ¶Ù„ Ø¨Ø²ÙŠØ§Ø±Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\")\n",
    "print(\"Got:      \", result2)\n",
    "print(\"Pass:\" , result2 == \"ØªÙØ¶Ù„ Ø¨Ø²ÙŠØ§Ø±Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\")\n",
    "print()\n",
    "    \n",
    "# Example 3: Remove non-Arabic noise (Latin letters, numbers, punctuation)\n",
    "example3 = \"Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù…Ø¹ Ø£Ø­Ø±Ù Ù„Ø§ØªÙŠÙ†ÙŠØ© Ù…Ø«Ù„ ABC ÙˆØ£Ø±Ù‚Ø§Ù… 123 ÙˆØ±Ù…ÙˆØ² @#!.\"\n",
    "# Expected: \"Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù…Ø¹ Ø£Ø­Ø±Ù Ù„Ø§ØªÙŠÙ†ÙŠØ© Ù…Ø«Ù„ ÙˆØ±Ù…ÙˆØ²\"\n",
    "result3 = remove_arabic_noise(example3)\n",
    "print(\"Example 3\")\n",
    "print(\"Input:    \", example3)\n",
    "print(\"Expected: \", \"Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù…Ø¹ Ø£Ø­Ø±Ù Ù„Ø§ØªÙŠÙ†ÙŠØ© Ù…Ø«Ù„ ÙˆØ±Ù…ÙˆØ²\")\n",
    "print(\"Got:      \", result3)\n",
    "print(\"Pass:\" , result3 == \"Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù…Ø¹ Ø£Ø­Ø±Ù Ù„Ø§ØªÙŠÙ†ÙŠØ© Ù…Ø«Ù„ ÙˆØ±Ù…ÙˆØ²\")\n",
    "print()\n",
    "    \n",
    "# Example 4: Remove extra spaces and HTML tags with tatweel.\n",
    "example4 = \"   <div>Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø³Ù„Ø§Ù…</div>   \"\n",
    "# Expected: \"Ø³Ù„Ø§Ù…\"\n",
    "result4 = remove_arabic_noise(example4)\n",
    "print(\"Example 4\")\n",
    "print(\"Input:    \", example4)\n",
    "print(\"Expected: \", \"Ø³Ù„Ø§Ù…\")\n",
    "print(\"Got:      \", result4)\n",
    "print(\"Pass:\" , result4 == \"Ø³Ù„Ø§Ù…\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Observation:_**</span> The overall removal of noise in text is successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> Look if their exists other types of noise need to be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Tokenization:_**</span> is the process of breaking a sequence of text into smaller units called tokens, such as words, phrases, symbols, and other elements. For the Arabic language, tokenization is a complex task due to the differences between the written and spoken forms of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_arabic(text: str, method='simple', model='msa', scheme='bwtok'):\n",
    "    '''\n",
    "    Tokenizes an Arabic sentence using either a simple or morphological approach.\n",
    "\n",
    "    :param text: The Arabic sentence to be tokenized.\n",
    "    :param method: Tokenization method to use. Options:\n",
    "                   - 'simple': Uses a basic whitespace-based tokenizer.\n",
    "                   - 'morphological': Uses a morphological analyzer for tokenization.\n",
    "    :param model: Specifies the morphological model to use (only applicable if `method='morphological'`).\n",
    "                  Options:\n",
    "                   - 'msa': Modern Standard Arabic (default).\n",
    "                   - 'egy': Egyptian Arabic.\n",
    "    :param scheme: Tokenization scheme for the morphological method. Options:\n",
    "                   - 'bwtok': Buckwalter tokenization (default).\n",
    "                   - 'd3tok': D3 tokenization.\n",
    "                   - 'atbtok': ATB tokenization.\n",
    "\n",
    "    :return: A list of tokenized words.\n",
    "    '''\n",
    "\n",
    "    if method == 'simple':\n",
    "        return simple_word_tokenize(text)\n",
    "    elif method == 'morphological':\n",
    "        words = simple_word_tokenize(text)\n",
    "\n",
    "        if model=='msa':\n",
    "            mle_msa = MLEDisambiguator.pretrained('calima-msa-r13') # Load a pre-trained disambiguator\n",
    "            msa_d3_tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, scheme=scheme)\n",
    "            words = msa_d3_tokenizer.tokenize(words)\n",
    "            return words\n",
    "        else:\n",
    "            mle_egy = MLEDisambiguator.pretrained('calima-egy-r13') # Load a pre-trained disambiguator\n",
    "            egy_bw_tokenizer = MorphologicalTokenizer(disambiguator=mle_egy, scheme='bwtok')\n",
    "            words = egy_bw_tokenizer.tokenize(words)\n",
    "            return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple tokenization: ['Ù‡Ø°Ø§', 'Ù…Ø«Ø§Ù„', 'Ø¹Ù„Ù‰', 'ØªÙ‚Ø·ÙŠØ¹', 'Ø§Ù„Ù†Øµ', 'Ø§Ù„Ø¹Ø±Ø¨ÙŠ', 'Ø¨Ø·Ø±ÙŠÙ‚Ø©', 'Ù…ØªÙ‚Ø¯Ù…Ø©', '.']\n",
      "Morphological tokenization: ['Ù‡Ø°Ø§', 'Ù…Ø«Ø§Ù„', 'Ø¹Ù„Ù‰', 'ØªÙ‚Ø·ÙŠØ¹', 'Ø§Ù„+_Ù†Øµ', 'Ø§Ù„+_Ø¹Ø±Ø¨ÙŠ', 'Ø¨+_Ø·Ø±ÙŠÙ‚_+Ø©', 'Ù…ØªÙ‚Ø¯Ù…_+Ø©', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ù‡Ø°Ø§ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙ‚Ø¯Ù…Ø©.\"\n",
    "simple_tokens = tokenize_arabic(text, 'simple')\n",
    "morphological_tokens = tokenize_arabic(text, 'morphological')\n",
    "\n",
    "print(\"Simple tokenization:\", simple_tokens)\n",
    "print(\"Morphological tokenization:\", morphological_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Dediacritization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Dediacritization:_**</span> Dediacritization is the process of removing Arabic diacritical marks. Diacritics increase data sparsity and so most Arabic NLP techniques ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_dediacrition(text: str, method='remove', tool='pyarabic') -> str:\n",
    "    '''\n",
    "    Removes or normalizes Arabic diacritical marks (Tashkeel).\n",
    "\n",
    "    :param text: An Arabic sentence that requires dediacritization.\n",
    "    :param method: The dediacritization method to apply. Options:\n",
    "                   - 'remove': Removes all diacritics (default).\n",
    "                   - 'normalize': Normalizes Hamza and Shadda while removing other diacritics.\n",
    "                   - 'keep': Keeps the diacritics as they are.\n",
    "    :param tool: The library to use for dediacritization. Options:\n",
    "                 - 'pyarabic': Uses `pyarabic` for diacritic removal (default).\n",
    "                 - 'camel': Uses `camel_tools` for diacritic removal.\n",
    "\n",
    "    :return: A string with the processed text.\n",
    "\n",
    "    **Example Usage:**\n",
    "    >>> text_with_diacritics = \"Ø§Ù„Ù„ÙÙ‘ØºÙŽØ©Ù Ø§Ù„Ø¹ÙŽØ±ÙŽØ¨ÙÙŠÙŽÙ‘Ø©Ù Ø¬ÙŽÙ…ÙÙŠÙ„ÙŽØ©ÙŒ\"\n",
    "    >>> arabic_dediacrition(text_with_diacritics, 'remove')\n",
    "    'Ø§Ù„Ù„ØºÙ‡ Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‡ Ø¬Ù…ÙŠÙ„Ù‡'\n",
    "    \n",
    "    >>> arabic_dediacrition(text_with_diacritics, 'normalize')\n",
    "    'Ø§Ù„Ù„ØºÙ‡ Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‡ Ø¬Ù…ÙŠÙ„Ù‡'\n",
    "\n",
    "    >>> arabic_dediacrition(text_with_diacritics, 'keep')\n",
    "    'Ø§Ù„Ù„ÙÙ‘ØºÙŽØ©Ù Ø§Ù„Ø¹ÙŽØ±ÙŽØ¨ÙÙŠÙŽÙ‘Ø©Ù Ø¬ÙŽÙ…ÙÙŠÙ„ÙŽØ©ÙŒ'\n",
    "    '''\n",
    "\n",
    "    if method == 'remove':\n",
    "        if tool == 'pyarabic':\n",
    "            return araby.strip_diacritics(text)\n",
    "        elif tool == 'camel':\n",
    "            return dediac_ar(text)\n",
    "    elif method == 'normalize':\n",
    "        return araby.normalize_hamza(araby.strip_shadda(text))\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Ø§Ù„Ù„ÙÙ‘ØºÙŽØ©Ù Ø§Ù„Ø¹ÙŽØ±ÙŽØ¨ÙÙŠÙŽÙ‘Ø©Ù Ø¬ÙŽÙ…ÙÙŠÙ„ÙŽØ©ÙŒ\n",
      "Removed diacritics using PyArabic: Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¬Ù…ÙŠÙ„Ø©\n",
      "Removed diacritics using CAMeL: Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¬Ù…ÙŠÙ„Ø©\n",
      "Normalized diacritics: Ø§Ù„Ù„ÙØºÙŽØ©Ù Ø§Ù„Ø¹ÙŽØ±ÙŽØ¨ÙÙŠÙŽØ©Ù Ø¬ÙŽÙ…ÙÙŠÙ„ÙŽØ©ÙŒ\n"
     ]
    }
   ],
   "source": [
    "text_with_diacritics = \"Ø§Ù„Ù„ÙÙ‘ØºÙŽØ©Ù Ø§Ù„Ø¹ÙŽØ±ÙŽØ¨ÙÙŠÙŽÙ‘Ø©Ù Ø¬ÙŽÙ…ÙÙŠÙ„ÙŽØ©ÙŒ\"\n",
    "removed_diacritics_1 = arabic_dediacrition(text_with_diacritics, 'remove', \"pyarabic\")\n",
    "removed_diacritics_2 = arabic_dediacrition(text_with_diacritics, 'remove', \"camel\")\n",
    "normalized_diacritics = arabic_dediacrition(text_with_diacritics, 'normalize')\n",
    "\n",
    "print(\"Original:\", text_with_diacritics)\n",
    "print(\"Removed diacritics using PyArabic:\", removed_diacritics_1)\n",
    "print(\"Removed diacritics using CAMeL:\", removed_diacritics_2)\n",
    "print(\"Normalized diacritics:\", normalized_diacritics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 Dialect Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Dialects Identification:_**</span> is to determine which city-level does a text belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_Note:_**</span> CAMel `DialectIdentifier` is not supported for Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from camel_tools.dialectid import DialectIdentifier\n",
    "\n",
    "def identify_dialect(text: str, target: str) -> list:\n",
    "    \"\"\"\n",
    "    Identifies the dialect of a given Arabic text at various levels of granularity.\n",
    "\n",
    "    This function uses a pretrained dialect identification model from Camel Tools\n",
    "    to determine the dialect of an input text. The model can distinguish among 25\n",
    "    city-level dialects as well as Modern Standard Arabic (MSA). In addition to\n",
    "    city-level identification, the model can provide aggregated predictions at the\n",
    "    regional and country levels. \n",
    "\n",
    "    **Note:** The Camel Tools dialect identification module is not available on Windows.\n",
    "\n",
    "    :param text: A string containing Arabic text.\n",
    "    :param target: A string indicating the level of dialect granularity.\n",
    "                   Options include:\n",
    "                     - \"city\": for fine-grained, city-level dialect identification.\n",
    "                     - \"country\": for aggregated country-level predictions.\n",
    "                     - \"region\": for aggregated region-level predictions.\n",
    "                     - Any other value defaults to the full prediction (typically a list of labels).\n",
    "    :return: A list of predicted dialect labels corresponding to the specified granularity.\n",
    "\n",
    "    **Example:**\n",
    "    >>> text = \"Ù‡Ø°Ø§ Ù†Øµ Ø¹Ø±Ø¨ÙŠ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ©.\"\n",
    "    >>> identify_dialect(text, \"city\")\n",
    "    ['Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©']  # (Example output; actual predictions depend on the pretrained model.)\n",
    "    \"\"\"\n",
    "    did = DialectIdentifier.pretrained()  # Pretrained dialect identification model.\n",
    "    \n",
    "    if target == \"city\":\n",
    "        return did.predict(text, \"city\")\n",
    "    elif target == \"country\":\n",
    "        return did.predict(text, \"country\")\n",
    "    elif target == \"region\":\n",
    "        # Note: Correcting a potential typo from 'predit' to 'predict'\n",
    "        return did.predict(text, \"region\")\n",
    "    else:\n",
    "        return did.predict(text)\n",
    "\n",
    "\n",
    "def normalize_dialect(text: str, target_dialect: str = 'MSA') -> str:\n",
    "    \"\"\"\n",
    "    Normalizes an Arabic text to a specified dialect variant.\n",
    "\n",
    "    This is a placeholder function. In practice, dialect normalization may involve\n",
    "    complex transformations to convert text from one dialect to another. For now,\n",
    "    the function simply returns the original text.\n",
    "\n",
    "    :param text: A string containing Arabic text.\n",
    "    :param target_dialect: A string representing the target dialect for normalization.\n",
    "                           Default is 'MSA' (Modern Standard Arabic).\n",
    "    :return: The input text unmodified (placeholder implementation).\n",
    "\n",
    "    **Example:**\n",
    "    >>> normalize_dialect(\"Ù‡Ø°Ø§ Ù†Øµ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ©\", target_dialect=\"MSA\")\n",
    "    \"Ù‡Ø°Ø§ Ù†Øµ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ©\"\n",
    "    \"\"\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Ø´Ù„ÙˆÙ†Ùƒ Ø­Ø¨ÙŠØ¨ÙŠØŸ Ø´Ø®Ø¨Ø§Ø±Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ\"\n",
    "# dialect = identify_dialect(text)\n",
    "# normalized_text = normalize_dialect(text)\n",
    "\n",
    "# print(\"Original text:\", text)\n",
    "# print(\"Identified dialect:\", dialect)\n",
    "# print(\"Normalized to MSA:\", normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Arabic Dialect Identifier\n",
    "# model_name = \"lafifi-24/arabicBert_arabic_dialect_identification\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# dialect_classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# text = \"Ù‡Ø°Ø§ Ù†Øµ Ø¹Ø±Ø¨ÙŠ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ©.\"\n",
    "\n",
    "# result = dialect_classifier(text)\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Arabic text (dialect identification)\n",
    "# text = \"Ø¹Ø§Ù…Ù„ Ø§Ù‡ ÙŠØ§ ØµØ§Ø­Ø¨ÙŠ.\"\n",
    "\n",
    "# Predictions\n",
    "# result = dialect_classifier(text)\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> look for other tools that perform dialect identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13 Punctuation Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Punctuation Removal:_**</span> is the elimination of any punctuation character-covering both standard English punctuation and common Arabic punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_punctuations(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation characters from an Arabic text.\n",
    "\n",
    "    This function replaces any punctuation characterâ€”covering both standard English punctuation \n",
    "    and common Arabic punctuation marks (e.g., the Arabic comma \"ØŒ\" and question mark \"ØŸ\")â€”with a space.\n",
    "    After replacement, it normalizes the whitespace by collapsing multiple spaces into one and\n",
    "    trimming leading and trailing whitespace.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input Arabic text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The text after removing punctuation and normalizing whitespace.\n",
    "\n",
    "    Example:\n",
    "        >>> remove_arabic_punctuations(\"Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\")\n",
    "        'Ù…Ø±Ø­Ø¨Ø§Ù‹ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ'\n",
    "    \"\"\"\n",
    "\n",
    "    punctuations = \"\"\"!\"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\\]^_`{|}~Ø›\"\"\"\n",
    "\n",
    "    text = re.sub('[%s]' % re.escape(punctuations), ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Pass\n",
      "Input:     Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\n",
      "Expected:  Ù…Ø±Ø­Ø¨Ø§Ù‹ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ\n",
      "Got:       Ù…Ø±Ø­Ø¨Ø§Ù‹ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ\n",
      "\n",
      "Test 2: Pass\n",
      "Input:     Ù‡Ø°Ø§ Ù†Øµ! ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰: Ø¹Ù„Ø§Ù…Ø§ØªØŒ ØªØ±Ù‚ÙŠÙ…ØŸ ÙˆØ£Ø®Ø±Ù‰.\n",
      "Expected:  Ù‡Ø°Ø§ Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ… ÙˆØ£Ø®Ø±Ù‰\n",
      "Got:       Ù‡Ø°Ø§ Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ… ÙˆØ£Ø®Ø±Ù‰\n",
      "\n",
      "Test 3: Pass\n",
      "Input:     \n",
      "Expected:  \n",
      "Got:       \n",
      "\n",
      "Test 4: Pass\n",
      "Input:     !@#$%^&*()\n",
      "Expected:  \n",
      "Got:       \n",
      "\n",
      "Test 5: Pass\n",
      "Input:     Ø³Ù„Ø§Ù… - ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\n",
      "Expected:  Ø³Ù„Ø§Ù… ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ\n",
      "Got:       Ø³Ù„Ø§Ù… ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ\n",
      "\n",
      "Test 6: Pass\n",
      "Input:     ØªØ¬Ø±Ø¨Ø©... Ù…Ø¹ Ù†Ù‚Ø§Ø· Ø«Ù„Ø§Ø«ÙŠØ©!!!\n",
      "Expected:  ØªØ¬Ø±Ø¨Ø© Ù…Ø¹ Ù†Ù‚Ø§Ø· Ø«Ù„Ø§Ø«ÙŠØ©\n",
      "Got:       ØªØ¬Ø±Ø¨Ø© Ù…Ø¹ Ù†Ù‚Ø§Ø· Ø«Ù„Ø§Ø«ÙŠØ©\n",
      "\n",
      "Test 7: Pass\n",
      "Input:     Ù‡Ø°Ø§ØŒ Ø°Ù„ÙƒØ› ÙˆÙ‡Ø°Ø§ØŸ\n",
      "Expected:  Ù‡Ø°Ø§ Ø°Ù„Ùƒ ÙˆÙ‡Ø°Ø§\n",
      "Got:       Ù‡Ø°Ø§ Ø°Ù„Ùƒ ÙˆÙ‡Ø°Ø§\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    # (Input text, Expected output)\n",
    "    (\"Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\", \"Ù…Ø±Ø­Ø¨Ø§Ù‹ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ\"),\n",
    "    (\"Ù‡Ø°Ø§ Ù†Øµ! ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰: Ø¹Ù„Ø§Ù…Ø§ØªØŒ ØªØ±Ù‚ÙŠÙ…ØŸ ÙˆØ£Ø®Ø±Ù‰.\", \"Ù‡Ø°Ø§ Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ… ÙˆØ£Ø®Ø±Ù‰\"),\n",
    "    (\"\", \"\"),\n",
    "    (\"!@#$%^&*()\", \"\"),\n",
    "    (\"Ø³Ù„Ø§Ù… - ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\", \"Ø³Ù„Ø§Ù… ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ\"),\n",
    "    (\"ØªØ¬Ø±Ø¨Ø©... Ù…Ø¹ Ù†Ù‚Ø§Ø· Ø«Ù„Ø§Ø«ÙŠØ©!!!\", \"ØªØ¬Ø±Ø¨Ø© Ù…Ø¹ Ù†Ù‚Ø§Ø· Ø«Ù„Ø§Ø«ÙŠØ©\"),\n",
    "    (\"Ù‡Ø°Ø§ØŒ Ø°Ù„ÙƒØ› ÙˆÙ‡Ø°Ø§ØŸ\", \"Ù‡Ø°Ø§ Ø°Ù„Ùƒ ÙˆÙ‡Ø°Ø§\")\n",
    "]\n",
    "\n",
    "for i, (input_text, expected) in enumerate(test_cases, 1):\n",
    "    result = remove_arabic_punctuations(input_text)\n",
    "    status = \"Pass\" if result == expected else \"Fail\"\n",
    "    print(f\"Test {i}: {status}\")\n",
    "    print(\"Input:    \", input_text)\n",
    "    print(\"Expected: \", expected)\n",
    "    print(\"Got:      \", result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14 Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Named entity recognition:_**</span> find and label named entities like proper nouns, organisations, places, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token in an input sentence, `NERecognizer` outputs a label that indicates the type of named-entity.The system outputs one of the following labels for each token: `'B-LOC'`, `'B-ORG'`, `'B-PERS'`, `'B-MISC'`, `'I-LOC'`, `'I-ORG'`, `'I-PERS'`, `'I-MISC'`, `'O'`.\n",
    "Named-entites can either be a `LOC` (location), `ORG` (organization), `PERS` (person), or `MISC` (miscallaneous).\n",
    "\n",
    "Labels beginning with `B` indicate that their corresponding tokens are the begininging of a multi-word named-entity or is a single-token named-entity'. Those begining with `I` indicate that their corresponding tokens are continuations of a multi-word named-entity. Words that aren't named-entities are given the `'O'` label.\n",
    "\n",
    "The example below illustrates how `NERecognizer` can be used to label named-entities in a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_arabic_entities(text: str, tool: str = \"nltk\") -> list:\n",
    "    \"\"\"\n",
    "    Recognize named entities in an Arabic sentence using a pretrained NER model.\n",
    "\n",
    "    For each token in the input sentence, the model outputs a label indicating\n",
    "    its named-entity type. The possible labels are:\n",
    "        - 'B-LOC', 'B-ORG', 'B-PERS', 'B-MISC': The beginning of a location,\n",
    "          organization, person, or miscellaneous entity (or a single-token entity).\n",
    "        - 'I-LOC', 'I-ORG', 'I-PERS', 'I-MISC': Continuation tokens for multi-word entities.\n",
    "        - 'O': A token that does not belong to any named entity.\n",
    "\n",
    "    The function processes the input text, obtains NER labels, and then aggregates\n",
    "    contiguous tokens with the same entity type into a single named entity.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): An Arabic sentence for which named-entity recognition is performed.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains a recognized entity (a string) and its type (e.g., 'LOC', 'ORG').\n",
    "                        If no entities are found, an empty list is returned.\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"ÙŠØ¹ÙŠØ´ Ù…Ø­Ù…Ø¯ ÙÙŠ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© ÙˆÙŠØ¹Ù…Ù„ ÙÙŠ Ø´Ø±ÙƒØ© Ø¬ÙˆØ¬Ù„.\"\n",
    "        >>> recognize_arabic_entities(text)\n",
    "        [('Ù…Ø­Ù…Ø¯', 'PERS'), ('Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©', 'LOC'), ('Ø¬ÙˆØ¬Ù„', 'ORG')]\n",
    "    \"\"\"\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    if tool == 'camel':\n",
    "        ner = NERecognizer.pretrained()\n",
    "        labels = ner.predict_sentence(simple_word_tokenize(text))\n",
    "\n",
    "        print(\"Raw labels: \", labels)\n",
    "    \n",
    "        words = simple_word_tokenize(text)\n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        \n",
    "        for word, label in zip(words, labels):\n",
    "            if label.startswith('B-'):\n",
    "                if current_entity:\n",
    "                    entities.append((' '.join(current_entity), current_label))\n",
    "                    current_entity = []\n",
    "                current_entity.append(word)\n",
    "                current_label = label[2:]\n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                current_entity.append(word)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append((' '.join(current_entity), current_label))\n",
    "                    current_entity = []\n",
    "                    current_label = None\n",
    "        \n",
    "        if current_entity:\n",
    "            entities.append((' '.join(current_entity), current_label))\n",
    "        \n",
    "        return entities\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\mazen\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw labels:  ['O', 'B-PERS', 'O', 'B-LOC', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "Text: ÙŠØ¹ÙŠØ´ Ù…Ø­Ù…Ø¯ ÙÙŠ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© ÙˆÙŠØ¹Ù…Ù„ ÙÙŠ Ø´Ø±ÙƒØ© Ø¬ÙˆØ¬Ù„.\n",
      "Recognized Entities:\n",
      "Ù…Ø­Ù…Ø¯ -> PERS\n",
      "Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© -> LOC\n",
      "Ø¬ÙˆØ¬Ù„ -> ORG\n"
     ]
    }
   ],
   "source": [
    "text = \"ÙŠØ¹ÙŠØ´ Ù…Ø­Ù…Ø¯ ÙÙŠ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© ÙˆÙŠØ¹Ù…Ù„ ÙÙŠ Ø´Ø±ÙƒØ© Ø¬ÙˆØ¬Ù„.\"\n",
    "\n",
    "entities = recognize_arabic_entities(text, tool=\"camel\")\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Recognized Entities:\")\n",
    "for entity, entity_type in entities:\n",
    "    print(f\"{entity} -> {entity_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.15 Morphological Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Morphological Analysis:_**</span> is the process of generating all possible readings (analyses) of a given word out of context. All analyses are generated from the undiacritized form of the input word. Each of these analyses is defined by a set lexical and morphological features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_morph_analysis(text: str):\n",
    "    \"\"\"\n",
    "    Perform morphological analysis on an Arabic word or phrase.\n",
    "\n",
    "    This function generates all possible morphological readings (analyses) for the input text \n",
    "    out of context. It loads the built-in morphological database (designed primarily for Modern \n",
    "    Standard Arabic) and uses it to analyze the given word or phrase. Each analysis typically \n",
    "    includes information such as the token, its lemma, root, part-of-speech (POS), and other \n",
    "    morphological features.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): An Arabic word or phrase to be analyzed. Note that the analysis is performed \n",
    "                    out-of-context, so the output represents all possible morphological interpretations.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of analysis results. Each element in the list is usually a dictionary \n",
    "              containing morphological details (e.g., 'token', 'lemma', 'root', 'pos', etc.).\n",
    "\n",
    "    Examples:\n",
    "        >>> # Analyze the verb \"ÙŠØ°Ù‡Ø¨\"\n",
    "        >>> analyses = arabic_morph_analysis(\"ÙŠØ°Ù‡Ø¨\")\n",
    "        >>> for analysis in analyses:\n",
    "        ...     print(analysis)\n",
    "        {'token': 'ÙŠØ°Ù‡Ø¨', 'lemma': 'Ø°Ù‡Ø¨', 'root': 'Ø° Ù‡ Ø¨', 'pos': 'ÙØ¹Ù„', ...}\n",
    "        \n",
    "        >>> # Analyze another word\n",
    "        >>> results = arabic_morph_analysis(\"ÙƒØªØ¨Øª\")\n",
    "        >>> print(results)\n",
    "        [{'token': 'ÙƒØªØ¨Øª', 'lemma': 'ÙƒØªØ¨', 'root': 'Ùƒ Øª Ø¨', 'pos': 'ÙØ¹Ù„', ...}, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    db = MorphologyDB.builtin_db()\n",
    "\n",
    "    analyzer = Analyzer(db)\n",
    "\n",
    "    analyses = analyzer.analyze(text)\n",
    "    \n",
    "    return analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing the word: ÙŠØ°Ù‡Ø¨\n",
      "Analysis 1: {'diac': 'ÙŠÙØ°ÙŽÙ‡Ù‘ÙØ¨', 'lex': 'Ø°ÙŽÙ‡Ù‘ÙŽØ¨', 'bw': 'ÙŠÙ/IV3MS+Ø°ÙŽÙ‡Ù‘ÙØ¨/IV', 'gloss': 'he;it+gild', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'u', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙŠÙØ°ÙŽÙ‡Ù‘ÙØ¨', 'caphi': 'y_u_dh_a_h_h_i_b', 'd1tok': 'ÙŠÙØ°ÙŽÙ‡Ù‘ÙØ¨', 'd2tok': 'ÙŠÙØ°ÙŽÙ‡Ù‘ÙØ¨', 'pos_logprob': -1.023208, 'd3tok': 'ÙŠÙØ°ÙŽÙ‡Ù‘ÙØ¨', 'd2seg': 'ÙŠÙØ°ÙŽÙ‡Ù‘ÙØ¨', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'Ø°.Ù‡.Ø¨', 'bwtok': 'ÙŠÙ+_Ø°ÙŽÙ‡Ù‘ÙØ¨', 'pattern': 'ÙŠÙ1ÙŽ2Ù‘Ù3', 'lex_logprob': -99.0, 'atbtok': 'ÙŠÙØ°ÙŽÙ‡Ù‘ÙØ¨', 'atbseg': 'ÙŠÙØ°ÙŽÙ‡Ù‘ÙØ¨', 'd1seg': 'ÙŠÙØ°ÙŽÙ‡Ù‘ÙØ¨', 'stem': 'Ø°ÙŽÙ‡Ù‘ÙØ¨', 'stemgloss': 'gild', 'stemcat': 'IV_yu'}\n",
      "Analysis 2: {'diac': 'ÙŠÙØ°Ù’Ù‡ÙØ¨', 'lex': 'Ø£ÙŽØ°Ù’Ù‡ÙŽØ¨', 'bw': 'ÙŠÙ/IV3MS+Ø°Ù’Ù‡ÙØ¨/IV', 'gloss': 'he;it+remove;eliminate', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'u', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙŠÙØ°Ù’Ù‡ÙØ¨', 'caphi': 'y_u_dh_h_i_b', 'd1tok': 'ÙŠÙØ°Ù’Ù‡ÙØ¨', 'd2tok': 'ÙŠÙØ°Ù’Ù‡ÙØ¨', 'pos_logprob': -1.023208, 'd3tok': 'ÙŠÙØ°Ù’Ù‡ÙØ¨', 'd2seg': 'ÙŠÙØ°Ù’Ù‡ÙØ¨', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'Ø°.Ù‡.Ø¨', 'bwtok': 'ÙŠÙ+_Ø°Ù’Ù‡ÙØ¨', 'pattern': 'ÙŠÙ1Ù’2Ù3', 'lex_logprob': -99.0, 'atbtok': 'ÙŠÙØ°Ù’Ù‡ÙØ¨', 'atbseg': 'ÙŠÙØ°Ù’Ù‡ÙØ¨', 'd1seg': 'ÙŠÙØ°Ù’Ù‡ÙØ¨', 'stem': 'Ø°Ù’Ù‡ÙØ¨', 'stemgloss': 'remove;eliminate', 'stemcat': 'IV_yu'}\n",
      "Analysis 3: {'diac': 'ÙŠÙØ°Ù’Ù‡ÙŽØ¨', 'lex': 'Ø£ÙŽØ°Ù’Ù‡ÙŽØ¨', 'bw': 'ÙŠÙ/IV3MS+Ø°Ù’Ù‡ÙŽØ¨/IV_PASS', 'gloss': 'he;it+be_removed;be_eliminated', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'p', 'mod': 'u', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙŠÙØ°Ù’Ù‡ÙŽØ¨', 'caphi': 'y_u_dh_h_a_b', 'd1tok': 'ÙŠÙØ°Ù’Ù‡ÙŽØ¨', 'd2tok': 'ÙŠÙØ°Ù’Ù‡ÙŽØ¨', 'pos_logprob': -1.023208, 'd3tok': 'ÙŠÙØ°Ù’Ù‡ÙŽØ¨', 'd2seg': 'ÙŠÙØ°Ù’Ù‡ÙŽØ¨', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB-PASS', 'root': 'Ø°.Ù‡.Ø¨', 'bwtok': 'ÙŠÙ+_Ø°Ù’Ù‡ÙŽØ¨', 'pattern': 'ÙŠÙ1Ù’2ÙŽ3', 'lex_logprob': -99.0, 'atbtok': 'ÙŠÙØ°Ù’Ù‡ÙŽØ¨', 'atbseg': 'ÙŠÙØ°Ù’Ù‡ÙŽØ¨', 'd1seg': 'ÙŠÙØ°Ù’Ù‡ÙŽØ¨', 'stem': 'Ø°Ù’Ù‡ÙŽØ¨', 'stemgloss': 'be_removed;be_eliminated', 'stemcat': 'IV_Pass_yu'}\n",
      "Analysis 4: {'diac': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'lex': 'Ø°ÙŽÙ‡ÙŽØ¨', 'bw': 'ÙŠÙŽ/IV3MS+Ø°Ù’Ù‡ÙŽØ¨/IV', 'gloss': 'he;it+go;depart', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'u', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'caphi': 'y_a_dh_h_a_b', 'd1tok': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'd2tok': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'pos_logprob': -1.023208, 'd3tok': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'd2seg': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'pos_lex_logprob': -3.895401, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'Ø°.Ù‡.Ø¨', 'bwtok': 'ÙŠÙŽ+_Ø°Ù’Ù‡ÙŽØ¨', 'pattern': 'ÙŠÙŽ1Ù’2ÙŽ3', 'lex_logprob': -3.895401, 'atbtok': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'atbseg': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'd1seg': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'stem': 'Ø°Ù’Ù‡ÙŽØ¨', 'stemgloss': 'go;depart', 'stemcat': 'IV'}\n",
      "Analysis 5: {'diac': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'lex': 'Ø°ÙŽÙ‡ÙŽØ¨', 'bw': 'ÙŠÙŽ/IV3MS+Ø°Ù’Ù‡ÙŽØ¨/IV', 'gloss': 'he;it+take_(with)', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'i', 'vox': 'a', 'mod': 'u', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'caphi': 'y_a_dh_h_a_b', 'd1tok': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'd2tok': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'pos_logprob': -1.023208, 'd3tok': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'd2seg': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'Ø°.Ù‡.Ø¨', 'bwtok': 'ÙŠÙŽ+_Ø°Ù’Ù‡ÙŽØ¨', 'pattern': 'ÙŠÙŽ1Ù’2ÙŽ3', 'lex_logprob': -99.0, 'atbtok': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'atbseg': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'd1seg': 'ÙŠÙŽØ°Ù’Ù‡ÙŽØ¨', 'stem': 'Ø°Ù’Ù‡ÙŽØ¨', 'stemgloss': 'take_(with)', 'stemcat': 'IV'}\n",
      "\n",
      "Analyzing the word: ÙƒØªØ¨Øª\n",
      "Analysis 1: {'diac': 'ÙƒÙØªÙØ¨ÙŽØª', 'lex': 'ÙƒÙŽØªÙŽØ¨', 'bw': 'ÙƒÙØªÙØ¨/PV_PASS+ÙŽØª/PVSUFF_SUBJ:3FS', 'gloss': 'be_written;be_fated;be_destined+it;they;she_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'p', 'vox': 'p', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'ÙƒÙØªÙØ¨ÙŽØª', 'caphi': 'k_u_t_i_b_a_t', 'd1tok': 'ÙƒÙØªÙØ¨ÙŽØª', 'd2tok': 'ÙƒÙØªÙØ¨ÙŽØª', 'pos_logprob': -1.023208, 'd3tok': 'ÙƒÙØªÙØ¨ÙŽØª', 'd2seg': 'ÙƒÙØªÙØ¨ÙŽØª', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'f', 'catib6': 'VRB-PASS', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'ÙƒÙØªÙØ¨_+ÙŽØª', 'pattern': '1Ù2Ù3ÙŽØª', 'lex_logprob': -3.648503, 'atbtok': 'ÙƒÙØªÙØ¨ÙŽØª', 'atbseg': 'ÙƒÙØªÙØ¨ÙŽØª', 'd1seg': 'ÙƒÙØªÙØ¨ÙŽØª', 'stem': 'ÙƒÙØªÙØ¨', 'stemgloss': 'be_written;be_fated;be_destined', 'stemcat': 'PV_Pass'}\n",
      "Analysis 2: {'diac': 'ÙƒÙØªÙØ¨Ù’ØªÙŽ', 'lex': 'ÙƒÙŽØªÙŽØ¨', 'bw': 'ÙƒÙØªÙØ¨/PV_PASS+ØªÙŽ/PVSUFF_SUBJ:2MS', 'gloss': 'be_written;be_fated;be_destined+you_[masc.sg.]_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '2', 'asp': 'p', 'vox': 'p', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙƒÙØªÙØ¨Ù’ØªÙŽ', 'caphi': 'k_u_t_i_b_t_a', 'd1tok': 'ÙƒÙØªÙØ¨Ù’ØªÙŽ', 'd2tok': 'ÙƒÙØªÙØ¨Ù’ØªÙŽ', 'pos_logprob': -1.023208, 'd3tok': 'ÙƒÙØªÙØ¨Ù’ØªÙŽ', 'd2seg': 'ÙƒÙØªÙØ¨Ù’ØªÙŽ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB-PASS', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'ÙƒÙØªÙØ¨_+ØªÙŽ', 'pattern': '1Ù2Ù3Ù’ØªÙŽ', 'lex_logprob': -3.648503, 'atbtok': 'ÙƒÙØªÙØ¨Ù’ØªÙŽ', 'atbseg': 'ÙƒÙØªÙØ¨Ù’ØªÙŽ', 'd1seg': 'ÙƒÙØªÙØ¨Ù’ØªÙŽ', 'stem': 'ÙƒÙØªÙØ¨', 'stemgloss': 'be_written;be_fated;be_destined', 'stemcat': 'PV_Pass'}\n",
      "Analysis 3: {'diac': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'lex': 'ÙƒÙŽØªÙŽØ¨', 'bw': 'ÙƒÙØªÙØ¨/PV_PASS+ØªÙ/PVSUFF_SUBJ:1S', 'gloss': 'be_written;be_fated;be_destined+I_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '1', 'asp': 'p', 'vox': 'p', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'caphi': 'k_u_t_i_b_t_u', 'd1tok': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'd2tok': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'pos_logprob': -1.023208, 'd3tok': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'd2seg': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB-PASS', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'ÙƒÙØªÙØ¨_+ØªÙ', 'pattern': '1Ù2Ù3Ù’ØªÙ', 'lex_logprob': -3.648503, 'atbtok': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'atbseg': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'd1seg': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'stem': 'ÙƒÙØªÙØ¨', 'stemgloss': 'be_written;be_fated;be_destined', 'stemcat': 'PV_Pass'}\n",
      "Analysis 4: {'diac': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'lex': 'ÙƒÙŽØªÙŽØ¨', 'bw': 'ÙƒÙØªÙØ¨/PV_PASS+ØªÙ/PVSUFF_SUBJ:2FS', 'gloss': 'be_written;be_fated;be_destined+you_[fem.sg.]_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '2', 'asp': 'p', 'vox': 'p', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'caphi': 'k_u_t_i_b_t_i', 'd1tok': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'd2tok': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'pos_logprob': -1.023208, 'd3tok': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'd2seg': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'f', 'catib6': 'VRB-PASS', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'ÙƒÙØªÙØ¨_+ØªÙ', 'pattern': '1Ù2Ù3Ù’ØªÙ', 'lex_logprob': -3.648503, 'atbtok': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'atbseg': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'd1seg': 'ÙƒÙØªÙØ¨Ù’ØªÙ', 'stem': 'ÙƒÙØªÙØ¨', 'stemgloss': 'be_written;be_fated;be_destined', 'stemcat': 'PV_Pass'}\n",
      "Analysis 5: {'diac': 'ÙƒÙŽØªÙŽØ¨ÙŽØª', 'lex': 'ÙƒÙŽØªÙŽØ¨', 'bw': 'ÙƒÙŽØªÙŽØ¨/PV+ÙŽØª/PVSUFF_SUBJ:3FS', 'gloss': 'write+it;they;she_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '3', 'asp': 'p', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'ÙƒÙŽØªÙŽØ¨ÙŽØª', 'caphi': 'k_a_t_a_b_a_t', 'd1tok': 'ÙƒÙŽØªÙŽØ¨ÙŽØª', 'd2tok': 'ÙƒÙŽØªÙŽØ¨ÙŽØª', 'pos_logprob': -1.023208, 'd3tok': 'ÙƒÙŽØªÙŽØ¨ÙŽØª', 'd2seg': 'ÙƒÙŽØªÙŽØ¨ÙŽØª', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'f', 'catib6': 'VRB', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'ÙƒÙŽØªÙŽØ¨_+ÙŽØª', 'pattern': '1ÙŽ2ÙŽ3ÙŽØª', 'lex_logprob': -3.648503, 'atbtok': 'ÙƒÙŽØªÙŽØ¨ÙŽØª', 'atbseg': 'ÙƒÙŽØªÙŽØ¨ÙŽØª', 'd1seg': 'ÙƒÙŽØªÙŽØ¨ÙŽØª', 'stem': 'ÙƒÙŽØªÙŽØ¨', 'stemgloss': 'write', 'stemcat': 'PV'}\n",
      "Analysis 6: {'diac': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙŽ', 'lex': 'ÙƒÙŽØªÙŽØ¨', 'bw': 'ÙƒÙŽØªÙŽØ¨/PV+ØªÙŽ/PVSUFF_SUBJ:2MS', 'gloss': 'write+you_[masc.sg.]_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '2', 'asp': 'p', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙŽ', 'caphi': 'k_a_t_a_b_t_a', 'd1tok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙŽ', 'd2tok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙŽ', 'pos_logprob': -1.023208, 'd3tok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙŽ', 'd2seg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙŽ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'ÙƒÙŽØªÙŽØ¨_+ØªÙŽ', 'pattern': '1ÙŽ2ÙŽ3Ù’ØªÙŽ', 'lex_logprob': -3.648503, 'atbtok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙŽ', 'atbseg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙŽ', 'd1seg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙŽ', 'stem': 'ÙƒÙŽØªÙŽØ¨', 'stemgloss': 'write', 'stemcat': 'PV'}\n",
      "Analysis 7: {'diac': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'lex': 'ÙƒÙŽØªÙŽØ¨', 'bw': 'ÙƒÙŽØªÙŽØ¨/PV+ØªÙ/PVSUFF_SUBJ:1S', 'gloss': 'write+I_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '1', 'asp': 'p', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'caphi': 'k_a_t_a_b_t_u', 'd1tok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'd2tok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'pos_logprob': -1.023208, 'd3tok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'd2seg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'm', 'catib6': 'VRB', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'ÙƒÙŽØªÙŽØ¨_+ØªÙ', 'pattern': '1ÙŽ2ÙŽ3Ù’ØªÙ', 'lex_logprob': -3.648503, 'atbtok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'atbseg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'd1seg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'stem': 'ÙƒÙŽØªÙŽØ¨', 'stemgloss': 'write', 'stemcat': 'PV'}\n",
      "Analysis 8: {'diac': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'lex': 'ÙƒÙŽØªÙŽØ¨', 'bw': 'ÙƒÙŽØªÙŽØ¨/PV+ØªÙ/PVSUFF_SUBJ:2FS', 'gloss': 'write+you_[fem.sg.]_<verb>', 'pos': 'verb', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': '2', 'asp': 'p', 'vox': 'a', 'mod': 'i', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'caphi': 'k_a_t_a_b_t_i', 'd1tok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'd2tok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'pos_logprob': -1.023208, 'd3tok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'd2seg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'pos_lex_logprob': -3.648503, 'num': 's', 'ud': 'VERB', 'gen': 'f', 'catib6': 'VRB', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'ÙƒÙŽØªÙŽØ¨_+ØªÙ', 'pattern': '1ÙŽ2ÙŽ3Ù’ØªÙ', 'lex_logprob': -3.648503, 'atbtok': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'atbseg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'd1seg': 'ÙƒÙŽØªÙŽØ¨Ù’ØªÙ', 'stem': 'ÙƒÙŽØªÙŽØ¨', 'stemgloss': 'write', 'stemcat': 'PV'}\n",
      "Analysis 9: {'diac': 'ÙƒÙŽØªÙØ¨ÙØª', 'lex': 'ØªÙØ¨ÙØª', 'bw': 'ÙƒÙŽ/PREP+ØªÙØ¨ÙØª/NOUN_PROP', 'gloss': 'like;such_as+Tibet', 'pos': 'noun_prop', 'prc3': '0', 'prc2': '0', 'prc1': 'ka_prep', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'u', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙƒÙŽ+_ØªÙØ¨ÙØª', 'caphi': 'k_a_t_i_b_i_t', 'd1tok': 'ÙƒÙŽØªÙØ¨ÙØª', 'd2tok': 'ÙƒÙŽ+_ØªÙØ¨ÙØª', 'pos_logprob': -1.047404, 'd3tok': 'ÙƒÙŽ+_ØªÙØ¨ÙØª', 'd2seg': 'ÙƒÙŽ+_ØªÙØ¨ÙØª', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'ADP+PROPN', 'gen': 'f', 'catib6': 'PRT+PROP', 'root': 'NTWS', 'bwtok': 'ÙƒÙŽ+_ØªÙØ¨ÙØª', 'pattern': 'ÙƒÙŽNTWS', 'lex_logprob': -99.0, 'atbtok': 'ÙƒÙŽ+_ØªÙØ¨ÙØª', 'atbseg': 'ÙƒÙŽ+_ØªÙØ¨ÙØª', 'd1seg': 'ÙƒÙŽØªÙØ¨ÙØª', 'stem': 'ØªÙØ¨ÙØª', 'stemgloss': 'Tibet', 'stemcat': 'N'}\n",
      "Analysis 10: {'diac': 'ÙƒÙŽØªÙØ¨ÙØªÙ', 'lex': 'ØªÙØ¨ÙØª', 'bw': 'ÙƒÙŽ/PREP+ØªÙØ¨ÙØª/NOUN_PROP+Ù/CASE_DEF_GEN', 'gloss': 'like;such_as+Tibet+[def.gen.]', 'pos': 'noun_prop', 'prc3': '0', 'prc2': '0', 'prc1': 'ka_prep', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'caphi': 'k_a_t_i_b_i_t_i', 'd1tok': 'ÙƒÙŽØªÙØ¨ÙØªÙ', 'd2tok': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'pos_logprob': -1.047404, 'd3tok': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'd2seg': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'ADP+PROPN', 'gen': 'f', 'catib6': 'PRT+PROP', 'root': 'NTWS', 'bwtok': 'ÙƒÙŽ+_ØªÙØ¨ÙØª_+Ù', 'pattern': 'ÙƒÙŽNTWSÙ', 'lex_logprob': -99.0, 'atbtok': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'atbseg': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'd1seg': 'ÙƒÙŽØªÙØ¨ÙØªÙ', 'stem': 'ØªÙØ¨ÙØª', 'stemgloss': 'Tibet', 'stemcat': 'N'}\n",
      "Analysis 11: {'diac': 'ÙƒÙŽØªÙØ¨ÙØªÙ', 'lex': 'ØªÙØ¨ÙØª', 'bw': 'ÙƒÙŽ/PREP+ØªÙØ¨ÙØª/NOUN_PROP+Ù/CASE_INDEF_GEN', 'gloss': 'like;such_as+Tibet+[indef.gen.]', 'pos': 'noun_prop', 'prc3': '0', 'prc2': '0', 'prc1': 'ka_prep', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'caphi': 'k_a_t_i_b_i_t_i_n', 'd1tok': 'ÙƒÙŽØªÙØ¨ÙØªÙ', 'd2tok': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'pos_logprob': -1.047404, 'd3tok': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'd2seg': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'ADP+PROPN', 'gen': 'f', 'catib6': 'PRT+PROP', 'root': 'NTWS', 'bwtok': 'ÙƒÙŽ+_ØªÙØ¨ÙØª_+Ù', 'pattern': 'ÙƒÙŽNTWSÙ', 'lex_logprob': -99.0, 'atbtok': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'atbseg': 'ÙƒÙŽ+_ØªÙØ¨ÙØªÙ', 'd1seg': 'ÙƒÙŽØªÙØ¨ÙØªÙ', 'stem': 'ØªÙØ¨ÙØª', 'stemgloss': 'Tibet', 'stemcat': 'N'}\n",
      "\n",
      "Analyzing the word: Ù…ÙƒØªÙˆØ¨\n",
      "Analysis 1: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/ADJ', 'gloss': 'written', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'u', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'caphi': 'm_a_k_t_uu_b', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3', 'lex_logprob': -4.747338, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 2: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/ADJ+ÙŽ/CASE_DEF_ACC', 'gloss': 'written+[def.acc.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'a', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'caphi': 'm_a_k_t_uu_b_a', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨_+ÙŽ', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3ÙŽ', 'lex_logprob': -4.747338, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 3: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/ADJ+ÙŒ/CASE_INDEF_NOM', 'gloss': 'written+[indef.nom.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'n', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'caphi': 'm_a_k_t_uu_b_u_n', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨_+ÙŒ', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3ÙŒ', 'lex_logprob': -4.747338, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 4: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/ADJ+Ù/CASE_DEF_GEN', 'gloss': 'written+[def.gen.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'g', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'caphi': 'm_a_k_t_uu_b_i', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨_+Ù', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3Ù', 'lex_logprob': -4.747338, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 5: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/ADJ+Ù/CASE_INDEF_GEN', 'gloss': 'written+[indef.gen.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'g', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'caphi': 'm_a_k_t_uu_b_i_n', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨_+Ù', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3Ù', 'lex_logprob': -4.747338, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 6: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/ADJ+Ù/CASE_DEF_NOM', 'gloss': 'written+[def.nom.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'n', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'caphi': 'm_a_k_t_uu_b_u', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_lex_logprob': -4.747338, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨_+Ù', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3Ù', 'lex_logprob': -4.747338, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'written', 'stemcat': 'N-ap'}\n",
      "Analysis 7: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/NOUN', 'gloss': 'letter;message', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'u', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'caphi': 'm_a_k_t_uu_b', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'pos_logprob': -0.4344233, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3', 'lex_logprob': -99.0, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n",
      "Analysis 8: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/NOUN+ÙŽ/CASE_DEF_ACC', 'gloss': 'letter;message+[def.acc.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'a', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'caphi': 'm_a_k_t_uu_b_a', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'pos_logprob': -0.4344233, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨_+ÙŽ', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3ÙŽ', 'lex_logprob': -99.0, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŽ', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n",
      "Analysis 9: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/NOUN+ÙŒ/CASE_INDEF_NOM', 'gloss': 'letter;message+[indef.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'n', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'caphi': 'm_a_k_t_uu_b_u_n', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'pos_logprob': -0.4344233, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨_+ÙŒ', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3ÙŒ', 'lex_logprob': -99.0, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨ÙŒ', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n",
      "Analysis 10: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/NOUN+Ù/CASE_DEF_GEN', 'gloss': 'letter;message+[def.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'caphi': 'm_a_k_t_uu_b_i', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_logprob': -0.4344233, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨_+Ù', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3Ù', 'lex_logprob': -99.0, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n",
      "Analysis 11: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/NOUN+Ù/CASE_INDEF_GEN', 'gloss': 'letter;message+[indef.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'caphi': 'm_a_k_t_uu_b_i_n', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_logprob': -0.4344233, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨_+Ù', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3Ù', 'lex_logprob': -99.0, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n",
      "Analysis 12: {'diac': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'lex': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'bw': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨/NOUN+Ù/CASE_DEF_NOM', 'gloss': 'letter;message+[def.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'n', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'caphi': 'm_a_k_t_uu_b_u', 'd1tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_logprob': -0.4344233, 'd3tok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd2seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨_+Ù', 'pattern': 'Ù…ÙŽ1Ù’2ÙÙˆ3Ù', 'lex_logprob': -99.0, 'atbtok': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'atbseg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'd1seg': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨Ù', 'stem': 'Ù…ÙŽÙƒÙ’ØªÙÙˆØ¨', 'stemgloss': 'letter;message', 'stemcat': 'Ndu'}\n"
     ]
    }
   ],
   "source": [
    "example_words = [\n",
    "    \"ÙŠØ°Ù‡Ø¨\",\n",
    "    \"ÙƒØªØ¨Øª\",\n",
    "    \"Ù…ÙƒØªÙˆØ¨\"\n",
    "]\n",
    "\n",
    "for word in example_words:\n",
    "    print(f\"\\nAnalyzing the word: {word}\")\n",
    "    results = arabic_morph_analysis(word)\n",
    "    if results:\n",
    "        for idx, analysis in enumerate(results, 1):\n",
    "            print(f\"Analysis {idx}: {analysis}\")\n",
    "    else:\n",
    "        print(\"No analyses found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.16 Word Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Word Segmentation:_**</span> is the process of segementing a concatenated Arabic text into a properly spaced sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_word_segmentation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Segment concatenated Arabic text into a properly spaced sentence.\n",
    "\n",
    "    This function uses Camel Tools' MaxMatchSegmenterâ€”a dictionary-based, greedy segmentation \n",
    "    algorithmâ€”to determine the most likely word boundaries in a concatenated Arabic string.\n",
    "    The algorithm attempts to match the longest possible valid words from the beginning of the \n",
    "    string, inserting spaces where appropriate.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): A concatenated Arabic string that requires segmentation.\n",
    "\n",
    "    Returns:\n",
    "        str: The input text segmented into individual words separated by a single space.\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"ÙˆÙ‚Ø§Ù„Ù…ØµØ¯Ø±Ø¥Ù†Ù‡Ù†Ø§ÙƒØªØ­Ø³Ù†Ø§ÙÙŠØ§Ù„ÙˆØ¶Ø¹\"\n",
    "        >>> arabic_word_segmentation(text)\n",
    "        \"ÙˆÙ‚ Ø§Ù„Ù…ØµØ¯Ø± Ø¥Ù†Ù‡ Ù†Ø§ ÙƒØª Ø­Ø³ Ù†Ø§ ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹\"\n",
    "        # (Actual segmentation may vary based on the dictionary and algorithm.)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ÙˆÙ‚Ø§Ù„Ù…ØµØ¯Ø±Ø¥Ù†Ù‡Ù†Ø§ÙƒØªØ­Ø³Ù†Ø§ÙÙŠØ§Ù„ÙˆØ¶Ø¹\n",
      "Segmented: ['ÙˆÙ‚Ø§Ù„Ù…ØµØ¯Ø±Ø¥Ù†Ù‡Ù†Ø§ÙƒØªØ­Ø³Ù†Ø§ÙÙŠØ§Ù„ÙˆØ¶Ø¹']\n"
     ]
    }
   ],
   "source": [
    "text = \"ÙˆÙ‚Ø§Ù„Ù…ØµØ¯Ø±Ø¥Ù†Ù‡Ù†Ø§ÙƒØªØ­Ø³Ù†Ø§ÙÙŠØ§Ù„ÙˆØ¶Ø¹\"\n",
    "segmented_text = tokenize_arabic(text)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Segmented:\", segmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> look into a way on how to implement the method (may be using min-max greedy approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.17 Part-of-speech tagging (POS tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Part-of-speech tagging:_**</span> is the process of determining of tagging a sentence with noun, verb, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_pos_tagging(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Perform part-of-speech (POS) tagging on an Arabic sentence.\n",
    "\n",
    "    This function uses a pre-trained Maximum Likelihood Estimation (MLE) disambiguator along with \n",
    "    a default POS tagger to assign part-of-speech tags to each token in the input Arabic text. \n",
    "    The process involves:\n",
    "      1. Tokenizing the input sentence using a simple word tokenizer.\n",
    "      2. Using the MLE disambiguator to resolve morphological ambiguities.\n",
    "      3. Tagging each token with its corresponding POS tag according to the model's tagging scheme.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): An Arabic sentence that needs POS tagging. The sentence should be in standard \n",
    "                    Arabic script and is expected to be a complete sentence.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples where each tuple contains a token from the input sentence and its \n",
    "              assigned POS tag. For example:\n",
    "              [('Ø§Ù„Ø·Ù„Ø§Ø¨', 'NOUN'), ('ÙŠØ°Ù‡Ø¨ÙˆÙ†', 'VERB'), ('Ø¥Ù„Ù‰', 'PREP'), ('Ø§Ù„Ù…Ø¯Ø±Ø³Ø©', 'NOUN')]\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"Ø§Ù„Ø·Ù„Ø§Ø¨ ÙŠØ°Ù‡Ø¨ÙˆÙ† Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\"\n",
    "        >>> arabic_pos_tagging(text)\n",
    "        [('Ø§Ù„Ø·Ù„Ø§Ø¨', 'NOUN'), ('ÙŠØ°Ù‡Ø¨ÙˆÙ†', 'VERB'), ('Ø¥Ù„Ù‰', 'PREP'), ('Ø§Ù„Ù…Ø¯Ø±Ø³Ø©', 'NOUN')]\n",
    "    \"\"\"\n",
    "    mle = MLEDisambiguator.pretrained()\n",
    "    tagger = DefaultTagger(mle, 'pos')\n",
    "    \n",
    "    sentence = simple_word_tokenize(text)\n",
    "    \n",
    "    pos_tags = tagger.tag(sentence)\n",
    "    \n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Basic sentence with a noun, verb, preposition, and noun.\n",
      "Input: Ø§Ù„Ø·Ù„Ø§Ø¨ ÙŠØ°Ù‡Ø¨ÙˆÙ† Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\n",
      "Output: ['noun', 'verb', 'prep', 'noun']\n",
      "--------------------------------------------------\n",
      "Example 2: Sentence with a noun, verb, and adverb.\n",
      "Input: Ø§Ù„Ø±Ø¦ÙŠØ³ ÙŠØªØ­Ø¯Ø« Ø¨ÙˆØ¶ÙˆØ­\n",
      "Output: ['noun', 'verb', 'noun']\n",
      "--------------------------------------------------\n",
      "Example 3: Empty string should return an empty list.\n",
      "Input: \n",
      "Output: []\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_sentences = [\n",
    "    (\"Ø§Ù„Ø·Ù„Ø§Ø¨ ÙŠØ°Ù‡Ø¨ÙˆÙ† Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\", \"Basic sentence with a noun, verb, preposition, and noun.\"),\n",
    "    (\"Ø§Ù„Ø±Ø¦ÙŠØ³ ÙŠØªØ­Ø¯Ø« Ø¨ÙˆØ¶ÙˆØ­\", \"Sentence with a noun, verb, and adverb.\"),\n",
    "    (\"\", \"Empty string should return an empty list.\")\n",
    "]\n",
    "\n",
    "for idx, (input_text, description) in enumerate(example_sentences, 1):\n",
    "    print(f\"Example {idx}: {description}\")\n",
    "    print(\"Input:\", input_text)\n",
    "    result = arabic_pos_tagging(input_text)\n",
    "    print(\"Output:\", result)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.18 Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Disambiguation:_**</span> is the process of determining what is the most likely analysis of a word in a given context. Disambiguation is the backbone for many Arabic NLP tasks such as diacritization, POS tagging and morphological tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_disambiguation(text: str, ):\n",
    "    \"\"\"\n",
    "    Perform morphological disambiguation on an Arabic sentence.\n",
    "\n",
    "    This function determines the most likely morphological analysis for each word\n",
    "    in the input text. It uses a pretrained Maximum Likelihood Estimation (MLE)\n",
    "    disambiguator.\n",
    "    \n",
    "    For each word, the disambiguator produces a list of possible analyses sorted from \n",
    "    most likely to least likely. The function extracts the following from the top analysis:\n",
    "        - The diacritized form ('diac')\n",
    "        - The part-of-speech tag ('pos')\n",
    "        - The lemma or lexical form ('lex')\n",
    "    \n",
    "    In cases where a word does not receive any analysis (i.e. the analyses list is empty),\n",
    "    a default value is returned for that token (an empty string for 'diac' and 'lex', and \"O\"\n",
    "    for the POS tag).\n",
    "\n",
    "    Parameters:\n",
    "        text (str): An Arabic sentence to be disambiguated.\n",
    "\n",
    "    Returns:\n",
    "            tuple: Three lists containing the diacritized forms, part-of-speech tags, \n",
    "                   and lemmas for each word in the sentence.\n",
    "    \n",
    "    Example:\n",
    "        >>> text = \"Ø°Ù‡Ø¨ Ø§Ù„Ø±Ø¬Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ù†Ùƒ\"\n",
    "        >>> diacritized, pos_tags, lemmas = arabic_disambiguation(text)\n",
    "        >>> print(lemmas)\n",
    "        ['Ø°Ù‡Ø¨', 'Ø§Ù„Ø±Ø¬Ù„', 'Ø¥Ù„Ù‰', 'Ø§Ù„Ø¨Ù†Ùƒ']\n",
    "    \n",
    "    Note:\n",
    "        Some words may not receive any analysis. In such cases, this function returns default\n",
    "        values (\"\" for diacritized/lemma and \"O\" for POS) for those words.\n",
    "    \"\"\"\n",
    "    mle = MLEDisambiguator.pretrained()\n",
    "    disambig = mle.disambiguate(text.split())\n",
    "    \n",
    "    diacritized = [d.analyses[0].analysis['diac'] if d.analyses else \"\" for d in disambig]\n",
    "    pos_tags    = [d.analyses[0].analysis['pos']  if d.analyses else \"O\" for d in disambig]\n",
    "    lemmas      = [d.analyses[0].analysis['lex']  if d.analyses else \"\" for d in disambig]\n",
    "\n",
    "    return diacritized, pos_tags, lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1: A sentence with a clear verb, noun, preposition, and noun.\n",
      "Input: Ø°Ù‡Ø¨ Ø§Ù„Ø±Ø¬Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ù†Ùƒ\n",
      "Diacritized: ['Ø°ÙŽÙ‡ÙŽØ¨ÙŽ', 'Ø§Ù„Ø±ÙŽØ¬ÙÙ„ÙŽ', 'Ø¥ÙÙ„ÙŽÙ‰', 'Ø§Ù„Ø¨ÙŽÙ†Ù’ÙƒÙ']\n",
      "POS tags:    ['verb', 'noun', 'prep', 'noun']\n",
      "Lemmas:      ['Ø°ÙŽÙ‡ÙŽØ¨', 'Ø±ÙŽØ¬ÙÙ„', 'Ø¥ÙÙ„ÙŽÙ‰', 'Ø¨ÙŽÙ†Ù’Ùƒ']\n",
      "\n",
      "Example 2: A sentence with a verb, noun, and noun.\n",
      "Input: ÙƒØªØ¨ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ø¯Ø±Ø³\n",
      "Diacritized: ['ÙƒÙŽØªÙŽØ¨ÙŽ', 'Ø§Ù„Ø·Ø§Ù„ÙØ¨Ù', 'Ø§Ù„Ø¯ÙŽØ±Ù’Ø³Ù']\n",
      "POS tags:    ['verb', 'noun', 'noun']\n",
      "Lemmas:      ['ÙƒÙŽØªÙŽØ¨', 'Ø·Ø§Ù„ÙØ¨', 'Ø¯ÙŽØ±Ù’Ø³']\n",
      "\n",
      "Example 3: Empty string should return empty lists.\n",
      "Input: \n",
      "Diacritized: []\n",
      "POS tags:    []\n",
      "Lemmas:      []\n"
     ]
    }
   ],
   "source": [
    "example_sentences = [\n",
    "    (\"Ø°Ù‡Ø¨ Ø§Ù„Ø±Ø¬Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ù†Ùƒ\", \"A sentence with a clear verb, noun, preposition, and noun.\"),\n",
    "    (\"ÙƒØªØ¨ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ø¯Ø±Ø³\", \"A sentence with a verb, noun, and noun.\"),\n",
    "    (\"\", \"Empty string should return empty lists.\")\n",
    "]\n",
    "\n",
    "for idx, (sentence, description) in enumerate(example_sentences, 1):\n",
    "    print(f\"\\nExample {idx}: {description}\")\n",
    "    print(\"Input:\", sentence)\n",
    "    try:\n",
    "        result = arabic_disambiguation(sentence)\n",
    "        if isinstance(result, tuple):\n",
    "            diacritized, pos_tags, lemmas = result\n",
    "            print(\"Diacritized:\", diacritized)\n",
    "            print(\"POS tags:   \", pos_tags)\n",
    "            print(\"Lemmas:     \", lemmas)\n",
    "    except Exception as e:\n",
    "        print(\"Error during disambiguation:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.19 Elongated Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Elongated Words:_**</span> is reducing sequences of repeated characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_elongated_words(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize elongated words in Arabic text by reducing sequences of repeated characters.\n",
    "\n",
    "    This function replaces any sequence of a character repeated more than once with exactly two \n",
    "    consecutive occurrences of that character. This helps in standardizing words that are often \n",
    "    elongated in informal text (e.g., social media or SMS) to express emphasis. For instance, \n",
    "    \"Ø¬Ù…ÙŠÙ„Ù„Ù„Ù„Ù„\" would be normalized to \"Ø¬Ù…ÙŠÙ„Ù„\".\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input sentence or word that may contain elongated characters.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized text where any sequence of repeated characters is reduced to two occurrences.\n",
    "\n",
    "    Examples:\n",
    "        >>> normalize_elongated_words(\"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\")\n",
    "        'Ù‡Ù‡'\n",
    "        >>> normalize_elongated_words(\"ÙƒØ¨ÙŠÙŠÙŠÙŠØ±\")\n",
    "        'ÙƒØ¨ÙŠÙŠØ±'\n",
    "        >>> normalize_elongated_words(\"Ù…Ø±Ø±Ø±Ø­Ø¨Ø§Ø§\")\n",
    "        'Ù…Ø±Ø±Ø­Ø¨Ø§Ø§'\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elongated: ÙŠØ§Ø§Ø§Ø§Ø§ Ø³Ù„Ø§Ø§Ø§Ø§Ù… Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ø§Ø§Ù…Ø¬ Ø§Ù„Ø±Ø§Ø§Ø§Ø§Ø¦Ø¹\n",
      "Normalized: ÙŠØ§Ø§ Ø³Ù„Ø§Ø§Ù… Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ø§Ù…Ø¬ Ø§Ù„Ø±Ø§Ø§Ø¦Ø¹\n"
     ]
    }
   ],
   "source": [
    "elongated_text = \"ÙŠØ§Ø§Ø§Ø§Ø§ Ø³Ù„Ø§Ø§Ø§Ø§Ù… Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ø§Ø§Ù…Ø¬ Ø§Ù„Ø±Ø§Ø§Ø§Ø§Ø¦Ø¹\"\n",
    "normalized_text = normalize_elongated_words(elongated_text)\n",
    "print(\"Elongated:\", elongated_text)\n",
    "print(\"Normalized:\", normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.20 Data Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Data Translation:_**</span> process of replacing each arabic word in the text with one of its english translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_arabic_word(text: str) -> list:\n",
    "    '''\n",
    "    A method that replaces each word in the text with one of its english translation.\n",
    "\n",
    "    :param text: a string to be processed\n",
    "    \n",
    "    :return: a list of translated text strings\n",
    "    '''\n",
    "    db = MorphologyDB.builtin_db()\n",
    "    analyzer = Analyzer(db)\n",
    "\n",
    "    words = text.split()\n",
    "    translated_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        analysis = analyzer.analyze(word)\n",
    "        if analysis:\n",
    "            # Pick a random translation\n",
    "            print(len(analysis))\n",
    "            print(analysis)\n",
    "            word_translation = random.choice(analysis)['stemgloss'].split(\";\")[0]\n",
    "            translated_words.append((word, word_translation))\n",
    "\n",
    "    return translated_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "[{'diac': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'lex': 'ÙƒÙØªÙ‘Ø§Ø¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªÙ‘Ø§Ø¨/NOUN', 'gloss': 'the+kuttab_(village_school);Quran_school', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'u', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨', 'caphi': '2_a_l_k_u_t_t_aa_b', 'd1tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'd2tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨', 'd2seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨', 'pattern': 'Ø§Ù„1Ù2Ù‘Ø§3', 'lex_logprob': -99.0, 'atbtok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'atbseg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'd1seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'stem': 'ÙƒÙØªÙ‘Ø§Ø¨', 'stemgloss': 'kuttab_(village_school);Quran_school', 'stemcat': 'N'}, {'diac': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'lex': 'ÙƒÙØªÙ‘Ø§Ø¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªÙ‘Ø§Ø¨/NOUN+ÙŽ/CASE_DEF_ACC', 'gloss': 'the+kuttab_(village_school);Quran_school+[def.acc.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'a', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'caphi': '2_a_l_k_u_t_t_aa_b_a', 'd1tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'd2tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'd2seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨_+ÙŽ', 'pattern': 'Ø§Ù„1Ù2Ù‘Ø§3ÙŽ', 'lex_logprob': -99.0, 'atbtok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'atbseg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'd1seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'stem': 'ÙƒÙØªÙ‘Ø§Ø¨', 'stemgloss': 'kuttab_(village_school);Quran_school', 'stemcat': 'N'}, {'diac': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'lex': 'ÙƒÙØªÙ‘Ø§Ø¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªÙ‘Ø§Ø¨/NOUN+Ù/CASE_DEF_GEN', 'gloss': 'the+kuttab_(village_school);Quran_school+[def.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨Ù', 'caphi': '2_a_l_k_u_t_t_aa_b_i', 'd1tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd2tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd2seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨_+Ù', 'pattern': 'Ø§Ù„1Ù2Ù‘Ø§3Ù', 'lex_logprob': -99.0, 'atbtok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'atbseg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd1seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'stem': 'ÙƒÙØªÙ‘Ø§Ø¨', 'stemgloss': 'kuttab_(village_school);Quran_school', 'stemcat': 'N'}, {'diac': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'lex': 'ÙƒÙØªÙ‘Ø§Ø¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªÙ‘Ø§Ø¨/NOUN+Ù/CASE_DEF_NOM', 'gloss': 'the+kuttab_(village_school);Quran_school+[def.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'n', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨Ù', 'caphi': '2_a_l_k_u_t_t_aa_b_u', 'd1tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd2tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd2seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨_+Ù', 'pattern': 'Ø§Ù„1Ù2Ù‘Ø§3Ù', 'lex_logprob': -99.0, 'atbtok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'atbseg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd1seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'stem': 'ÙƒÙØªÙ‘Ø§Ø¨', 'stemgloss': 'kuttab_(village_school);Quran_school', 'stemcat': 'N'}, {'diac': 'Ø§Ù„ÙƒÙØªØ§Ø¨', 'lex': 'ÙƒÙØªØ§Ø¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªØ§Ø¨/NOUN', 'gloss': 'the+book', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'u', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨', 'caphi': '2_a_l_k_i_t_aa_b', 'd1tok': 'Ø§Ù„ÙƒÙØªØ§Ø¨', 'd2tok': 'Ø§Ù„ÙƒÙØªØ§Ø¨', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨', 'd2seg': 'Ø§Ù„ÙƒÙØªØ§Ø¨', 'pos_lex_logprob': -3.511249, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨', 'pattern': 'Ø§Ù„1Ù2Ø§3', 'lex_logprob': -3.511249, 'atbtok': 'Ø§Ù„ÙƒÙØªØ§Ø¨', 'atbseg': 'Ø§Ù„ÙƒÙØªØ§Ø¨', 'd1seg': 'Ø§Ù„ÙƒÙØªØ§Ø¨', 'stem': 'ÙƒÙØªØ§Ø¨', 'stemgloss': 'book', 'stemcat': 'Ndu'}, {'diac': 'Ø§Ù„ÙƒÙØªØ§Ø¨ÙŽ', 'lex': 'ÙƒÙØªØ§Ø¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªØ§Ø¨/NOUN+ÙŽ/CASE_DEF_ACC', 'gloss': 'the+book+[def.acc.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'a', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨ÙŽ', 'caphi': '2_a_l_k_i_t_aa_b_a', 'd1tok': 'Ø§Ù„ÙƒÙØªØ§Ø¨ÙŽ', 'd2tok': 'Ø§Ù„ÙƒÙØªØ§Ø¨ÙŽ', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨ÙŽ', 'd2seg': 'Ø§Ù„ÙƒÙØªØ§Ø¨ÙŽ', 'pos_lex_logprob': -3.511249, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨_+ÙŽ', 'pattern': 'Ø§Ù„1Ù2Ø§3ÙŽ', 'lex_logprob': -3.511249, 'atbtok': 'Ø§Ù„ÙƒÙØªØ§Ø¨ÙŽ', 'atbseg': 'Ø§Ù„ÙƒÙØªØ§Ø¨ÙŽ', 'd1seg': 'Ø§Ù„ÙƒÙØªØ§Ø¨ÙŽ', 'stem': 'ÙƒÙØªØ§Ø¨', 'stemgloss': 'book', 'stemcat': 'Ndu'}, {'diac': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'lex': 'ÙƒÙØªØ§Ø¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªØ§Ø¨/NOUN+Ù/CASE_DEF_GEN', 'gloss': 'the+book+[def.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨Ù', 'caphi': '2_a_l_k_i_t_aa_b_i', 'd1tok': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'd2tok': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨Ù', 'd2seg': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'pos_lex_logprob': -3.511249, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨_+Ù', 'pattern': 'Ø§Ù„1Ù2Ø§3Ù', 'lex_logprob': -3.511249, 'atbtok': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'atbseg': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'd1seg': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'stem': 'ÙƒÙØªØ§Ø¨', 'stemgloss': 'book', 'stemcat': 'Ndu'}, {'diac': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'lex': 'ÙƒÙØªØ§Ø¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªØ§Ø¨/NOUN+Ù/CASE_DEF_NOM', 'gloss': 'the+book+[def.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'n', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨Ù', 'caphi': '2_a_l_k_i_t_aa_b_u', 'd1tok': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'd2tok': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨Ù', 'd2seg': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'pos_lex_logprob': -3.511249, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªØ§Ø¨_+Ù', 'pattern': 'Ø§Ù„1Ù2Ø§3Ù', 'lex_logprob': -3.511249, 'atbtok': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'atbseg': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'd1seg': 'Ø§Ù„ÙƒÙØªØ§Ø¨Ù', 'stem': 'ÙƒÙØªØ§Ø¨', 'stemgloss': 'book', 'stemcat': 'Ndu'}, {'diac': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'lex': 'ÙƒØ§ØªÙØ¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªÙ‘Ø§Ø¨/NOUN', 'gloss': 'the+authors;writers', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'u', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨', 'caphi': '2_a_l_k_u_t_t_aa_b', 'd1tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'd2tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨', 'd2seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'pos_lex_logprob': -3.909189, 'num': 'p', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨', 'pattern': 'Ø§Ù„1Ù2Ù‘Ø§3', 'lex_logprob': -3.909189, 'atbtok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'atbseg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'd1seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨', 'stem': 'ÙƒÙØªÙ‘Ø§Ø¨', 'stemgloss': 'authors;writers', 'stemcat': 'N'}, {'diac': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'lex': 'ÙƒØ§ØªÙØ¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªÙ‘Ø§Ø¨/NOUN+ÙŽ/CASE_DEF_ACC', 'gloss': 'the+authors;writers+[def.acc.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'a', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'caphi': '2_a_l_k_u_t_t_aa_b_a', 'd1tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'd2tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'd2seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'pos_lex_logprob': -3.909189, 'num': 'p', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨_+ÙŽ', 'pattern': 'Ø§Ù„1Ù2Ù‘Ø§3ÙŽ', 'lex_logprob': -3.909189, 'atbtok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'atbseg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'd1seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨ÙŽ', 'stem': 'ÙƒÙØªÙ‘Ø§Ø¨', 'stemgloss': 'authors;writers', 'stemcat': 'N'}, {'diac': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'lex': 'ÙƒØ§ØªÙØ¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªÙ‘Ø§Ø¨/NOUN+Ù/CASE_DEF_GEN', 'gloss': 'the+authors;writers+[def.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'g', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨Ù', 'caphi': '2_a_l_k_u_t_t_aa_b_i', 'd1tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd2tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd2seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'pos_lex_logprob': -3.909189, 'num': 'p', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨_+Ù', 'pattern': 'Ø§Ù„1Ù2Ù‘Ø§3Ù', 'lex_logprob': -3.909189, 'atbtok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'atbseg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd1seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'stem': 'ÙƒÙØªÙ‘Ø§Ø¨', 'stemgloss': 'authors;writers', 'stemcat': 'N'}, {'diac': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'lex': 'ÙƒØ§ØªÙØ¨', 'bw': 'Ø§Ù„/DET+ÙƒÙØªÙ‘Ø§Ø¨/NOUN+Ù/CASE_DEF_NOM', 'gloss': 'the+authors;writers+[def.nom.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'n', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨Ù', 'caphi': '2_a_l_k_u_t_t_aa_b_u', 'd1tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd2tok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'pos_logprob': -0.4344233, 'd3tok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd2seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'pos_lex_logprob': -3.909189, 'num': 'p', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ùƒ.Øª.Ø¨', 'bwtok': 'Ø§Ù„+_ÙƒÙØªÙ‘Ø§Ø¨_+Ù', 'pattern': 'Ø§Ù„1Ù2Ù‘Ø§3Ù', 'lex_logprob': -3.909189, 'atbtok': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'atbseg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'd1seg': 'Ø§Ù„ÙƒÙØªÙ‘Ø§Ø¨Ù', 'stem': 'ÙƒÙØªÙ‘Ø§Ø¨', 'stemgloss': 'authors;writers', 'stemcat': 'N'}]\n",
      "6\n",
      "[{'diac': 'Ù…ÙÙÙÙŠØ¯', 'lex': 'Ù…ÙÙÙÙŠØ¯', 'bw': 'Ù…ÙÙÙÙŠØ¯/ADJ', 'gloss': 'useful;beneficial', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'u', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙÙÙÙŠØ¯', 'caphi': 'm_u_f_ii_d', 'd1tok': 'Ù…ÙÙÙÙŠØ¯', 'd2tok': 'Ù…ÙÙÙÙŠØ¯', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙÙÙÙŠØ¯', 'd2seg': 'Ù…ÙÙÙÙŠØ¯', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ù.#.Ø¯', 'bwtok': 'Ù…ÙÙÙÙŠØ¯', 'pattern': 'Ù…Ù1ÙÙŠ3', 'lex_logprob': -4.6224, 'atbtok': 'Ù…ÙÙÙÙŠØ¯', 'atbseg': 'Ù…ÙÙÙÙŠØ¯', 'd1seg': 'Ù…ÙÙÙÙŠØ¯', 'stem': 'Ù…ÙÙÙÙŠØ¯', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}, {'diac': 'Ù…ÙÙÙÙŠØ¯ÙŽ', 'lex': 'Ù…ÙÙÙÙŠØ¯', 'bw': 'Ù…ÙÙÙÙŠØ¯/ADJ+ÙŽ/CASE_DEF_ACC', 'gloss': 'useful;beneficial+[def.acc.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'a', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙÙÙÙŠØ¯ÙŽ', 'caphi': 'm_u_f_ii_d_a', 'd1tok': 'Ù…ÙÙÙÙŠØ¯ÙŽ', 'd2tok': 'Ù…ÙÙÙÙŠØ¯ÙŽ', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙÙÙÙŠØ¯ÙŽ', 'd2seg': 'Ù…ÙÙÙÙŠØ¯ÙŽ', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ù.#.Ø¯', 'bwtok': 'Ù…ÙÙÙÙŠØ¯_+ÙŽ', 'pattern': 'Ù…Ù1ÙÙŠ3ÙŽ', 'lex_logprob': -4.6224, 'atbtok': 'Ù…ÙÙÙÙŠØ¯ÙŽ', 'atbseg': 'Ù…ÙÙÙÙŠØ¯ÙŽ', 'd1seg': 'Ù…ÙÙÙÙŠØ¯ÙŽ', 'stem': 'Ù…ÙÙÙÙŠØ¯', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}, {'diac': 'Ù…ÙÙÙÙŠØ¯ÙŒ', 'lex': 'Ù…ÙÙÙÙŠØ¯', 'bw': 'Ù…ÙÙÙÙŠØ¯/ADJ+ÙŒ/CASE_INDEF_NOM', 'gloss': 'useful;beneficial+[indef.nom.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'n', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙÙÙÙŠØ¯ÙŒ', 'caphi': 'm_u_f_ii_d_u_n', 'd1tok': 'Ù…ÙÙÙÙŠØ¯ÙŒ', 'd2tok': 'Ù…ÙÙÙÙŠØ¯ÙŒ', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙÙÙÙŠØ¯ÙŒ', 'd2seg': 'Ù…ÙÙÙÙŠØ¯ÙŒ', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ù.#.Ø¯', 'bwtok': 'Ù…ÙÙÙÙŠØ¯_+ÙŒ', 'pattern': 'Ù…Ù1ÙÙŠ3ÙŒ', 'lex_logprob': -4.6224, 'atbtok': 'Ù…ÙÙÙÙŠØ¯ÙŒ', 'atbseg': 'Ù…ÙÙÙÙŠØ¯ÙŒ', 'd1seg': 'Ù…ÙÙÙÙŠØ¯ÙŒ', 'stem': 'Ù…ÙÙÙÙŠØ¯', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}, {'diac': 'Ù…ÙÙÙÙŠØ¯Ù', 'lex': 'Ù…ÙÙÙÙŠØ¯', 'bw': 'Ù…ÙÙÙÙŠØ¯/ADJ+Ù/CASE_DEF_GEN', 'gloss': 'useful;beneficial+[def.gen.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'g', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙÙÙÙŠØ¯Ù', 'caphi': 'm_u_f_ii_d_i', 'd1tok': 'Ù…ÙÙÙÙŠØ¯Ù', 'd2tok': 'Ù…ÙÙÙÙŠØ¯Ù', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙÙÙÙŠØ¯Ù', 'd2seg': 'Ù…ÙÙÙÙŠØ¯Ù', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ù.#.Ø¯', 'bwtok': 'Ù…ÙÙÙÙŠØ¯_+Ù', 'pattern': 'Ù…Ù1ÙÙŠ3Ù', 'lex_logprob': -4.6224, 'atbtok': 'Ù…ÙÙÙÙŠØ¯Ù', 'atbseg': 'Ù…ÙÙÙÙŠØ¯Ù', 'd1seg': 'Ù…ÙÙÙÙŠØ¯Ù', 'stem': 'Ù…ÙÙÙÙŠØ¯', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}, {'diac': 'Ù…ÙÙÙÙŠØ¯Ù', 'lex': 'Ù…ÙÙÙÙŠØ¯', 'bw': 'Ù…ÙÙÙÙŠØ¯/ADJ+Ù/CASE_INDEF_GEN', 'gloss': 'useful;beneficial+[indef.gen.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'g', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙÙÙÙŠØ¯Ù', 'caphi': 'm_u_f_ii_d_i_n', 'd1tok': 'Ù…ÙÙÙÙŠØ¯Ù', 'd2tok': 'Ù…ÙÙÙÙŠØ¯Ù', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙÙÙÙŠØ¯Ù', 'd2seg': 'Ù…ÙÙÙÙŠØ¯Ù', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ù.#.Ø¯', 'bwtok': 'Ù…ÙÙÙÙŠØ¯_+Ù', 'pattern': 'Ù…Ù1ÙÙŠ3Ù', 'lex_logprob': -4.6224, 'atbtok': 'Ù…ÙÙÙÙŠØ¯Ù', 'atbseg': 'Ù…ÙÙÙÙŠØ¯Ù', 'd1seg': 'Ù…ÙÙÙÙŠØ¯Ù', 'stem': 'Ù…ÙÙÙÙŠØ¯', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}, {'diac': 'Ù…ÙÙÙÙŠØ¯Ù', 'lex': 'Ù…ÙÙÙÙŠØ¯', 'bw': 'Ù…ÙÙÙÙŠØ¯/ADJ+Ù/CASE_DEF_NOM', 'gloss': 'useful;beneficial+[def.nom.]', 'pos': 'adj', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'c', 'cas': 'n', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'Ù…ÙÙÙÙŠØ¯Ù', 'caphi': 'm_u_f_ii_d_u', 'd1tok': 'Ù…ÙÙÙÙŠØ¯Ù', 'd2tok': 'Ù…ÙÙÙÙŠØ¯Ù', 'pos_logprob': -0.9868824, 'd3tok': 'Ù…ÙÙÙÙŠØ¯Ù', 'd2seg': 'Ù…ÙÙÙÙŠØ¯Ù', 'pos_lex_logprob': -4.6224, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': 'Ù.#.Ø¯', 'bwtok': 'Ù…ÙÙÙÙŠØ¯_+Ù', 'pattern': 'Ù…Ù1ÙÙŠ3Ù', 'lex_logprob': -4.6224, 'atbtok': 'Ù…ÙÙÙÙŠØ¯Ù', 'atbseg': 'Ù…ÙÙÙÙŠØ¯Ù', 'd1seg': 'Ù…ÙÙÙÙŠØ¯Ù', 'stem': 'Ù…ÙÙÙÙŠØ¯', 'stemgloss': 'useful;beneficial', 'stemcat': 'N-ap'}]\n",
      "2\n",
      "[{'diac': 'Ù„ÙÙ„Ù‚ÙØ±Ø§Ø¡ÙŽØ©', 'lex': 'Ù‚ÙØ±Ø§Ø¡ÙŽØ©', 'bw': 'Ù„Ù/PREP+Ø§Ù„/DET+Ù‚ÙØ±Ø§Ø¡/NOUN+ÙŽØ©/NSUFF_FEM_SG', 'gloss': 'to;for_+_the+reading+[fem.sg.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': 'li_prep', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'u', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'Ù„Ù+_Ù„+_Ù‚ÙØ±Ø§Ø¡ÙŽØ©', 'caphi': 'l_i_l_q_i_r_aa_2_a', 'd1tok': 'Ù„ÙÙ„Ù‚ÙØ±Ø§Ø¡ÙŽØ©', 'd2tok': 'Ù„Ù+_Ø§Ù„Ù‚ÙØ±Ø§Ø¡ÙŽØ©', 'pos_logprob': -0.4344233, 'd3tok': 'Ù„Ù+_Ø§Ù„+_Ù‚ÙØ±Ø§Ø¡ÙŽØ©', 'd2seg': 'Ù„Ù+_Ù„Ù‚ÙØ±Ø§Ø¡ÙŽØ©', 'pos_lex_logprob': -4.210219, 'num': 's', 'ud': 'ADP+NOUN', 'gen': 'f', 'catib6': 'PRT+NOM', 'root': 'Ù‚.Ø±.#', 'bwtok': 'Ù„Ù+_Ø§Ù„+_Ù‚ÙØ±Ø§Ø¡_+ÙŽØ©', 'pattern': 'Ù„ÙÙ„1Ù2Ø§Ø¡ÙŽØ©', 'lex_logprob': -4.210219, 'atbtok': 'Ù„Ù+_Ø§Ù„Ù‚ÙØ±Ø§Ø¡ÙŽØ©', 'atbseg': 'Ù„Ù+_Ù„Ù‚ÙØ±Ø§Ø¡ÙŽØ©', 'd1seg': 'Ù„ÙÙ„Ù‚ÙØ±Ø§Ø¡ÙŽØ©', 'stem': 'Ù‚ÙØ±Ø§Ø¡', 'stemgloss': 'reading', 'stemcat': 'NapAt'}, {'diac': 'Ù„ÙÙ„Ù‚ÙØ±Ø§Ø¡ÙŽØ©Ù', 'lex': 'Ù‚ÙØ±Ø§Ø¡ÙŽØ©', 'bw': 'Ù„Ù/PREP+Ø§Ù„/DET+Ù‚ÙØ±Ø§Ø¡/NOUN+ÙŽØ©/NSUFF_FEM_SG+Ù/CASE_DEF_GEN', 'gloss': 'to;for_+_the+reading+[fem.sg.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': 'li_prep', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'f', 'form_num': 's', 'd3seg': 'Ù„Ù+_Ù„+_Ù‚ÙØ±Ø§Ø¡ÙŽØ©Ù', 'caphi': 'l_i_l_q_i_r_aa_2_a_t_i', 'd1tok': 'Ù„ÙÙ„Ù‚ÙØ±Ø§Ø¡ÙŽØ©Ù', 'd2tok': 'Ù„Ù+_Ø§Ù„Ù‚ÙØ±Ø§Ø¡ÙŽØ©Ù', 'pos_logprob': -0.4344233, 'd3tok': 'Ù„Ù+_Ø§Ù„+_Ù‚ÙØ±Ø§Ø¡ÙŽØ©Ù', 'd2seg': 'Ù„Ù+_Ù„Ù‚ÙØ±Ø§Ø¡ÙŽØ©Ù', 'pos_lex_logprob': -4.210219, 'num': 's', 'ud': 'ADP+NOUN', 'gen': 'f', 'catib6': 'PRT+NOM', 'root': 'Ù‚.Ø±.#', 'bwtok': 'Ù„Ù+_Ø§Ù„+_Ù‚ÙØ±Ø§Ø¡_+ÙŽØ©_+Ù', 'pattern': 'Ù„ÙÙ„1Ù2Ø§Ø¡ÙŽØ©Ù', 'lex_logprob': -4.210219, 'atbtok': 'Ù„Ù+_Ø§Ù„Ù‚ÙØ±Ø§Ø¡ÙŽØ©Ù', 'atbseg': 'Ù„Ù+_Ù„Ù‚ÙØ±Ø§Ø¡ÙŽØ©Ù', 'd1seg': 'Ù„ÙÙ„Ù‚ÙØ±Ø§Ø¡ÙŽØ©Ù', 'stem': 'Ù‚ÙØ±Ø§Ø¡', 'stemgloss': 'reading', 'stemcat': 'NapAt'}]\n",
      "Original: Ø§Ù„ÙƒØªØ§Ø¨ Ù…ÙÙŠØ¯ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©\n",
      "Tranlated data:\n",
      "1. original: Ø§Ù„ÙƒØªØ§Ø¨, translated: authors\n",
      "2. original: Ù…ÙÙŠØ¯, translated: useful\n",
      "3. original: Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©, translated: reading\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "original_text = \"Ø§Ù„ÙƒØªØ§Ø¨ Ù…ÙÙŠØ¯ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©\"\n",
    "translated_text = translate_arabic_word(original_text)\n",
    "\n",
    "print(\"Original:\", original_text)\n",
    "print(\"Tranlated data:\")\n",
    "for i, (original, translated) in enumerate(translated_text, 1):\n",
    "    print(f\"{i}. original: {original}, translated: {translated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_NOTE:_**</span> The meaning of the text could be altered depending on the tashkeel added by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.21 Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Generation:_**</span> is the process of inflecting a lemma for a set of morphological features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_word_generation(word: str, pos: str = 'noun', gen: str = 'm', num: str = 'p'):\n",
    "    \"\"\"\n",
    "    Inflect an Arabic lemma into its fully diacritized form(s) based on specified morphological features.\n",
    "\n",
    "    This function generates all possible inflected (diacritized) forms for a given Arabic lemma\n",
    "    by applying a set of morphological features. It leverages a built-in morphological database (with\n",
    "    generation flags enabled) and a morphological generator to produce analyses that include details \n",
    "    such as the diacritized form ('diac'), part-of-speech, and other features.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The lemma (base form) of the Arabic word to be inflected.\n",
    "        pos (str, optional): The part-of-speech tag for the word. For example, 'noun' or 'verb'. \n",
    "                             Default is 'noun'.\n",
    "        gen (str, optional): The grammatical gender to be applied. For example, 'm' for masculine or \n",
    "                             'f' for feminine. Default is 'm' (masculine).\n",
    "        num (str, optional): The number specification, such as 's' for singular or 'p' for plural. \n",
    "                             Default is 'p' (plural).\n",
    "\n",
    "    Returns:\n",
    "        set: A set of unique diacritized forms (strings) produced by the morphological generator.\n",
    "             Each element represents a possible inflection for the input word given the features.\n",
    "\n",
    "    Example:\n",
    "        >>> # Inflect the noun \"ÙƒØªØ§Ø¨\" (book) as a masculine plural.\n",
    "        >>> forms = arabic_word_generation(\"ÙƒØªØ§Ø¨\", pos=\"noun\", gen=\"m\", num=\"p\")\n",
    "        >>> print(forms)\n",
    "        {'ÙƒÙØªÙØ¨', 'ÙƒÙØªÙØ¨ÙŒ'}\n",
    "        \n",
    "        >>> # Inflect the adjective \"Ø¬Ø¯ÙŠØ¯\" (new) as a masculine singular.\n",
    "        >>> forms = arabic_word_generation(\"Ø¬Ø¯ÙŠØ¯\", pos=\"adj\", gen=\"m\", num=\"s\")\n",
    "        >>> print(forms)\n",
    "        {'Ø¬ÙŽØ¯ÙÙŠØ¯', 'Ø¬ÙŽØ¯ÙÙŠØ¯ÙŒ'}\n",
    "\n",
    "    Note:\n",
    "        The actual output depends on the underlying morphological database and generator.\n",
    "        Ensure that the necessary classes (e.g., MorphologyDB and Generator) are imported and available.\n",
    "    \"\"\"\n",
    "    db = MorphologyDB.builtin_db(flags='g')\n",
    "    \n",
    "    generator = Generator(db)\n",
    "    \n",
    "    lemma = arabic_lemmatization(word)\n",
    "    \n",
    "    features = {\n",
    "        'pos': pos,\n",
    "        'gen': gen,\n",
    "        'num': num\n",
    "    }\n",
    "    \n",
    "    analyses = generator.generate(lemma, features)\n",
    "    \n",
    "    return set([a['diac'] for a in analyses])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: yellow\">**_Note:_**</span> `'pos'` is the only *required* feature that needs to be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 - Noun (ÙƒØªØ§Ø¨) as masculine plural:\n",
      "Input word: ÙƒØªØ§Ø¨\n",
      "Generated forms: set()\n",
      "--------------------------------------------------\n",
      "Example 2 - Adjective (Ø¬Ø¯ÙŠØ¯) as masculine singular:\n",
      "Input word: Ø¬Ø¯ÙŠØ¯\n",
      "Generated forms: set()\n",
      "--------------------------------------------------\n",
      "Example 3 - Noun (Ù…Ø¹Ù„Ù…Ø©) as feminine singular:\n",
      "Input word: Ù…Ø¹Ù„Ù…Ø©\n",
      "Generated forms: set()\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Inflecting a noun (book) as masculine plural.\n",
    "word1 = \"ÙƒØªØ§Ø¨\"\n",
    "print(\"Example 1 - Noun (ÙƒØªØ§Ø¨) as masculine plural:\")\n",
    "forms1 = arabic_word_generation(word1, pos=\"noun\", gen=\"m\", num=\"p\")\n",
    "print(\"Input word:\", word1)\n",
    "print(\"Generated forms:\", forms1)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Example 2: Inflecting an adjective (new) as masculine singular.\n",
    "word2 = \"Ø¬Ø¯ÙŠØ¯\"\n",
    "print(\"Example 2 - Adjective (Ø¬Ø¯ÙŠØ¯) as masculine singular:\")\n",
    "forms2 = arabic_word_generation(word2, pos=\"adj\", gen=\"m\", num=\"s\")\n",
    "print(\"Input word:\", word2)\n",
    "print(\"Generated forms:\", forms2)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Example 3: Inflecting a noun with feminine features.\n",
    "word3 = \"Ù…Ø¹Ù„Ù…Ø©\"  # base form might be provided without diacritics\n",
    "print(\"Example 3 - Noun (Ù…Ø¹Ù„Ù…Ø©) as feminine singular:\")\n",
    "forms3 = arabic_word_generation(word3, pos=\"noun\", gen=\"f\", num=\"s\")\n",
    "print(\"Input word:\", word3)\n",
    "print(\"Generated forms:\", forms3)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> determine why the Generator is not working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.22 Reinflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Reinflection:_**</span> is the process of converting a given word in any form to a different form (i.e. tense, gender, etc). The CAMeL Tools reinflector works similar to the generator except that the word doesn't have to be a lemma and it is not have to be restricted to a specific `'pos'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_reinflection(word: str, num: str = 'd', prc1: str = 'bi_prep') -> set:\n",
    "    \"\"\"\n",
    "    Generate reinflected forms of an Arabic word based on specified morphological features.\n",
    "\n",
    "    This function takes an input word (typically in its lemma form) and applies reinflection\n",
    "    to produce alternative forms. Reinflection is the process of converting a word into different\n",
    "    forms (e.g., adjusting tense, gender, number, or attaching prefixes) as dictated by the desired\n",
    "    morphological features.\n",
    "\n",
    "    The function uses a built-in morphological database loaded with the 'r' (reinflection)\n",
    "    flag, along with a reinflector object, to produce analyses of the word. It then extracts the\n",
    "    diacritized form ('diac') from each analysis and returns a set of unique reinflected forms.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The Arabic word (typically its lemma) to be reinflected.\n",
    "        num (str, optional): A morphological feature representing number or a related property.\n",
    "                             The default value 'd' indicates a default or unspecified number feature.\n",
    "        prc1 (str, optional): A morphological feature typically used to indicate a proclitic (prefix)\n",
    "                              that might be attached to the word (e.g., a preposition). The default value\n",
    "                              'bi_prep' might indicate a proclitic for the preposition \"Ø¨Ù€\". Adjust these\n",
    "                              features based on your reinflection requirements.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of unique diacritized forms (strings) that represent the reinflected variants of the word,\n",
    "             according to the specified morphological features.\n",
    "\n",
    "    Examples:\n",
    "        >>> # Reinflect the word \"ÙƒØªØ¨\" with default features.\n",
    "        >>> forms = arabic_reinflection(\"ÙƒØªØ¨\")\n",
    "        >>> print(forms)\n",
    "        {'ÙƒÙØªÙØ¨', 'ÙƒÙØªÙØ¨'}  # (Example output; actual forms depend on the database and reinflection rules.)\n",
    "        \n",
    "        >>> # Reinflect the word \"Ø¯Ø±Ø³\" specifying singular number and a proclitic for \"Ø¨Ù€\"\n",
    "        >>> forms = arabic_reinflection(\"Ø¯Ø±Ø³\", num=\"s\", prc1=\"bi_prep\")\n",
    "        >>> print(forms)\n",
    "        {'Ø¯ÙØ±ÙØ³', 'Ø¯ÙØ±ÙØ³ÙŽ'}  # (Example output)\n",
    "\n",
    "    Note:\n",
    "        The actual output is contingent on the underlying morphological database and reinflection rules\n",
    "        provided by the library. Make sure the classes MorphologyDB and Reinflector are correctly imported and\n",
    "        available in your environment.\n",
    "    \"\"\"\n",
    "    db = MorphologyDB.builtin_db(flags='r')\n",
    "\n",
    "    reinflector = Reinflector(db)\n",
    "\n",
    "    features = {\n",
    "        'num': num,\n",
    "        'prc1': prc1\n",
    "    }\n",
    "\n",
    "    analyses = reinflector.reinflect(word, features)\n",
    "\n",
    "    return set(a['diac'] for a in analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Default reinflection for the word 'ÙƒØªØ¨'.\n",
      "Input word: ÙƒØªØ¨\n",
      "Reinflected forms: {'Ø¨ÙÙƒÙØªØ§Ø¨ÙŽÙŠÙ’Ù†Ù', 'Ø¨ÙÙƒÙØªØ§Ø¨ÙŽÙŠÙ’'}\n",
      "--------------------------------------------------\n",
      "Test Case 2: Reinflection for 'Ø¯Ø±Ø³' with singular number and 'Ø¨Ù€' prefix.\n",
      "Input word: Ø¯Ø±Ø³\n",
      "Reinflected forms: {'Ø¨ÙØ¯ÙŽØ±Ù’Ø³', 'Ø¨ÙØ¯ÙŽØ±Ù’Ø³Ù', 'Ø¨ÙØ¯ÙŽØ±Ù’Ø³Ù'}\n",
      "--------------------------------------------------\n",
      "Test Case 3: Reinflection for 'Ø´Ø±Ø¨' with plural number and 'Ø¨Ù€' prefix.\n",
      "Input word: Ø´Ø±Ø¨\n",
      "Reinflected forms: set()\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    (\"ÙƒØªØ¨\", \"d\", \"bi_prep\", \"Default reinflection for the word 'ÙƒØªØ¨'.\"),\n",
    "    (\"Ø¯Ø±Ø³\", \"s\", \"bi_prep\", \"Reinflection for 'Ø¯Ø±Ø³' with singular number and 'Ø¨Ù€' prefix.\"),\n",
    "    (\"Ø´Ø±Ø¨\", \"p\", \"bi_prep\", \"Reinflection for 'Ø´Ø±Ø¨' with plural number and 'Ø¨Ù€' prefix.\"),\n",
    "]\n",
    "\n",
    "for idx, (word, num, prc1, desc) in enumerate(examples, 1):\n",
    "    print(f\"Test Case {idx}: {desc}\")\n",
    "    forms = arabic_reinflection(word, num, prc1)\n",
    "    print(\"Input word:\", word)\n",
    "    print(\"Reinflected forms:\", forms)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.23 Morphological Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Morphological Tokenization:_**</span> is a type of tokenization whereby Arabic words are split into component prefixes, stems, and suffixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MorphologicalTokenizer` class used to tokenize words in different schemes. It behaves very much like the `DefaultTagger` (used previously) in that it uses a disambiguator to first disambiguate words and then extracts a particular tokenization feature, but it has the following differences:\n",
    "\n",
    "- While the `DefaultTagger` produces exactly one output for each input word, the `MorphologicalTokenizer` might produce multiple output tokens.\n",
    "-  The `MorphologicalTokenizer` can be configured to produce diacritized and undiacritized output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_morphological_tokenization(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Perform morphological tokenization on an Arabic sentence.\n",
    "\n",
    "    This function first tokenizes the input sentence into words using a simple Arabic \n",
    "    word tokenizer. Then, it loads a pretrained morphological disambiguator (using the \n",
    "    'calima-msa-r13' model) and applies a morphological tokenizer to generate detailed \n",
    "    morphological tokens. The tokenizer is configured with:\n",
    "      - scheme='d3tok': specifying a particular morphological tokenization scheme.\n",
    "      - split=True: to output each morphological token as a separate string.\n",
    "      - diac=True: to output the tokens with diacritics.\n",
    "    \n",
    "    The result is a list of tokens that represent the morphological breakdown of the \n",
    "    input text. Note that the exact output depends on the model and its configuration.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): An Arabic sentence to be morphologically tokenized.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of morphological tokens (as strings). Each token represents a segment \n",
    "              of the input word based on its morphological structure, potentially including diacritics.\n",
    "    \n",
    "    Example:\n",
    "        >>> text = \"Ø§Ù„Ø·Ù„Ø§Ø¨Ù ÙŠØ¯Ø±Ø³ÙˆÙ†ÙŽ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©Ù\"\n",
    "        >>> tokens = arabic_morphological_tokenization(text)\n",
    "        >>> print(tokens)\n",
    "        ['Ø§Ù„', 'Ø·Ù„Ø§Ø¨Ù', 'ÙŠ', 'Ø¯Ø±', 'Ø³ÙˆÙ†ÙŽ', 'ÙÙŠ', 'Ø§Ù„', 'Ø¬Ø§Ù…Ø¹Ø©Ù']\n",
    "        # (Note: The actual segmentation may vary depending on the disambiguator and tokenizer configuration.)\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence into words using a simple tokenizer.\n",
    "    words = simple_word_tokenize(text)\n",
    "\n",
    "    # Load a pretrained morphological disambiguator (using the calima-msa-r13 model).\n",
    "    mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "\n",
    "    # Initialize the morphological tokenizer with specified configuration:\n",
    "    # - scheme: 'd3tok' to determine the tokenization scheme.\n",
    "    # - split: True to split the output into individual tokens.\n",
    "    # - diac: True to include diacritized forms in the output.\n",
    "    tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True, diac=True)\n",
    "    \n",
    "    # Perform morphological tokenization on the pre-tokenized words.\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: A basic sentence with common morphological structure.\n",
      "Input: Ø§Ù„Ø·Ù„Ø§Ø¨Ù ÙŠØ¯Ø±Ø³ÙˆÙ†ÙŽ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©Ù\n",
      "Output tokens: ['Ø§Ù„+', 'Ø·ÙÙ„Ù‘Ø§Ø¨Ù', 'ÙŠÙŽØ¯Ù’Ø±ÙØ³ÙÙˆÙ†ÙŽ', 'ÙÙÙŠ', 'Ø§Ù„+', 'Ø¬Ø§Ù…ÙØ¹ÙŽØ©Ù']\n",
      "--------------------------------------------------\n",
      "Test Case 2: A sentence with a verb, noun, and adverb.\n",
      "Input: ÙƒØªØ¨ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ø¯Ø±Ø³ Ø¨Ø³Ø±Ø¹Ø©\n",
      "Output tokens: ['ÙƒÙŽØªÙŽØ¨ÙŽ', 'Ø§Ù„+', 'Ø·Ø§Ù„ÙØ¨Ù', 'Ø§Ù„+', 'Ø¯ÙŽØ±Ù’Ø³Ù', 'Ø¨Ù+', 'Ø³ÙØ±Ù’Ø¹ÙŽØ©Ù']\n",
      "--------------------------------------------------\n",
      "Test Case 3: Empty string should return an empty list.\n",
      "Input: \n",
      "Output tokens: []\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    # (input text, description)\n",
    "    (\"Ø§Ù„Ø·Ù„Ø§Ø¨Ù ÙŠØ¯Ø±Ø³ÙˆÙ†ÙŽ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©Ù\", \"A basic sentence with common morphological structure.\"),\n",
    "    (\"ÙƒØªØ¨ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ø¯Ø±Ø³ Ø¨Ø³Ø±Ø¹Ø©\", \"A sentence with a verb, noun, and adverb.\"),\n",
    "    (\"\", \"Empty string should return an empty list.\")\n",
    "]\n",
    "\n",
    "for idx, (input_text, description) in enumerate(examples, 1):\n",
    "    print(f\"Test Case {idx}: {description}\")\n",
    "    print(\"Input:\", input_text)\n",
    "    tokens = arabic_morphological_tokenization(input_text)\n",
    "    print(\"Output tokens:\", tokens)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.24 Finding Synonyms (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_synonyms(text: str, num_augmentations: int = 1, tool: str = 'sina') -> list:\n",
    "    '''\n",
    "    Replaces each word in the text with one of its synonyms using the specified tool.\n",
    "    \n",
    "    Parameters:\n",
    "      text (str): The input text to process.\n",
    "      num_augmentations (int): Number of augmented outputs to generate.\n",
    "      tool (str): The augmentation tool to use ('sina' uses the evaluate_synonyms method).\n",
    "    \n",
    "    Returns:\n",
    "      list: A list of augmented text strings.\n",
    "    '''\n",
    "    # If using another tool, e.g. morphology analyzer, initialize it accordingly.\n",
    "    # db = MorphologyDB.builtin_db()\n",
    "    # analyzer = Analyzer(db)\n",
    "    \n",
    "    words = text.split()\n",
    "    augmented_texts = []\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            synonyms_result = evaluate_synonyms(word, 3)\n",
    "            if synonyms_result:\n",
    "                synonym_candidates = [syn[0] for syn in synonyms_result if syn and syn[0] != word]\n",
    "                if synonym_candidates:\n",
    "                    new_word = random.choice(synonym_candidates)\n",
    "                else:\n",
    "                    new_word = word\n",
    "            else:\n",
    "                new_word = word\n",
    "            new_words.append(new_word)\n",
    "        augmented_texts.append(' '.join(new_words))\n",
    "    \n",
    "    return augmented_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> find a way to do data synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.25 Data Augmenetation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> find a way to do data augmentation.\n",
    "\n",
    "Links:\n",
    "1. https://medium.com/@Mustafa77/data-augmentation-using-transformers-and-similarity-measures-2812c4853ed3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> Translate Arabic to English and perform natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Handling Very Common Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_common_words(df: pd.DataFrame, mode='remove'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handling Very Rare Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_rare_words(df: pd.DataFrame, mode='remove'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handling Numbers and Special Characters in Arabic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_numbers_and_special_chars(text, mode='remove'):\n",
    "    \"\"\"\n",
    "    Process Arabic text by either removing or normalizing numbers and special characters.\n",
    "\n",
    "    This function handles numbers and special characters in an Arabic text in one of two ways:\n",
    "      - 'remove': Eliminates all characters that are not Arabic letters (Unicode range \\u0600-\\u06FF) or whitespace.\n",
    "      - 'normalize': Converts Arabic digits (Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©) to their corresponding Western numeral characters (0-9)\n",
    "                     while leaving other characters unchanged.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input Arabic text, which may include numbers and special characters.\n",
    "        mode (str, optional): The mode of processing, either 'remove' or 'normalize'. \n",
    "                              Default is 'remove'.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text after applying the specified operation.\n",
    "\n",
    "    Examples:\n",
    "        >>> handle_numbers_and_special_chars(\"Ø§Ù„Ù„ØºØ© Ù¡Ù¢Ù£ Ø¬Ù…ÙŠÙ„Ø©!\", mode='remove')\n",
    "        'Ø§Ù„Ù„ØºØ© Ø¬Ù…ÙŠÙ„Ø©'\n",
    "        >>> handle_numbers_and_special_chars(\"Ø§Ù„Ù„ØºØ© Ù¡Ù¢Ù£ Ø¬Ù…ÙŠÙ„Ø©!\", mode='normalize')\n",
    "        'Ø§Ù„Ù„ØºØ© 123 Ø¬Ù…ÙŠÙ„Ø©!'\n",
    "    \"\"\"\n",
    "    if mode == 'remove':\n",
    "        # Remove any character that is not an Arabic letter (or whitespace)\n",
    "        return re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    elif mode == 'normalize':\n",
    "        # Normalize Hindi numbers to Arabic numerals\n",
    "        number_map = {\n",
    "            'Ù ': '0', 'Ù¡': '1', 'Ù¢': '2', 'Ù£': '3', 'Ù¤': '4',\n",
    "            'Ù¥': '5', 'Ù¦': '6', 'Ù§': '7', 'Ù¨': '8', 'Ù©': '9'\n",
    "        }\n",
    "        for arabic, western in number_map.items():\n",
    "            text = text.replace(arabic, western)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: Mode = remove\n",
      "Input:     Ø§Ù„Ù„ØºØ© Ù¡Ù¢Ù£ Ø¬Ù…ÙŠÙ„Ø©!\n",
      "Expected:  'Ø§Ù„Ù„ØºØ© Ø¬Ù…ÙŠÙ„Ø©'\n",
      "Result:    'Ø§Ù„Ù„ØºØ© Ù¡Ù¢Ù£ Ø¬Ù…ÙŠÙ„Ø©'\n",
      "Pass: False\n",
      "----------------------------------------\n",
      "Test Case 2: Mode = normalize\n",
      "Input:     Ø§Ù„Ù„ØºØ© Ù¡Ù¢Ù£ Ø¬Ù…ÙŠÙ„Ø©!\n",
      "Expected:  'Ø§Ù„Ù„ØºØ© 123 Ø¬Ù…ÙŠÙ„Ø©!'\n",
      "Result:    'Ø§Ù„Ù„ØºØ© 123 Ø¬Ù…ÙŠÙ„Ø©!'\n",
      "Pass: True\n",
      "----------------------------------------\n",
      "Test Case 3: Mode = remove\n",
      "Input:     Ù‡Ø°Ø§ Ù†Øµ Ù…Ø¹ Ø±Ù…ÙˆØ² Ù…Ø«Ù„ #ØŒ Ùˆ Ù¤Ù¥Ù¦!\n",
      "Expected:  'Ù‡Ø°Ø§ Ù†Øµ Ù…Ø¹ Ø±Ù…ÙˆØ² Ù…Ø«Ù„  Ùˆ '\n",
      "Result:    'Ù‡Ø°Ø§ Ù†Øµ Ù…Ø¹ Ø±Ù…ÙˆØ² Ù…Ø«Ù„ ØŒ Ùˆ Ù¤Ù¥Ù¦'\n",
      "Pass: False\n",
      "----------------------------------------\n",
      "Test Case 4: Mode = normalize\n",
      "Input:     Ù‡Ø°Ø§ Ù†Øµ Ù…Ø¹ Ø±Ù…ÙˆØ² Ù…Ø«Ù„ #ØŒ Ùˆ Ù¤Ù¥Ù¦!\n",
      "Expected:  'Ù‡Ø°Ø§ Ù†Øµ Ù…Ø¹ Ø±Ù…ÙˆØ² Ù…Ø«Ù„ #ØŒ Ùˆ 456!'\n",
      "Result:    'Ù‡Ø°Ø§ Ù†Øµ Ù…Ø¹ Ø±Ù…ÙˆØ² Ù…Ø«Ù„ #ØŒ Ùˆ 456!'\n",
      "Pass: True\n",
      "----------------------------------------\n",
      "Test Case 5: Mode = remove\n",
      "Input:     \n",
      "Expected:  ''\n",
      "Result:    ''\n",
      "Pass: True\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Ø§Ù„Ù„ØºØ© Ù¡Ù¢Ù£ Ø¬Ù…ÙŠÙ„Ø©!\",\n",
    "        \"mode\": \"remove\",\n",
    "        \"expected\": \"Ø§Ù„Ù„ØºØ© Ø¬Ù…ÙŠÙ„Ø©\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Ø§Ù„Ù„ØºØ© Ù¡Ù¢Ù£ Ø¬Ù…ÙŠÙ„Ø©!\",\n",
    "        \"mode\": \"normalize\",\n",
    "        \"expected\": \"Ø§Ù„Ù„ØºØ© 123 Ø¬Ù…ÙŠÙ„Ø©!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Ù‡Ø°Ø§ Ù†Øµ Ù…Ø¹ Ø±Ù…ÙˆØ² Ù…Ø«Ù„ #ØŒ Ùˆ Ù¤Ù¥Ù¦!\",\n",
    "        \"mode\": \"remove\",\n",
    "        \"expected\": \"Ù‡Ø°Ø§ Ù†Øµ Ù…Ø¹ Ø±Ù…ÙˆØ² Ù…Ø«Ù„  Ùˆ \"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Ù‡Ø°Ø§ Ù†Øµ Ù…Ø¹ Ø±Ù…ÙˆØ² Ù…Ø«Ù„ #ØŒ Ùˆ Ù¤Ù¥Ù¦!\",\n",
    "        \"mode\": \"normalize\",\n",
    "        \"expected\": \"Ù‡Ø°Ø§ Ù†Øµ Ù…Ø¹ Ø±Ù…ÙˆØ² Ù…Ø«Ù„ #ØŒ Ùˆ 456!\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\",\n",
    "        \"mode\": \"remove\",\n",
    "        \"expected\": \"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "for idx, case in enumerate(examples, 1):\n",
    "    result = handle_numbers_and_special_chars(case[\"input\"], mode=case[\"mode\"])\n",
    "    print(f\"Test Case {idx}: Mode = {case['mode']}\")\n",
    "    print(\"Input:    \", case[\"input\"])\n",
    "    print(\"Expected: \", repr(case[\"expected\"]))\n",
    "    print(\"Result:   \", repr(result))\n",
    "    print(\"Pass:\", result == case[\"expected\"])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> fix `handle_numbers_and_special_chars` method as it sometimes provides incorrect output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Preporcessing</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Text Classification:_**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_arabic_text(text: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Classify Arabic text using the specified pretrained model from Hugging Face.\n",
    "    \n",
    "    Parameters:\n",
    "      text (str): The Arabic text to classify.\n",
    "      model_name (str): The Hugging Face model name for sequence classification.\n",
    "      \n",
    "    Returns:\n",
    "      list: A list of probabilities corresponding to each class.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "    processed_text = arabert_prep.preprocess(text)\n",
    "\n",
    "    inputs = tokenizer(processed_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return predictions.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2025-02-15 06:19:28,990 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø±Ø§Ø¦Ø¹ ÙˆÙ…ÙÙŠØ¯ Ø¬Ø¯Ø§Ù‹\n",
      "Classification probabilities: [0.8355841636657715, 0.1644158959388733]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø±Ø§Ø¦Ø¹ ÙˆÙ…ÙÙŠØ¯ Ø¬Ø¯Ø§Ù‹\"\n",
    "classification = classify_arabic_text(text, model_name=\"aubmindlab/bert-base-arabertv2\")\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Classification probabilities: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> finetune model to perform text classification across multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Sentiment Analysis:_**</span> is identifying whether a text is classified as positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_arabic_sentiment(text: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of an Arabic text using a pretrained sentiment analysis model.\n",
    "\n",
    "    This function uses the Hugging Face Transformers sentiment analysis pipeline with the\n",
    "    model \"CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\" to classify the sentiment\n",
    "    of the input Arabic text. It returns a tuple containing the sentiment label (e.g., \"POSITIVE\"\n",
    "    or \"NEGATIVE\") and the associated confidence score.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The Arabic text to be analyzed for sentiment.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (label, score) where:\n",
    "            - label (str): The predicted sentiment label.\n",
    "            - score (float): The confidence score (between 0 and 1) for the predicted sentiment.\n",
    "\n",
    "    Example:\n",
    "        >>> label, score = analyze_arabic_sentiment(\"Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø±Ø§Ø¦Ø¹ ÙˆÙ…ÙÙŠØ¯ Ø¬Ø¯Ø§Ù‹\")\n",
    "        >>> print(f\"Sentiment: {label} (confidence: {score:.2f})\")\n",
    "\n",
    "    \"\"\"\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\")\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    return result['label'], result['score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Ø£Ù†Ø§ Ø³Ø¹ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ù…Ù†ØªØ¬!\n",
      "Sentiment: positive, Score: 0.9928115010261536\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Ø£Ù†Ø§ Ø³Ø¹ÙŠØ¯ Ø¬Ø¯Ø§Ù‹ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ù…Ù†ØªØ¬!\"\n",
    "sentiment, score = analyze_arabic_sentiment(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Sentiment: {sentiment}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Ø£Ù†Øª Ù…Ø§Ù„Ùƒ ÙŠØ§Ø¶ !\n",
      "Sentiment: negative, Score: 0.9415218234062195\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Ø£Ù†Øª Ù…Ø§Ù„Ùƒ ÙŠØ§Ø¶ !\"\n",
    "sentiment, score = analyze_arabic_sentiment(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Sentiment: {sentiment}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Word Embedding:_**</span> is a way of representing words as dense, continuous vectors in a high-dimensional space. These vectors capture semantic relationships between words so that words with similar meanings are mapped to nearby points in the vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_word_embedding(text: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute the word embedding for an Arabic word and retrieve its most similar terms.\n",
    "\n",
    "    This function loads a pretrained Word2Vec model, cleans and normalizes the input Arabic text,\n",
    "    and then retrieves the word embedding vector for the cleaned word. Additionally, it finds and \n",
    "    prints the most similar words based on cosine similarity within the embedding space.\n",
    "\n",
    "    The cleaning process includes:\n",
    "      - Replacing various Arabic characters with normalized forms.\n",
    "      - Removing diacritics (tashkeel) and repeated character elongations.\n",
    "      - Trimming whitespace and handling specific punctuation or symbols.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The Arabic word or phrase to process.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - word_vector: The vector representation of the cleaned word (typically a NumPy array).\n",
    "            - most_similar: A list of tuples, where each tuple consists of a similar word (str) \n",
    "              and its similarity score (float).\n",
    "\n",
    "    Example:\n",
    "        >>> vector, similar = arabic_word_embedding(\"Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©\")\n",
    "        Most similar words (and their similarity scores) are printed, and 'vector' holds the embedding for the cleaned word.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the pretrained Word2Vec model.\n",
    "    model = gensim.models.Word2Vec.load('./models/tweet_cbow_300/tweets_cbow_300')\n",
    "\n",
    "    # Clean/Normalize Arabic Text\n",
    "    def clean_str(text):\n",
    "        search = [\"Ø£\", \"Ø¥\", \"Ø¢\", \"Ø©\", \"_\", \"-\", \"/\", \".\", \"ØŒ\", \" Ùˆ \", \" ÙŠØ§ \", '\"', \"Ù€\", \"'\", \"Ù‰\", \"\\\\\", '\\n', '\\t', '&quot;', '?', 'ØŸ', '!']\n",
    "        replace = [\"Ø§\", \"Ø§\", \"Ø§\", \"Ù‡\", \" \", \" \", \"\", \"\", \"\", \" Ùˆ\", \" ÙŠØ§\", \"\", \"\", \"\", \"ÙŠ\", \"\", \" \", \" \", \" ? \", \" ØŸ \", \" ! \"]\n",
    "        \n",
    "        # Remove tashkeel (diacritics)\n",
    "        p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "        text = re.sub(p_tashkeel, \"\", text)\n",
    "        \n",
    "        # Remove longation (repeated characters)\n",
    "        p_longation = re.compile(r'(.)\\1+')\n",
    "        subst = r\"\\1\\1\"\n",
    "        text = re.sub(p_longation, subst, text)\n",
    "        \n",
    "        text = text.replace('ÙˆÙˆ', 'Ùˆ')\n",
    "        text = text.replace('ÙŠÙŠ', 'ÙŠ')\n",
    "        text = text.replace('Ø§Ø§', 'Ø§')\n",
    "        \n",
    "        for i in range(len(search)):\n",
    "            text = text.replace(search[i], replace[i])\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    # Clean the input text\n",
    "    word = clean_str(text)\n",
    "\n",
    "    # Retrieve and print the most similar terms to the cleaned word\n",
    "    most_similar = model.wv.most_similar(word)\n",
    "    print(\"Most similar terms to '{}':\".format(word))\n",
    "    for term, score in most_similar:\n",
    "        print(term, score)\n",
    "        \n",
    "    # Retrieve the word vector\n",
    "    word_vector = model.wv[word]\n",
    "\n",
    "    return word_vector, most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext is not working on Windows\n",
    "# import fasttext\n",
    "\n",
    "def word2vec_fasttext(text: str):\n",
    "    # create word embedding model\n",
    "    model = fasttext.train_unsupervised('xxx.txt', epoch=25)\n",
    "\n",
    "    # get word embeddings for words in text\n",
    "    word_embeddings = model.get_word_vector(text)\n",
    "\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø§Ù„Ø§Ø³ÙƒÙ†Ø¯Ø±ÙŠÙ‡ 0.822142481803894\n",
      "Ø§Ø³ÙˆØ§Ù† 0.7448597550392151\n",
      "Ø§Ù„Ø¬ÙŠØ²Ù‡ 0.7406017184257507\n",
      "Ø§Ù„Ù…Ù†ØµÙˆØ±Ù‡ 0.7375915050506592\n",
      "Ø§Ù„Ø§Ø³Ù…Ø§Ø¹ÙŠÙ„ÙŠÙ‡ 0.7310688495635986\n",
      "Ø¨ÙˆØ±Ø³Ø¹ÙŠØ¯ 0.7242382168769836\n",
      "Ø§Ù„Ø§Ù‚ØµØ± 0.7210926413536072\n",
      "Ø­Ù„ÙˆØ§Ù† 0.7209565043449402\n",
      "Ø¯Ù…ÙŠØ§Ø· 0.7096551060676575\n",
      "Ø·Ù†Ø·Ø§ 0.7094075679779053\n",
      "Word vector for 'Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©': [-0.36143455  0.32451856  0.14601593  0.32183495 -2.4407325   2.3771033\n",
      " -0.02987524 -0.36511382 -1.9445266   2.0458498  -0.4462689   0.8169745\n",
      "  0.57143867 -0.14586152 -2.7012775   1.5832865   1.6561981   2.3886893\n",
      " -0.7477331  -2.2364702  -0.3022785  -0.44031492  1.0667934  -1.2664819\n",
      "  0.5260765   0.87624025  0.58786726 -0.59008116 -2.0730557   1.4947067\n",
      "  0.61162     4.520309    0.03703779 -0.02008293 -1.1760161  -0.907512\n",
      " -0.6775007  -1.4298267   0.43027702 -0.3751945   2.1304162   1.6183015\n",
      " -0.7221879   0.3284036  -1.151335    1.0218043  -0.19037294 -0.25370607\n",
      "  0.55373436  0.2848552  -2.660286    1.0729909   0.5107816  -2.057838\n",
      " -1.3162624  -0.8008683  -2.131203    1.3305361  -0.08949289 -0.20919245\n",
      "  0.6519448  -0.42523125 -2.1237717   1.7458177   0.9424573  -1.6369351\n",
      " -0.4509722  -0.7313934   0.373175    2.1887953   1.2610693   1.3537819\n",
      " -1.8460715  -0.42807204 -0.5282065  -1.7234492   1.5000864   0.04967218\n",
      " -2.7448952   1.9553512  -0.3313362  -1.8806909   1.7006743  -2.7383993\n",
      " -0.6716011  -2.0714629   0.5913716  -0.6824405   0.30297148 -0.90492505\n",
      " -0.7712346   0.5803537   0.1335388   2.7877033  -1.5064228  -1.8370237\n",
      "  0.2697579   2.3486383   2.103636    1.521233   -1.4001724   0.82219344\n",
      "  0.8240768  -0.15086019  0.2961153  -1.0394131   0.5568691   0.28836966\n",
      "  0.6295702   1.7095642  -0.85605335 -0.6515357   1.4827238   1.002474\n",
      "  0.19940409  1.070932   -0.5550646   2.214102    0.37053385 -0.79781234\n",
      " -1.6374146  -0.60718566 -1.9219145  -2.0604632  -1.6528308   0.33980826\n",
      " -3.5135572   0.41424054  1.9958695  -0.35256767  2.6131797  -3.3231125\n",
      " -0.12307548 -0.9431562   1.1456573  -0.430937    1.071695   -0.6364221\n",
      " -1.233112    0.33730644  1.1635537   1.5573068  -1.2561119   0.15221852\n",
      "  0.4580792  -1.0473022   0.32258037 -2.594469   -0.5203601  -3.1735766\n",
      " -0.19842161 -1.7157494  -1.4219034  -0.5446474   0.4559874   1.8500901\n",
      " -1.9773417   0.5568433  -1.7871313  -0.71735185  1.132678   -0.837672\n",
      " -0.9265186  -0.277091   -0.4498865  -0.65538085 -0.29954773  0.162286\n",
      " -0.5294098   0.83181196  0.73085546 -2.3702266   1.0934252   2.0041482\n",
      "  2.1957505  -0.07741582 -2.0670712   1.0325543  -2.668172    1.6843041\n",
      "  0.3038795  -0.65110064 -0.80047244  2.6929338   2.8901029  -0.34811088\n",
      "  0.99073356  1.696957    1.4957255  -0.33842573  1.4915138   0.58355224\n",
      " -2.7133086   2.4071438   1.3807678  -0.01793567 -1.3098323   1.5640415\n",
      " -0.15761942  3.5385716  -0.941214   -0.7848572  -1.0996732  -0.24231693\n",
      " -2.0031528   0.07146657  0.14217159 -0.26462936 -0.27889195 -0.3159959\n",
      "  0.48282027  1.7386041   1.9550546  -1.6407006  -0.18940933  0.56804603\n",
      "  0.23227286 -3.058908    0.15069857  0.83813035 -1.6782109   0.16098635\n",
      " -2.5502133  -0.23519206  0.77975184  3.4241571  -0.4207713  -0.11863837\n",
      " -1.1716788   0.75626653 -1.7658116  -1.3981905  -0.1646244   3.7715173\n",
      " -0.2594515   0.8704249   2.596015   -0.14752974 -2.328448   -1.4490464\n",
      " -1.2590245  -0.17023873  0.3576156   1.7758955   1.4414039  -1.2650518\n",
      "  0.21300997  0.65599275 -1.1683582  -0.38035727  1.3608054  -0.6101739\n",
      "  0.47013474  6.905608   -0.95340157 -0.6166638  -2.072214    0.19559622\n",
      " -1.0854616   3.545438   -2.2825742  -1.2395469  -0.01394261 -0.6912439\n",
      " -0.68033797  0.38063502 -0.34611645  0.15759547 -0.6383159   1.1406134\n",
      "  2.8847592  -0.50538653  0.6145402   0.11379281 -1.8095436   2.7481453\n",
      "  0.91495186 -0.39614347 -1.0227388   1.1418179  -1.3758924  -0.26808223\n",
      "  0.44675693 -1.7718596  -0.22452602 -0.39037687 -0.4203093  -0.57415867\n",
      "  1.2875202   1.020447    0.12890834  0.4915232   0.9719728  -0.70402974\n",
      " -3.5051332   0.2558523  -1.3064789  -1.0550935   1.9999434  -1.8610594 ]\n"
     ]
    }
   ],
   "source": [
    "vector, similar = arabic_word_embedding(\"Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©\")\n",
    "print(\"Word vector for 'Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©':\", vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Label Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: green\">**_Topic Modeling:_**</span> is an unsupervised machine learning technique for finding abstract topics in a large collection of documents. It helps in organizing, understanding and summarizing large collections of textual information and discovering the latent topics that vary among documents in a given corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet allocation (LDA) and Non-Negative Matrix Fatorization (NMF) are two of the most popular topic modeling techniques. LDA uses a probabilistic approach whereas NMF uses matrix factorization approach, however, new techniques that are based on BERT for topic modeling do exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://colab.research.google.com/drive/1OT_wcYKpKS73uR6y7IVYjJVxaP-C1H3k?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Translate to English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color: red\">**_TODO:_**</span> Translate Arabic to English and perform natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detecting Sarcasm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://medium.com/@rehabreda/unraveling-sarcasm-in-arabic-with-arabert-a-comprehensive-guide-from-data-preprocessing-to-a4dc7e30b39d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Pipeline Execution</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Visualize Data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#DA8359;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Resources</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. BERT for Arabic Topic Modeling: An Experimental Study on BERTopic Technique (https://github.com/iwan-rg/Arabic-Topic-Modeling?tab=readme-ov-file)\n",
    "2. NYU ABU DHABI (https://nyuad.nyu.edu/en/research/faculty-labs-and-projects/computational-approaches-to-modeling-language-lab/research/arabic-natural-language-processing.html)\n",
    "3. CAMeL Tools (https://camel-tools.readthedocs.io/en/latest/api.html)\n",
    "4. Comprehensive Arabic NLP Data Processing and Cleaning Guide (https://github.com/h9-tect/Arabic_nlp_preprocessing)\n",
    "5. PyArabic (https://pyarabic.readthedocs.io/ar/latest/)\n",
    "6. AUB MIND LAB (https://huggingface.co/aubmindlab)\n",
    "7. AraBERT (https://github.com/aub-mind/arabert/tree/master)\n",
    "8. Awesome Resources for Arabic NLP Repo (https://github.com/Curated-Awesome-Lists/awesome-arabic-nlp?tab=readme-ov-file)\n",
    "9. Arabic Dialect Identification Models (https://github.com/Lafifi-24/arabic-dialect-identification?tab=readme-ov-file)\n",
    "10. AraVec (https://github.com/bakrianoo/aravec/blob/master/AraVec%202.0/README.md)\n",
    "11. Text Classifier (https://github.com/mustaphakamil/Arabic-text-classification/blob/master/Text%20Classifier%20NLP.ipynb)\n",
    "12. BERT for Arabic Topic Modeling (https://colab.research.google.com/drive/1OT_wcYKpKS73uR6y7IVYjJVxaP-C1H3k?usp=sharing#scrollTo=SNa-KtKDRnus)\n",
    "13. Dialect Identification (https://medium.com/@kmelad43/arabic-dialect-identification-774de9315140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
