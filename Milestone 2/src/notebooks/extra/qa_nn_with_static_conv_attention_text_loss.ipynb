{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11356685,"sourceType":"datasetVersion","datasetId":7107330},{"sourceId":11359226,"sourceType":"datasetVersion","datasetId":7109338},{"sourceId":331064,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":277744,"modelId":298640}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T10:22:26.405111Z","iopub.execute_input":"2025-04-11T10:22:26.405451Z","iopub.status.idle":"2025-04-11T10:23:56.827859Z","shell.execute_reply.started":"2025-04-11T10:22:26.405426Z","shell.execute_reply":"2025-04-11T10:23:56.827001Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import re\nimport torch\nimport argparse\nimport numpy as np\nimport torch.nn.functional as F\nimport itertools\nimport collections\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport nltk\nfrom nltk.stem import WordNetLemmatizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:00:03.460351Z","iopub.execute_input":"2025-04-12T07:00:03.460869Z","iopub.status.idle":"2025-04-12T07:00:09.769874Z","shell.execute_reply.started":"2025-04-12T07:00:03.460844Z","shell.execute_reply":"2025-04-12T07:00:09.769110Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"nltk.download('wordnet')\nnltk.download('omw-1.4')\nlemmatizer = WordNetLemmatizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:00:13.417245Z","iopub.execute_input":"2025-04-12T07:00:13.417684Z","iopub.status.idle":"2025-04-12T07:00:13.684371Z","shell.execute_reply.started":"2025-04-12T07:00:13.417661Z","shell.execute_reply":"2025-04-12T07:00:13.683563Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"PAD_TOKEN = '[PAD]'\nUNK_TOKEN = '[UNK]'\n\nauto_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:00:15.649445Z","iopub.execute_input":"2025-04-12T07:00:15.649978Z","iopub.status.idle":"2025-04-12T07:00:19.138395Z","shell.execute_reply.started":"2025-04-12T07:00:15.649953Z","shell.execute_reply":"2025-04-12T07:00:19.137822Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d60c7f7e0704dc7b986bb4320ce4f92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c106a22f735e4e45a3139f3556fe6b42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf0dff7cdc264ffbbb2e3ea617b8f37a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98203b4aadd04d6aadaf2ea4269cc026"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Utils.py","metadata":{}},{"cell_type":"code","source":"def cuda(args, tensor):\n    \"\"\"\n    Places tensor on CUDA device (by default, uses cuda:0).\n    \n    Returns:\n        Tensor on CUDA device.\n    \"\"\"\n    if args.use_gpu and torch:\n        return tensor.cuda()\n    else:\n        return tensor\n\ndef unpack(tensor):\n    \"\"\"\n    Unpacks a tensor into a Python list.\n\n    Args:\n        tensor: PyTorch tensor.\n\n    Returns:\n        Python list with tensor contents.\n    \"\"\"\n    if tensor.requires_grad:\n        tensor = tensor.detach()\n    return tensor.cpu().numpy().tolist()\n\ndef load_embeddings(path):\n    \"\"\"\n    Loads GloVe-style embeddings into memory.\n    Args:\n        path: Embedding path, e.g. \"glove/glove.6B.300d.txt\".\n\n    Returns:\n        Dictionary mapping words (strings) to vectors (list of floats).\n    \"\"\"\n    embedding_map = {}\n    with open(path, 'r', encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                pieces = line.rstrip().split()\n                word = pieces[0].lower()  # Normalize to lowercase\n                embedding_map[word] = [float(weight) for weight in pieces[1:]]\n                \n                # Also store lemma if different\n                lemma = lemmatizer.lemmatize(word)\n                if lemma != word and lemma not in embedding_map:\n                    embedding_map[lemma] = [float(weight) for weight in pieces[1:]]\n            except:\n                pass\n    return embedding_map\n\ndef embed_batch(embedding_map, embedding_layer, batch_token_ids, idx2word, embed_dim):\n    \"\"\"\n    Iteratively converts a batch of token id sequences into their embeddings.\n\n    Args:\n        embedding_map (dict): Mapping from to embedding vectors.\n        batch_token_ids (List[List[int]]): Batch where each element is a list of token ids.\n        idx2word (dict): Mapping from token ID (int) to the corresponding word (str)\n        embed_dim (int): The dimensionality of the embeddings.\n    \n    Returns:\n        Numpy array of shape (batch_size, seq_len, embed_dim) containing the embeddings.\n    \"\"\"\n    batch_embeddings = []\n    \n    for token_ids in batch_token_ids:\n        sequence_embeddings = []\n        for token_id in token_ids:\n            # Retrieve the corresponding word for the token id.\n            token = idx2word.get(token_id.item(), None)\n            # print(\"Token\", token_id.item(), token)\n            if token is None and token not in embedding_map:\n                token_embedding = np.zeros(embed_dim)\n            else:\n                try:\n                    token_tensor = torch.tensor([token_id.item()], device=device)\n                    token_embedding = embedding_layer(token_tensor).squeeze(0).cpu().detach().numpy()\n                except Exception as e:\n                    print(f\"Token ID {token_id} caused error: {e}\")\n                    token_embedding = np.zeros(embed_dim)\n\n            sequence_embeddings.append(token_embedding)\n        batch_embeddings.append(sequence_embeddings)\n    return np.array(batch_embeddings)\n\ndef co_attention(context_embedding, question_embedding, conv=True):\n    \"\"\"\n    Co-attention mechanism that computes attention between context and question encodings.\n    If `convolution=True`, applies local smoothing to the affinity matrix.\n\n    Args:\n        context_embedding (Tensor): (B, context_len, d)\n        question_embedding (Tensor): (B, question_len, d)\n        convolution (bool): whether to apply convolution-based smoothing.\n\n    Returns:\n        CP (Tensor): passage attention context\n        E_Out (Tensor): final encoder output\n    \"\"\"\n    # Step 1: Affinity matrix A ∈ (B, context_len, question_len)\n    A = torch.bmm(context_embedding, question_embedding.transpose(1, 2))\n    # print(\"context_embedding = \", context_embedding[0])\n    # print(\"question_embedding = \", question_embedding[0])\n    \n    # print(\"context_embedding:\", context_embedding)\n    # print(\"question_embedding:\", question_embedding)\n    # print(\"Affinity range:\", A.min().item(), A.max().item())\n\n    # Apply learned smoothing\n    if conv:\n        # A = conv_co_attention(A)\n        A = conv_co_attention(A)\n\n    # Step 2: Passage-to-question attention (row-wise)\n    A_P = F.softmax(A, dim=2)\n\n    # Step 3: Question-to-passage attention (column-wise)\n    A_Q = F.softmax(A.transpose(1, 2), dim=2)\n\n    # Step 4: Passage attention context: CP = H^P × A^Q\n    # print(\"Context Embedding Shape\", context_embedding.shape)\n    # print(\"Question Embedding Shape\", question_embedding.shape)\n    # print(\"A_Q Shape\", A_Q.shape)\n    CP = torch.bmm(A_Q, context_embedding)\n    # print(\"CP Shape\", CP.shape)  # (B, Lq, d)\n\n    # Step 5: Encoder output: concat(H^P, [H^Q; CP] × A^P)\n    # QC = torch.cat([question_embedding, CP], dim=1)\n    QC_1 = torch.bmm(A_P, question_embedding)  # (B, Lq, d)\n    # print(\"QC_1 Shape\", QC_1.shape)\n\n    QC_2 = torch.bmm(A_P, CP)  # (B, Lq, d)\n    # print(\"QC_2 Shape\", QC_2.shape)\n\n    # QC = torch.cat([QC_1, QC_2], dim=1) # (B, Lq, 2d)\n    QC = torch.cat([QC_1, QC_2], dim=-1)  # (B, Lq, 2d)\n    # print(\"QC Shape\", QC.shape)\n\n    # Final encoder output\n    # E_Out = torch.cat([context_embedding, QC], dim=2)\n    E_Out = torch.cat([context_embedding, QC], dim=-1)  # (B, Lq, 3d)\n    E_Out = nn.LayerNorm(E_Out.shape[-1]).to(E_Out.device)(E_Out)\n    E_Out = torch.tanh(E_Out)  # Apply non-linearity\n    # print(\"E_Out Shape\", E_Out.shape)\n\n    return CP, E_Out\n\ndef create_gaussian_kernel(kernel_width, device, sigma=1.0):\n    \"\"\"Creates a 1D Gaussian kernel.\"\"\"\n    x = torch.arange(-kernel_width//2 + 1, kernel_width//2 + 1, dtype=torch.float, device=device)\n    kernel = torch.exp(-x**2 / (2*sigma**2))\n    kernel /= kernel.sum()  # Normalize to sum to 1\n    return kernel.view(1, 1, -1)\n\ndef conv_co_attention(A, kernel_width=11):\n    \"\"\"\n    Enhanced convolution to shift attention to neighboring words.\n    Applies 1D convolution along context dimension per question word.\n    \"\"\"\n    B, Lp, Lq = A.shape\n    # Permute A for per-question-word processing: (B, Lq, Lp) -> (B*Lq, 1, Lp)\n    A_reshaped = A.permute(0, 2, 1).reshape(-1, 1, Lp)\n    \n    # Create Gaussian kernel with odd kernel width (e.g., 11)\n    kernel = create_gaussian_kernel(kernel_width, A.device, sigma=1.0)\n    \n    # Use symmetric padding that keeps the sequence length unchanged.\n    padded_length = (kernel_width - 1) // 2\n    smoothed_A = F.conv1d(A_reshaped, kernel, padding=padded_length)\n    \n    # Reshape back: current shape is (B*Lq, 1, Lp) --> (B, Lq, Lp) then permute to (B, Lp, Lq)\n    smoothed_A = smoothed_A.view(B, Lq, Lp).permute(0, 2, 1)\n    A_adjusted = A + smoothed_A  # Enhance original scores with neighbor context\n    return F.softmax(A_adjusted, dim=-1)\n\ndef tokenize_with_bert(text):\n    # Tokenize the text and request offset mappings.\n    encoding = auto_tokenizer(\n        text,\n        return_offsets_mapping=True,\n        add_special_tokens=False  # Disable adding special tokens to mimic simple whitespace tokenization.\n    )\n    \n    # Retrieve the tokens.\n    tokens = auto_tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n    \n    # Retrieve the spans from the offset mapping.\n    spans = encoding['offset_mapping']\n    return tokens, spans\n    \ndef create_embedding_matrix(vocab, embedding_map, embedding_dim=300, scale=0.6):\n    \"\"\"Initialize embedding matrix with:\n    - GloVe vectors for known words\n    - Random vectors for UNK tokens\n    - Zero vector for padding\n    \"\"\"\n    # Initialize with random normal distribution (match GloVe scale)\n    embedding_matrix = np.random.normal(\n        scale=scale, \n        size=(len(vocab), embedding_dim)\n    )\n    \n    # Handle special tokens\n    embedding_matrix[vocab.encoding[PAD_TOKEN]] = np.zeros(embedding_dim)\n    unk_idx = vocab.encoding[UNK_TOKEN]\n    embedding_matrix[unk_idx] = np.random.normal(scale=scale, size=embedding_dim)\n    \n    for word, idx in vocab.encoding.items():\n        if word in [PAD_TOKEN, UNK_TOKEN]:\n            continue\n            \n        # Try direct match\n        if word in embedding_map:\n            embedding_matrix[idx] = embedding_map[word]\n            continue\n            \n        # Try lemma\n        lemma = lemmatizer.lemmatize(word)\n        if lemma in embedding_map:\n            embedding_matrix[idx] = embedding_map[lemma]\n            continue\n            \n        # Try lowercase lemma\n        lower_lemma = lemmatizer.lemmatize(word.lower())\n        if lower_lemma in embedding_map:\n            embedding_matrix[idx] = embedding_map[lower_lemma]\n\n    return torch.tensor(embedding_matrix, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:09:58.424953Z","iopub.execute_input":"2025-04-12T07:09:58.425549Z","iopub.status.idle":"2025-04-12T07:09:58.443848Z","shell.execute_reply.started":"2025-04-12T07:09:58.425526Z","shell.execute_reply":"2025-04-12T07:09:58.443013Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Data.py","metadata":{}},{"cell_type":"code","source":"class Vocabulary:\n    \"\"\"\n    Creates mappings for words → indices and indices → words.\n    \"\"\"\n    def __init__(self, samples, vocab_size):\n        self.samples = samples\n        self.vocab_size = vocab_size\n        self.words = self._initialize(samples, vocab_size)\n        self.encoding = {word: idx for idx, word in enumerate(self.words)}\n        self.decoding = {idx: word for idx, word in enumerate(self.words)}\n\n    def _initialize(self, samples, vocab_size):\n        \"\"\"Build vocabulary with lemma support\"\"\"\n        embedding_map = load_embeddings(\"/kaggle/input/glove/other/default/1/glove.6B.300d.txt\")\n        vocab_counts = collections.defaultdict(int)\n        \n        for _, row in samples.iterrows():\n            # Get base tokens\n            tokens = re.findall(r\"\\w+(?:[-']\\w+)*\", row['context'].lower()) + \\\n                     re.findall(r\"\\w+(?:[-']\\w+)*\", row['question'].lower())\n            \n            # Count both original and lemma forms\n            for token in tokens:\n                vocab_counts[token] += 1\n                lemma = lemmatizer.lemmatize(token)\n                if lemma != token:\n                    vocab_counts[lemma] += 0.5  # Partial count for lemmas\n        \n        # Sort by combined frequency\n        sorted_words = sorted(vocab_counts.items(), \n                            key=lambda x: (-x[1], x[0]))[:vocab_size-2]\n        \n        return [PAD_TOKEN, UNK_TOKEN] + [w[0] for w in sorted_words]\n        \n    def __len__(self):\n        return len(self.words)\n\nclass Tokenizer:\n    \"\"\"\n    Converts lists of words to indices and vice versa.\n    \"\"\"\n    def __init__(self, vocabulary):\n        self.vocabulary = vocabulary\n        self.pad_token_id = vocabulary.encoding[PAD_TOKEN]\n        self.unk_token_id = vocabulary.encoding[UNK_TOKEN]\n\n    def convert_tokens_to_ids(self, tokens):\n        return [self.vocabulary.encoding.get(token.lower(), self.unk_token_id) for token in tokens]\n\n    def convert_ids_to_tokens(self, token_ids):\n        return [self.vocabulary.decoding.get(token_id, UNK_TOKEN) for token_id in token_ids]\n\nclass QADataset(Dataset):\n    \"\"\"\n    Data generator for a QA task; the JSON file should contain character-level answer indices.\n    \"\"\"\n    def __init__(self, path):\n        # Load JSON-lines file; each line is a JSON object.\n        self.samples = pd.read_json(path, lines=True)\n        self.tokenizer = None\n        # Default pad token id; updated after tokenizer registration.\n        self.pad_token_id = 0\n\n    def _collate_batch(self, batch):\n        batch = [sample for sample in batch if sample is not None]\n        if len(batch) == 0:\n            return None  # All samples failed\n    \n        max_context_len = max(sample['context'].size(0) for sample in batch)\n        max_question_len = max(sample['question'].size(0) for sample in batch)\n    \n        contexts = torch.stack([\n            torch.cat([\n                sample['context'],\n                torch.full((max_context_len - sample['context'].size(0),), self.pad_token_id, dtype=torch.long)\n            ]) for sample in batch\n        ])\n    \n        questions = torch.stack([\n            torch.cat([\n                sample['question'],\n                torch.full((max_question_len - sample['question'].size(0),), self.pad_token_id, dtype=torch.long)\n            ]) for sample in batch\n        ])\n    \n        answer_starts = torch.stack([sample['answer_start'] for sample in batch])\n        answer_ends = torch.stack([sample['answer_end'] for sample in batch])\n    \n        return {\n            'context': contexts,\n            'question': questions,\n            'answer_start': answer_starts,\n            'answer_end': answer_ends\n        }\n\n    def register_tokenizer(self, tokenizer):\n        \"\"\"\n        Registers a Tokenizer instance and updates pad token id.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.pad_token_id = tokenizer.pad_token_id\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples.iloc[idx]\n        context_str = sample['context']\n        question_str = sample['question']\n        answers = sample['answers']\n    \n        context_tokens, context_spans = tokenize_with_bert(context_str)\n        question_tokens, _ = tokenize_with_bert(question_str)\n        context_indices = self.tokenizer.convert_tokens_to_ids(context_tokens)\n        question_indices = self.tokenizer.convert_tokens_to_ids(question_tokens)\n    \n        answer_text = answers['text'][0].strip()\n        answer_tokens, _ = tokenize_with_bert(answer_text)\n    \n        context_tokens_lower = [t.lower() for t in context_tokens]\n        answer_tokens_lower = [t.lower() for t in answer_tokens]\n    \n        token_start, token_end = -1, -1\n        for i in range(len(context_tokens_lower) - len(answer_tokens_lower) + 1):\n            if context_tokens_lower[i:i+len(answer_tokens_lower)] == answer_tokens_lower:\n                token_start = i\n                token_end = i + len(answer_tokens_lower) - 1\n                break\n    \n        if token_start == -1 or token_end == -1:\n            # skip\n            return None\n    \n        return {\n            'context': torch.tensor(context_indices, dtype=torch.long),\n            'question': torch.tensor(question_indices, dtype=torch.long),\n            'answer_start': torch.tensor(token_start, dtype=torch.long),\n            'answer_end': torch.tensor(token_end, dtype=torch.long),\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:05:45.096290Z","iopub.execute_input":"2025-04-12T07:05:45.096565Z","iopub.status.idle":"2025-04-12T07:05:45.111644Z","shell.execute_reply.started":"2025-04-12T07:05:45.096544Z","shell.execute_reply":"2025-04-12T07:05:45.111016Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Model.py","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass BiLSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.3):\n        super(BiLSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        # Create a bidirectional LSTM layer; note batch_first=True keeps tensors as (batch, seq, feature)\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(dropout_prob)\n\n    def forward(self, x, return_sequence=True):\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, seq_len, input_size).\n            return_sequence: If True, return the entire sequence; if False, return the last hidden state.\n\n        \"\"\"\n        # Initialize hidden state and cell state\n        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n\n        # Run the input sequence through the LSTM layer\n        output, _ = self.lstm(x, (h0, c0))\n        \n        # Apply dropout to the output of the LSTM layer\n        output = self.dropout(output)\n\n        # Return the entire sequence\n        return output # Shape: (batch_size, seq_len, hidden_size * 2)\n\nclass QAModel(nn.Module):\n    def __init__(self, vocab_size, vocab_decoder, embedding_dim, hidden_size, num_layers, output_size=300, dropout_prob=0.3):\n        super(QAModel, self).__init__()\n\n        self.vocab_size = vocab_size\n        self.vocab_decoder = vocab_decoder\n\n        # Embedding map using pre-trained GloVe embeddings\n        self.embedding_map = load_embeddings(\"/kaggle/input/glove/other/default/1/glove.6B.300d.txt\")\n\n        # Initialize Embedding layer\n        self.embedding_layer = nn.Embedding(\n            num_embeddings = len(vocab),\n            embedding_dim = embedding_dim,\n            padding_idx = vocab.encoding[PAD_TOKEN]\n        )\n\n        # Load pretrained weights\n        embedding_matrix = create_embedding_matrix(vocab, self.embedding_map)\n        self.embedding_layer.weight.data.copy_(embedding_matrix)\n\n        # Freeze only GloVe vectors\n        for i, word in enumerate(vocab.words):\n            if word in self.embedding_map and word != UNK_TOKEN:\n                self.embedding_layer.weight.requires_grad_(False)\n\n        # Context Modeling\n        self.start_decoder = BiLSTMModel(embedding_dim * 3, hidden_size, num_layers, output_size=output_size)\n        self.end_decoder = BiLSTMModel(embedding_dim * 6, hidden_size, num_layers, output_size)\n\n        # Prediction Layers - two linear layers for start and end index predictions.\n        self.start_linear = nn.Linear(3 * embedding_dim + 2 * hidden_size, 1)\n        self.end_linear = nn.Linear(3 * embedding_dim + 2 * hidden_size, 1)\n    \n    def forward(self, context_ids, question_ids):\n        \"\"\"\n        Forward pass that includes contextual encoding.\n\n        Args:\n            context_ids (Tensor): shape (batch, context_len)\n            question_ids (Tensor): shape (batch, question_len)\n\n        Returns:\n            start_logits: Tensor of shape (batch, context_len)\n            end_logits: Tensor of question word IDs.\n            affinity: (batch, context_len, question_len)\n        \"\"\"\n        ### Word Embedding\n        context_emb_np = embed_batch(embedding_map=self.embedding_map, embedding_layer=self.embedding_layer, batch_token_ids=context_ids, idx2word=self.vocab_decoder, embed_dim=300)\n        question_emb_np = embed_batch(embedding_map=self.embedding_map, embedding_layer=self.embedding_layer, batch_token_ids=question_ids, idx2word=self.vocab_decoder, embed_dim=300)\n\n        # Convert numpy arrays to torch tensors (and ensure they are float type).\n        context_emb = torch.from_numpy(context_emb_np).float().contiguous().to(device)\n        question_emb = torch.from_numpy(question_emb_np).float().contiguous().to(device)\n\n        # --- Encoder: Contextual Embedding via CoAttention ---\n        # passage_attention_context, encoder_out = co_attention(context_emb, question_emb, True)\n        passage_attention_context, encoder_out = co_attention(\n            context_emb, question_emb, conv=True\n        )\n        # encoder_out shape: (B, L, 3*embedding_dim)\n\n        # --- Decoder for the Start Index Prediction ---\n        start_decoded = self.start_decoder(encoder_out)\n        # start_decoded shape: (B, L, 2*hidden_size)\n\n        # Concatenate encoder output with the decoded representation\n        start_input = torch.cat([encoder_out, start_decoded], dim=-1)  # (B, L, 3*embedding_dim + 2*hidden_size)\n        start_logits = self.start_linear(start_input).squeeze(-1)\n        start_probs = F.softmax(start_logits, dim=-1).unsqueeze(-1)\n\n        # Compute weighted summary using start_probs\n        start_summary = torch.sum(encoder_out * start_probs, dim=1, keepdim=True)\n        start_summary_expanded = start_summary.repeat(1, encoder_out.size(1), 1)  # (B, L, 3*embedding_dim)\n\n        # --- Prepare Features for End Index Prediction ---\n        combined_for_end = torch.cat([encoder_out, start_summary_expanded], dim=-1)  # (B, L, 6*embedding_dim)\n        \n        # --- Decoder for the End Index Prediction ---\n        end_decoded = self.end_decoder(combined_for_end)\n        \n        # Concatenate encoder output with the end decoded representation\n        # (Note: ensure arguments to torch.cat are provided as a list)\n        end_input = torch.cat([encoder_out, end_decoded], dim=-1)  # (B, L, 3*embedding_dim + 2*hidden_size)\n        end_logits = self.end_linear(end_input).squeeze(-1)\n\n        return start_logits, end_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:10:12.778540Z","iopub.execute_input":"2025-04-12T07:10:12.779234Z","iopub.status.idle":"2025-04-12T07:10:12.793027Z","shell.execute_reply.started":"2025-04-12T07:10:12.779210Z","shell.execute_reply":"2025-04-12T07:10:12.792215Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Main.py","metadata":{}},{"cell_type":"code","source":"# Load the datasets\ntrain_dataset = QADataset(path=\"/kaggle/input/squad-v2/train.json\")\n\n# Instantiate the vocabulary and tokenizer\nvocab = Vocabulary(train_dataset.samples, vocab_size=200000)\ntokenizer = Tokenizer(vocab)\ntrain_dataset.tokenizer = tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:05:54.617919Z","iopub.execute_input":"2025-04-12T07:05:54.618436Z","iopub.status.idle":"2025-04-12T07:08:01.862022Z","shell.execute_reply.started":"2025-04-12T07:05:54.618412Z","shell.execute_reply":"2025-04-12T07:08:01.861483Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import copy\n\n# Make a shallow copy of the dataset object\nfiltered_train_dataset = copy.deepcopy(train_dataset)\n\n# Replace only the samples with the filtered ones\nfiltered_train_dataset.samples = train_dataset.samples[\n    train_dataset.samples['context'].apply(lambda x: len(x) <= 400)\n].reset_index(drop=True)\n\nprint(\"Number of samples:\", len(filtered_train_dataset.samples))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T07:08:44.710969Z","iopub.execute_input":"2025-04-12T07:08:44.711554Z","iopub.status.idle":"2025-04-12T07:08:44.991796Z","shell.execute_reply.started":"2025-04-12T07:08:44.711532Z","shell.execute_reply":"2025-04-12T07:08:44.991127Z"}},"outputs":[{"name":"stdout","text":"Number of samples: 17822\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(torch.cuda.is_available())        # Should return True\nprint(torch.cuda.device_count())        # Should be > 0\nprint(torch.cuda.get_device_name(0))    # Should show your NVIDIA GPU\nprint(f\"Using device: {device}\")\n\n# Hyperparameters\nvocab_size = 200000\nembedding_dim = 300         # Embedding size for word embeddings\nhidden_size = 128           # Hidden size for LSTM\nnum_layers = 1              # Number of LSTM layers\noutput_size = 400           # Not used in our prediction layers for QA; prediction layers output logits per token\nbatch_size = 32\nlearning_rate = 0.001\n\n# Create DataLoader\ntrain_loader = DataLoader(filtered_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset._collate_batch)\n\n# Initialize the model\nmodel = QAModel(vocab_size=vocab_size,\n                vocab_decoder=vocab.decoding,\n                embedding_dim=embedding_dim,\n                hidden_size=hidden_size,\n                num_layers=num_layers,\n                output_size=output_size).to(device)\n\n# Loss function and optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nscheduler = CosineAnnealingLR(optimizer, T_max=10)\n\n# Checkpoint directory (modify as needed)\ncheckpoint_dir = \"/kaggle/working/model/\"\nimport os\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# Number of training epochs\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    valid_sample_count = 0\n\n    print(f\"Training Epoch {epoch + 1}/{num_epochs}...\")\n\n    # Iterate over the DataLoader\n    for batch in train_loader:\n        if batch is None:\n            continue  # skip batch if all samples failed\n                    \n        batch_size = batch['context'].size(0)\n        valid_sample_count += batch_size  # <-- Count valid samples\n            \n        # Unpack the batch\n        contexts = batch['context'].to(device)        # shape: (batch_size, context_len)\n        questions = batch['question'].to(device)       # shape: (batch_size, question_len)\n        start_positions = batch['answer_start'].to(device)  # shape: (batch_size,)\n        end_positions = batch['answer_end'].to(device)      # shape: (batch_size,)\n\n        # Forward pass: model returns start_logits, end_logits\n        start_logits, end_logits = model(contexts, questions)\n\n        def enforce_position_constraints(end_logits, start_positions):\n            \"\"\"\n            Mask positions in end_logits that are before the corresponding start positions.\n            \n            Args:\n                end_logits (Tensor): shape (batch_size, seq_len)\n                start_positions (Tensor): shape (batch_size,)\n                \n            Returns:\n                Tensor of the same shape as end_logits with positions before the start masked to -inf.\n            \"\"\"\n            # Ensure end_probs is 2D (batch_size, seq_len)\n            if end_logits.dim() == 3:\n                end_logits = end_logits.squeeze(-1)\n                \n            batch_size, seq_len = end_logits.shape\n            \n            # Create a tensor of position indices for each sequence\n            positions = torch.arange(seq_len, device=end_logits.device).unsqueeze(0).expand(batch_size, seq_len)\n            \n            # Create a mask where positions are before the corresponding start position\n            mask = positions < start_positions.unsqueeze(1)\n            \n            # Mask out positions by setting them to -inf\n            end_logits = end_logits.masked_fill(mask, -float('inf'))\n            \n            return end_logits\n            \n        end_logits = enforce_position_constraints(end_logits, start_positions)\n        \n        # Compute loss for start and end positions\n        start_loss = F.cross_entropy(start_logits, start_positions, reduction='none')\n        end_loss = F.cross_entropy(end_logits, end_positions, reduction='none')\n        ce_loss = 0.7 * start_loss.mean() + 0.3 * end_loss.mean()\n\n        # Span length regularization\n        pred_lengths = end_logits.argmax(-1) - start_logits.argmax(-1)\n        length_loss = F.relu(1 - pred_lengths.float()).mean()  # Penalize invalid spans\n\n        loss = ce_loss + 0.1 * length_loss\n\n        # -------------------------------------------------------------------------\n        # Additional Text Loss based on predicted and ground truth answer spans.\n        # -------------------------------------------------------------------------\n        \n        # Retrieve the context token embeddings using the model's embedding layer.\n        # Assumes model.embedding is the embedding layer and returns (batch_size, seq_len, embedding_dim)\n        context_embeddings = model.embedding_layer(contexts)\n        \n        # Determine predicted answer spans from model outputs.\n        pred_start_indices = start_logits.argmax(dim=1)  # shape: (batch_size,)\n        pred_end_indices   = end_logits.argmax(dim=1)      # shape: (batch_size,)\n    \n        predicted_embeddings = []\n        gt_embeddings = []\n\n        for i in range(contexts.size(0)):\n            # ---------------------------\n            # Predicted answer embedding\n            # ---------------------------\n            pred_s = pred_start_indices[i].item()\n            pred_e = pred_end_indices[i].item()\n            \n            # Ensure a valid span; if not, fallback to a single token.\n            if pred_e < pred_s:\n                pred_e = pred_s\n                \n            pred_span_embeds = context_embeddings[i, pred_s:pred_e+1, :]  # (span_length, embedding_dim)\n            \n            if pred_span_embeds.size(0) > 0:\n                pred_emb = pred_span_embeds.mean(dim=0)\n            else:\n                pred_emb = context_embeddings[i, pred_s, :]\n                \n            predicted_embeddings.append(pred_emb)\n            \n            # -------------------------------------\n            # Ground truth answer embedding\n            # -------------------------------------\n            # Use provided start_positions and end_positions (which are word indices)\n            gt_s = start_positions[i].item()\n            gt_e = end_positions[i].item()\n            \n            if gt_e < gt_s:\n                gt_e = gt_s\n                \n            gt_span_embeds = context_embeddings[i, gt_s:gt_e+1, :]  # (span_length, embedding_dim)\n            \n            if gt_span_embeds.size(0) > 0:\n                gt_emb = gt_span_embeds.mean(dim=0)\n            else:\n                gt_emb = context_embeddings[i, gt_s, :]\n                \n            gt_embeddings.append(gt_emb)\n        \n        predicted_embeddings = torch.stack(predicted_embeddings)  # (batch_size, embedding_dim)\n        gt_embeddings = torch.stack(gt_embeddings)                # (batch_size, embedding_dim)\n        \n        # Compute cosine similarity between predicted and ground truth embeddings.\n        cos_sim = F.cosine_similarity(predicted_embeddings, gt_embeddings, dim=1)\n        \n        # Define text loss as 1 minus the cosine similarity (perfect match gives 0 loss).\n        text_loss = (1 - cos_sim).mean()\n        \n        # Weight the text loss and add it to the overall loss.\n        alpha = 0.3  # Adjust weighting factor as needed.\n        loss = loss + alpha * text_loss\n\n        # Backward pass and optimization\n        loss.backward()\n        max_grad_norm = 1.0\n\n        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm, norm_type=2)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        running_loss += loss.item()\n    \n    avg_loss = running_loss / len(train_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n    print(f\"✅ Valid training samples this epoch: {valid_sample_count}\")\n\n    # Save a checkpoint every 5 epochs (or customize as needed)\n    if (epoch + 1) % 5 == 0:\n        checkpoint_path = os.path.join(checkpoint_dir, f\"qa_model_epoch_{epoch+1}.pt\")\n        checkpoint = {\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'vocab': vocab\n        }\n        torch.save(checkpoint, checkpoint_path)\n        print(f\"Checkpoint saved at {checkpoint_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T10:32:10.052325Z","iopub.execute_input":"2025-04-12T10:32:10.052602Z","iopub.status.idle":"2025-04-12T11:43:16.210808Z","shell.execute_reply.started":"2025-04-12T10:32:10.052580Z","shell.execute_reply":"2025-04-12T11:43:16.209807Z"}},"outputs":[{"name":"stdout","text":"True\n1\nTesla P100-PCIE-16GB\nUsing device: cuda\nTraining Epoch 1/15...\nEpoch [1/15], Loss: 3.6734\n✅ Valid training samples this epoch: 17722\nTraining Epoch 2/15...\nEpoch [2/15], Loss: 3.2208\n✅ Valid training samples this epoch: 17722\nTraining Epoch 3/15...\nEpoch [3/15], Loss: 2.9229\n✅ Valid training samples this epoch: 17722\nTraining Epoch 4/15...\nEpoch [4/15], Loss: 2.6022\n✅ Valid training samples this epoch: 17722\nTraining Epoch 5/15...\nEpoch [5/15], Loss: 2.3117\n✅ Valid training samples this epoch: 17722\nCheckpoint saved at /kaggle/working/model/qa_model_epoch_5.pt\nTraining Epoch 6/15...\nEpoch [6/15], Loss: 2.0362\n✅ Valid training samples this epoch: 17722\nTraining Epoch 7/15...\nEpoch [7/15], Loss: 1.7459\n✅ Valid training samples this epoch: 17722\nTraining Epoch 8/15...\nEpoch [8/15], Loss: 1.4959\n✅ Valid training samples this epoch: 17722\nTraining Epoch 9/15...\nEpoch [9/15], Loss: 1.2855\n✅ Valid training samples this epoch: 17722\nTraining Epoch 10/15...\nEpoch [10/15], Loss: 1.1087\n✅ Valid training samples this epoch: 17722\nCheckpoint saved at /kaggle/working/model/qa_model_epoch_10.pt\nTraining Epoch 11/15...\nEpoch [11/15], Loss: 0.9494\n✅ Valid training samples this epoch: 17722\nTraining Epoch 12/15...\nEpoch [12/15], Loss: 0.8282\n✅ Valid training samples this epoch: 17722\nTraining Epoch 13/15...\nEpoch [13/15], Loss: 0.7104\n✅ Valid training samples this epoch: 17722\nTraining Epoch 14/15...\nEpoch [14/15], Loss: 0.6161\n✅ Valid training samples this epoch: 17722\nTraining Epoch 15/15...\nEpoch [15/15], Loss: 0.5380\n✅ Valid training samples this epoch: 17722\nCheckpoint saved at /kaggle/working/model/qa_model_epoch_15.pt\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('models', 'zip', '/kaggle/working/model/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T11:43:29.808048Z","iopub.execute_input":"2025-04-12T11:43:29.808807Z","iopub.status.idle":"2025-04-12T11:44:12.200983Z","shell.execute_reply.started":"2025-04-12T11:43:29.808781Z","shell.execute_reply":"2025-04-12T11:44:12.200270Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/models.zip'"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'models.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T11:44:51.026800Z","iopub.execute_input":"2025-04-12T11:44:51.027095Z","iopub.status.idle":"2025-04-12T11:44:51.032128Z","shell.execute_reply.started":"2025-04-12T11:44:51.027048Z","shell.execute_reply":"2025-04-12T11:44:51.031378Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/models.zip","text/html":"<a href='models.zip' target='_blank'>models.zip</a><br>"},"metadata":{}}],"execution_count":51},{"cell_type":"markdown","source":"## Model Testing","metadata":{}},{"cell_type":"code","source":"# --- Utility functions ---\ndef compute_em(predicted, actual):\n    return int(predicted.strip().lower() == actual.strip().lower())\n\ndef compute_f1(predicted, actual):\n    pred_tokens = predicted.strip().lower().split()\n    actual_tokens = actual.strip().lower().split()\n    \n    common = set(pred_tokens) & set(actual_tokens)\n    if not common:\n        return 0.0\n    \n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(actual_tokens)\n    f1 = 2 * precision * recall / (precision + recall)\n    return f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T11:44:53.030188Z","iopub.execute_input":"2025-04-12T11:44:53.030472Z","iopub.status.idle":"2025-04-12T11:44:53.035625Z","shell.execute_reply.started":"2025-04-12T11:44:53.030449Z","shell.execute_reply":"2025-04-12T11:44:53.034831Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# --- Setup ---\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Checkpoint directory (modify as needed)\ncheckpoint_dir = \"/kaggle/working/out/\"\nimport os\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# Load validation dataset\nval_dataset = QADataset(path=\"/kaggle/input/validation/validation.json\")\ncheckpoint_dir = \"/kaggle/working/model/\"\ncheckpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith(\".pt\")])\n\n# Prepare validation vocab from any one checkpoint (they all use same vocab)\nsample_checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoints[-1]), map_location=device)\nval_vocab = sample_checkpoint['vocab']\nval_tokenizer = Tokenizer(val_vocab)\nval_dataset.tokenizer = val_tokenizer\n\n# Filter short contexts only\nval_dataset.samples = val_dataset.samples[val_dataset.samples['context'].apply(lambda x: len(x) <= 400)].reset_index(drop=True)\n\n# Shared hyperparameters\nvocab_size = 200000\nembedding_dim = 300\nhidden_size = 128\nnum_layers = 1\noutput_size = 400\n\n# Output summary\nsummary = []\n\n# === Loop Over Checkpoints ===\nfor ckpt_file in checkpoints:\n    print(f\"\\nEvaluating {ckpt_file}...\")\n\n    # Load model\n    checkpoint = torch.load(os.path.join(checkpoint_dir, ckpt_file), map_location=device)\n    model = QAModel(\n        vocab_size=vocab_size,\n        vocab_decoder=val_vocab.decoding,\n        embedding_dim=embedding_dim,\n        hidden_size=hidden_size,\n        num_layers=num_layers,\n        output_size=output_size\n    ).to(device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n\n    em_scores = []\n    f1_scores = []\n    output_data = []\n\n    valid_sample_count = 0\n    for i in range(len(val_dataset)):\n        sample = val_dataset[i]\n        \n        if sample is None:\n            continue\n        \n        row = val_dataset.samples.iloc[i]\n        valid_sample_count = valid_sample_count + 1\n\n        context_text = row['context']\n        question_text = row['question']\n        answer_text = row['answers']['text'][0].strip()\n        answer_start = row['answers']['answer_start'][0]\n        answer_end = answer_start + len(answer_text)\n\n        context_tensor = sample['context'].unsqueeze(0).to(device)\n        question_tensor = sample['question'].unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            start_logits, end_logits = model(context_tensor, question_tensor)\n            start_idx = torch.argmax(start_logits, dim=1).item()\n            end_idx = torch.argmax(end_logits, dim=1).item()\n            if end_idx < start_idx:\n                end_idx = start_idx\n\n        context_tokens = val_tokenizer.convert_ids_to_tokens(sample['context'].tolist())\n        predicted_answer = \" \".join(context_tokens[start_idx:end_idx + 1]).strip()\n\n        em = compute_em(predicted_answer, answer_text)\n        f1 = compute_f1(predicted_answer, answer_text)\n\n        em_scores.append(em)\n        f1_scores.append(f1)\n\n        output_data.append({\n            \"context\": context_text,\n            \"question\": question_text,\n            \"answer\": answer_text,\n            \"predicted_answer\": predicted_answer,\n            \"answer_start_index\": answer_start,\n            \"answer_end_index\": answer_end,\n            \"predicted_start_index\": start_idx,\n            \"predicted_end_index\": end_idx\n        })\n\n    # Save per-checkpoint results\n    epoch_num = ckpt_file.split(\"_\")[-1].replace(\".pt\", \"\")\n    df_output = pd.DataFrame(output_data)\n    df_output.to_csv(f\"/kaggle/working/out/validation_output_epoch_{epoch_num}.csv\", index=False)\n\n    # Log summary\n    mean_em = np.mean(em_scores) * 100\n    mean_f1 = np.mean(f1_scores) * 100\n    summary.append({\n        \"checkpoint\": ckpt_file,\n        \"EM\": mean_em,\n        \"F1\": mean_f1\n    })\n\n    print(f\"→ EM: {mean_em:.2f}%, F1: {mean_f1:.2f}%\")\n    print(f\"✅ Valid testing samples this epoch: {valid_sample_count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T11:44:58.861657Z","iopub.execute_input":"2025-04-12T11:44:58.862005Z","iopub.status.idle":"2025-04-12T11:47:29.211908Z","shell.execute_reply.started":"2025-04-12T11:44:58.861981Z","shell.execute_reply":"2025-04-12T11:47:29.211166Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3961438651.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  sample_checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoints[-1]), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluating qa_model_epoch_10.pt...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3961438651.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(os.path.join(checkpoint_dir, ckpt_file), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"→ EM: 6.12%, F1: 14.67%\n✅ Valid testing samples this epoch: 735\n\nEvaluating qa_model_epoch_15.pt...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3961438651.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(os.path.join(checkpoint_dir, ckpt_file), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"→ EM: 6.39%, F1: 14.83%\n✅ Valid testing samples this epoch: 735\n\nEvaluating qa_model_epoch_5.pt...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3961438651.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(os.path.join(checkpoint_dir, ckpt_file), map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"→ EM: 7.76%, F1: 15.41%\n✅ Valid testing samples this epoch: 735\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# Save summary CSV\nsummary_df = pd.DataFrame(summary)\nsummary_df.to_csv(\"/kaggle/working/out/eval_summary.csv\", index=False)\n\nprint(\"\\n✅ Evaluation complete! Results saved to:\")\nprint(\"- /kaggle/working/out/eval_summary.csv (summary)\")\nprint(\"- /kaggle/working/out/validation_output_epoch_*.csv (detailed per checkpoint)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T10:23:42.372855Z","iopub.execute_input":"2025-04-12T10:23:42.373134Z","iopub.status.idle":"2025-04-12T10:23:42.379544Z","shell.execute_reply.started":"2025-04-12T10:23:42.373110Z","shell.execute_reply":"2025-04-12T10:23:42.378867Z"}},"outputs":[{"name":"stdout","text":"\n✅ Evaluation complete! Results saved to:\n- /kaggle/working/out/eval_summary.csv (summary)\n- /kaggle/working/out/validation_output_epoch_*.csv (detailed per checkpoint)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('out', 'zip', '/kaggle/working/out/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T10:23:42.380286Z","iopub.execute_input":"2025-04-12T10:23:42.380602Z","iopub.status.idle":"2025-04-12T10:23:42.423110Z","shell.execute_reply.started":"2025-04-12T10:23:42.380575Z","shell.execute_reply":"2025-04-12T10:23:42.422560Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/out.zip'"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'out.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T10:23:42.423635Z","iopub.execute_input":"2025-04-12T10:23:42.423800Z","iopub.status.idle":"2025-04-12T10:23:42.428535Z","shell.execute_reply.started":"2025-04-12T10:23:42.423786Z","shell.execute_reply":"2025-04-12T10:23:42.427742Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/out.zip","text/html":"<a href='out.zip' target='_blank'>out.zip</a><br>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"summary_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T10:23:42.429149Z","iopub.execute_input":"2025-04-12T10:23:42.429301Z","iopub.status.idle":"2025-04-12T10:23:42.445716Z","shell.execute_reply.started":"2025-04-12T10:23:42.429289Z","shell.execute_reply":"2025-04-12T10:23:42.445026Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"             checkpoint        EM         F1\n0  qa_model_epoch_10.pt  7.346939  15.141960\n1  qa_model_epoch_15.pt  7.755102  16.124140\n2   qa_model_epoch_5.pt  6.802721  15.616295","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>checkpoint</th>\n      <th>EM</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>qa_model_epoch_10.pt</td>\n      <td>7.346939</td>\n      <td>15.141960</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>qa_model_epoch_15.pt</td>\n      <td>7.755102</td>\n      <td>16.124140</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>qa_model_epoch_5.pt</td>\n      <td>6.802721</td>\n      <td>15.616295</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}