{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Imports</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:28:44.673481Z",
     "iopub.status.busy": "2025-04-18T11:28:44.672943Z",
     "iopub.status.idle": "2025-04-18T11:28:44.689582Z",
     "shell.execute_reply": "2025-04-18T11:28:44.688362Z",
     "shell.execute_reply.started": "2025-04-18T11:28:44.673345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:28:46.978215Z",
     "iopub.status.busy": "2025-04-18T11:28:46.977608Z",
     "iopub.status.idle": "2025-04-18T11:28:47.199944Z",
     "shell.execute_reply": "2025-04-18T11:28:47.198785Z",
     "shell.execute_reply.started": "2025-04-18T11:28:46.978153Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:28:48.862840Z",
     "iopub.status.busy": "2025-04-18T11:28:48.862512Z",
     "iopub.status.idle": "2025-04-18T11:28:49.209546Z",
     "shell.execute_reply": "2025-04-18T11:28:49.208438Z",
     "shell.execute_reply.started": "2025-04-18T11:28:48.862812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = '[PAD]'\n",
    "UNK_TOKEN = '[UNK]'\n",
    "\n",
    "auto_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Helper Functions</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:28:50.988334Z",
     "iopub.status.busy": "2025-04-18T11:28:50.987845Z",
     "iopub.status.idle": "2025-04-18T11:28:51.029089Z",
     "shell.execute_reply": "2025-04-18T11:28:51.027420Z",
     "shell.execute_reply.started": "2025-04-18T11:28:50.988282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cuda(args, tensor):\n",
    "    \"\"\"\n",
    "    Places tensor on CUDA device (by default, uses cuda:0).\n",
    "    \n",
    "    Returns:\n",
    "        Tensor on CUDA device.\n",
    "    \"\"\"\n",
    "    if args.use_gpu and torch:\n",
    "        return tensor.cuda()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def unpack(tensor):\n",
    "    \"\"\"\n",
    "    Unpacks a tensor into a Python list.\n",
    "\n",
    "    Args:\n",
    "        tensor: PyTorch tensor.\n",
    "\n",
    "    Returns:\n",
    "        Python list with tensor contents.\n",
    "    \"\"\"\n",
    "    if tensor.requires_grad:\n",
    "        tensor = tensor.detach()\n",
    "    return tensor.cpu().numpy().tolist()\n",
    "\n",
    "def load_embeddings(path):\n",
    "    \"\"\"\n",
    "    Loads GloVe-style embeddings into memory.\n",
    "    Args:\n",
    "        path: Embedding path, e.g. \"glove/glove.6B.300d.txt\".\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping words (strings) to vectors (list of floats).\n",
    "    \"\"\"\n",
    "    embedding_map = {}\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                pieces = line.rstrip().split()\n",
    "                word = pieces[0].lower()  # Normalize to lowercase\n",
    "                embedding_map[word] = [float(weight) for weight in pieces[1:]]\n",
    "                \n",
    "                # Also store lemma if different\n",
    "                lemma = lemmatizer.lemmatize(word)\n",
    "                if lemma != word and lemma not in embedding_map:\n",
    "                    embedding_map[lemma] = [float(weight) for weight in pieces[1:]]\n",
    "            except:\n",
    "                pass\n",
    "    return embedding_map\n",
    "\n",
    "def embed_batch(embedding_map, embedding_layer, batch_token_ids, idx2word, embed_dim):\n",
    "    \"\"\"\n",
    "    Iteratively converts a batch of token id sequences into their embeddings.\n",
    "\n",
    "    Args:\n",
    "        embedding_map (dict): Mapping from to embedding vectors.\n",
    "        batch_token_ids (List[List[int]]): Batch where each element is a list of token ids.\n",
    "        idx2word (dict): Mapping from token ID (int) to the corresponding word (str)\n",
    "        embed_dim (int): The dimensionality of the embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array of shape (batch_size, seq_len, embed_dim) containing the embeddings.\n",
    "    \"\"\"\n",
    "    batch_embeddings = []\n",
    "    \n",
    "    for token_ids in batch_token_ids:\n",
    "        sequence_embeddings = []\n",
    "        for token_id in token_ids:\n",
    "            # Retrieve the corresponding word for the token id.\n",
    "            token = idx2word.get(token_id.item(), None)\n",
    "            # print(\"Token\", token_id.item(), token)\n",
    "            if token is None and token not in embedding_map:\n",
    "                token_embedding = np.zeros(embed_dim)\n",
    "            else:\n",
    "                try:\n",
    "                    token_tensor = torch.tensor([token_id.item()], device=device)\n",
    "                    token_embedding = embedding_layer(token_tensor).squeeze(0).cpu().detach().numpy()\n",
    "                except Exception as e:\n",
    "                    print(f\"Token ID {token_id} caused error: {e}\")\n",
    "                    token_embedding = np.zeros(embed_dim)\n",
    "\n",
    "            sequence_embeddings.append(token_embedding)\n",
    "        batch_embeddings.append(sequence_embeddings)\n",
    "    return np.array(batch_embeddings)\n",
    "\n",
    "def co_attention(context_embedding, question_embedding, conv=True):\n",
    "    \"\"\"\n",
    "    Co-attention mechanism that computes attention between context and question encodings.\n",
    "    If `convolution=True`, applies local smoothing to the affinity matrix.\n",
    "\n",
    "    Args:\n",
    "        context_embedding (Tensor): (B, context_len, d)\n",
    "        question_embedding (Tensor): (B, question_len, d)\n",
    "        convolution (bool): whether to apply convolution-based smoothing.\n",
    "\n",
    "    Returns:\n",
    "        CP (Tensor): passage attention context\n",
    "        E_Out (Tensor): final encoder output\n",
    "    \"\"\"\n",
    "    # Step 1: Affinity matrix A ∈ (B, context_len, question_len)\n",
    "    A = torch.bmm(context_embedding, question_embedding.transpose(1, 2))\n",
    "    # print(\"context_embedding = \", context_embedding[0])\n",
    "    # print(\"question_embedding = \", question_embedding[0])\n",
    "    \n",
    "    # print(\"context_embedding:\", context_embedding)\n",
    "    # print(\"question_embedding:\", question_embedding)\n",
    "    # print(\"Affinity range:\", A.min().item(), A.max().item())\n",
    "\n",
    "    # Apply learned smoothing\n",
    "    if conv:\n",
    "        A = conv_co_attention(A)\n",
    "\n",
    "    # Step 2: Passage-to-question attention (row-wise)\n",
    "    A_P = F.softmax(A, dim=2)\n",
    "\n",
    "    # Step 3: Question-to-passage attention (column-wise)\n",
    "    A_Q = F.softmax(A.transpose(1, 2), dim=2)\n",
    "\n",
    "    # Step 4: Passage attention context: CP = H^P × A^Q\n",
    "    # print(\"Context Embedding Shape\", context_embedding.shape)\n",
    "    # print(\"Question Embedding Shape\", question_embedding.shape)\n",
    "    # print(\"A_Q Shape\", A_Q.shape)\n",
    "    CP = torch.bmm(A_Q, context_embedding)\n",
    "    # print(\"CP Shape\", CP.shape)  # (B, Lq, d)\n",
    "\n",
    "    # Step 5: Encoder output: concat(H^P, [H^Q; CP] × A^P)\n",
    "    # QC = torch.cat([question_embedding, CP], dim=1)\n",
    "    QC_1 = torch.bmm(A_P, question_embedding)  # (B, Lq, d)\n",
    "    # print(\"QC_1 Shape\", QC_1.shape)\n",
    "\n",
    "    QC_2 = torch.bmm(A_P, CP)  # (B, Lq, d)\n",
    "    # print(\"QC_2 Shape\", QC_2.shape)\n",
    "\n",
    "    # add & norm\n",
    "    norm_1 = nn.LayerNorm(QC_1.shape[-1]).to(QC_1.device)\n",
    "    QC = norm_1(QC_1 + QC_2)  # (B, Lq, d)\n",
    "\n",
    "    # QC = torch.cat([QC_1, QC_2], dim=1) # (B, Lq, 2d)\n",
    "    # QC = torch.cat([QC_1, QC_2], dim=-1)  # (B, Lq, 2d)\n",
    "    # print(\"QC Shape\", QC.shape)\n",
    "\n",
    "    # Final encoder output\n",
    "    # E_Out = torch.cat([context_embedding, QC], dim=2)\n",
    "    E_Out = torch.cat([context_embedding, QC], dim=-1)  # (B, Lq, 3d)\n",
    "    E_Out = nn.LayerNorm(E_Out.shape[-1]).to(E_Out.device)(E_Out)\n",
    "    E_Out = torch.tanh(E_Out)  # Apply non-linearity\n",
    "\n",
    "    # project to original dimension\n",
    "    E_Out = nn.Linear(E_Out.shape[-1], context_embedding.shape[-1]).to(E_Out.device)(E_Out)\n",
    "    # print(\"E_Out Shape\", E_Out.shape)\n",
    "\n",
    "    return CP, E_Out\n",
    "\n",
    "def create_gaussian_kernel(kernel_width, device, sigma=1.0):\n",
    "    \"\"\"Creates a 1D Gaussian kernel.\"\"\"\n",
    "    x = torch.arange(-kernel_width//2 + 1, kernel_width//2 + 1, dtype=torch.float, device=device)\n",
    "    kernel = torch.exp(-x**2 / (2*sigma**2))\n",
    "    kernel /= kernel.sum()  # Normalize to sum to 1\n",
    "    return kernel.view(1, 1, -1)\n",
    "\n",
    "def conv_co_attention(A, kernel_width=11):\n",
    "    \"\"\"\n",
    "    Enhanced convolution to shift attention to neighboring words.\n",
    "    Applies 1D convolution along context dimension per question word.\n",
    "    \"\"\"\n",
    "    B, Lp, Lq = A.shape\n",
    "    # Permute A for per-question-word processing: (B, Lq, Lp) -> (B*Lq, 1, Lp)\n",
    "    A_reshaped = A.permute(0, 2, 1).reshape(-1, 1, Lp)\n",
    "    \n",
    "    # Create Gaussian kernel with odd kernel width (e.g., 11)\n",
    "    kernel = create_gaussian_kernel(kernel_width, A.device, sigma=1.0)\n",
    "    \n",
    "    # Use symmetric padding that keeps the sequence length unchanged.\n",
    "    padded_length = (kernel_width - 1) // 2\n",
    "    smoothed_A = F.conv1d(A_reshaped, kernel, padding=padded_length)\n",
    "    \n",
    "    # Reshape back: current shape is (B*Lq, 1, Lp) --> (B, Lq, Lp) then permute to (B, Lp, Lq)\n",
    "    smoothed_A = smoothed_A.view(B, Lq, Lp).permute(0, 2, 1)\n",
    "    A_adjusted = A + smoothed_A  # Enhance original scores with neighbor context\n",
    "    return F.softmax(A_adjusted, dim=-1)\n",
    "\n",
    "def tokenize_with_bert(text):\n",
    "    # Tokenize the text and request offset mappings.\n",
    "    encoding = auto_tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=False  # Disable adding special tokens to mimic simple whitespace tokenization.\n",
    "    )\n",
    "    \n",
    "    # Retrieve the tokens.\n",
    "    tokens = auto_tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "    \n",
    "    # Retrieve the spans from the offset mapping.\n",
    "    spans = encoding['offset_mapping']\n",
    "    return tokens, spans\n",
    "    \n",
    "def create_embedding_matrix(vocab, embedding_map, embedding_dim=300, scale=0.6):\n",
    "    \"\"\"Initialize embedding matrix with:\n",
    "    - GloVe vectors for known words\n",
    "    - Random vectors for UNK tokens\n",
    "    - Zero vector for padding\n",
    "    \"\"\"\n",
    "    # Initialize with random normal distribution (match GloVe scale)\n",
    "    embedding_matrix = np.random.normal(\n",
    "        scale=scale, \n",
    "        size=(len(vocab), embedding_dim)\n",
    "    )\n",
    "    \n",
    "    # Handle special tokens\n",
    "    embedding_matrix[vocab.encoding[PAD_TOKEN]] = np.zeros(embedding_dim)\n",
    "    unk_idx = vocab.encoding[UNK_TOKEN]\n",
    "    embedding_matrix[unk_idx] = np.random.normal(scale=scale, size=embedding_dim)\n",
    "    \n",
    "    for word, idx in vocab.encoding.items():\n",
    "        if word in [PAD_TOKEN, UNK_TOKEN]:\n",
    "            continue\n",
    "            \n",
    "        # Try direct match\n",
    "        if word in embedding_map:\n",
    "            embedding_matrix[idx] = embedding_map[word]\n",
    "            continue\n",
    "            \n",
    "        # Try lemma\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma in embedding_map:\n",
    "            embedding_matrix[idx] = embedding_map[lemma]\n",
    "            continue\n",
    "            \n",
    "        # Try lowercase lemma\n",
    "        lower_lemma = lemmatizer.lemmatize(word.lower())\n",
    "        if lower_lemma in embedding_map:\n",
    "            embedding_matrix[idx] = embedding_map[lower_lemma]\n",
    "\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "def enforce_position_constraints(end_logits, start_positions):\n",
    "    \"\"\"\n",
    "    Mask end_logits positions before the corresponding start_positions.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = end_logits.size()\n",
    "    positions = torch.arange(seq_len, device=end_logits.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "    mask = positions < start_positions.unsqueeze(1)\n",
    "    return end_logits.masked_fill(mask, float('-inf'))\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    tokens = re.findall(r\"\\w+(?:[-']\\w+)*\", text.lower())\n",
    "    spans = []\n",
    "    start = 0\n",
    "    for token in tokens:\n",
    "        start = text.lower().find(token, start)\n",
    "        spans.append((start, start + len(token)))\n",
    "        start += len(token)\n",
    "    return tokens, spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Data Handling</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:28:55.237805Z",
     "iopub.status.busy": "2025-04-18T11:28:55.237462Z",
     "iopub.status.idle": "2025-04-18T11:28:55.261847Z",
     "shell.execute_reply": "2025-04-18T11:28:55.260730Z",
     "shell.execute_reply.started": "2025-04-18T11:28:55.237781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "#                                                                                                      #\n",
    "#                                                Vocabulary                                            #\n",
    "#                                                                                                      #\n",
    "########################################################################################################\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Creates mappings for words → indices and indices → words.\n",
    "    \"\"\"\n",
    "    def __init__(self, samples, vocab_size):\n",
    "        self.samples = samples\n",
    "        self.vocab_size = vocab_size\n",
    "        self.words = self._initialize(samples, vocab_size)\n",
    "        self.encoding = {word: idx for idx, word in enumerate(self.words)}\n",
    "        self.decoding = {idx: word for idx, word in enumerate(self.words)}\n",
    "\n",
    "    def _initialize(self, samples, vocab_size):\n",
    "        \"\"\"Build vocabulary with lemma support\"\"\"\n",
    "        embedding_map = load_embeddings(\"/kaggle/input/glove/other/default/1/glove.6B.300d.txt\")\n",
    "        vocab_counts = collections.defaultdict(int)\n",
    "        \n",
    "        for _, row in samples.iterrows():\n",
    "            # Get base tokens\n",
    "            tokens = re.findall(r\"\\w+(?:[-']\\w+)*\", row['context'].lower()) + \\\n",
    "                     re.findall(r\"\\w+(?:[-']\\w+)*\", row['question'].lower())\n",
    "            \n",
    "            # Count both original and lemma forms\n",
    "            for token in tokens:\n",
    "                vocab_counts[token] += 1\n",
    "                lemma = lemmatizer.lemmatize(token)\n",
    "                if lemma != token:\n",
    "                    vocab_counts[lemma] += 0.5  # Partial count for lemmas\n",
    "        \n",
    "        # Sort by combined frequency\n",
    "        sorted_words = sorted(vocab_counts.items(), \n",
    "                            key=lambda x: (-x[1], x[0]))[:vocab_size-2]\n",
    "        \n",
    "        return [PAD_TOKEN, UNK_TOKEN] + [w[0] for w in sorted_words]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "########################################################################################################\n",
    "#                                                                                                      #\n",
    "#                                                Tokenizer                                             #\n",
    "#                                                                                                      #\n",
    "########################################################################################################\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Converts lists of words to indices and vice versa.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.pad_token_id = vocabulary.encoding[PAD_TOKEN]\n",
    "        self.unk_token_id = vocabulary.encoding[UNK_TOKEN]\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.vocabulary.encoding.get(token.lower(), self.unk_token_id) for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        return [self.vocabulary.decoding.get(token_id, UNK_TOKEN) for token_id in token_ids]\n",
    "\n",
    "########################################################################################################\n",
    "#                                                                                                      #\n",
    "#                                                QADataset                                             #\n",
    "#                                                                                                      #\n",
    "########################################################################################################\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Data generator for a QA task; the JSON file should contain character-level answer indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        # Load JSON-lines file; each line is a JSON object.\n",
    "        self.samples = pd.read_json(path, lines=True)\n",
    "        self.tokenizer = None\n",
    "        # Default pad token id; updated after tokenizer registration.\n",
    "        self.pad_token_id = 0\n",
    "\n",
    "    def _collate_batch(self, batch):\n",
    "        batch = [sample for sample in batch if sample is not None]\n",
    "        if len(batch) == 0:\n",
    "            return None  # All samples failed\n",
    "    \n",
    "        max_context_len = max(sample['context'].size(0) for sample in batch)\n",
    "        max_question_len = max(sample['question'].size(0) for sample in batch)\n",
    "        \n",
    "        contexts = torch.stack([\n",
    "            torch.cat([\n",
    "                sample['context'],\n",
    "                torch.full((max_context_len - sample['context'].size(0),), self.pad_token_id, dtype=torch.long)\n",
    "            ]) for sample in batch\n",
    "        ])\n",
    "    \n",
    "        questions = torch.stack([\n",
    "            torch.cat([\n",
    "                sample['question'],\n",
    "                torch.full((max_question_len - sample['question'].size(0),), self.pad_token_id, dtype=torch.long)\n",
    "            ]) for sample in batch\n",
    "        ])\n",
    "    \n",
    "        answer_starts = torch.stack([sample['answer_start'] for sample in batch])\n",
    "        answer_ends = torch.stack([sample['answer_end'] for sample in batch])\n",
    "    \n",
    "        return {\n",
    "            'context': contexts,\n",
    "            'question': questions,\n",
    "            'answer_start': answer_starts,\n",
    "            'answer_end': answer_ends\n",
    "        }\n",
    "\n",
    "    def register_tokenizer(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Registers a Tokenizer instance and updates pad token id.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples.iloc[idx]\n",
    "        context_str = sample['context']\n",
    "        question_str = sample['question']\n",
    "        answers = sample['answers']\n",
    "    \n",
    "        # Tokenize context and question\n",
    "        # context_tokens, context_spans = tokenize_with_bert(context_str)\n",
    "        # question_tokens, _ = tokenize_with_bert(question_str)\n",
    "\n",
    "        context_tokens, context_spans = simple_tokenizer(context_str)\n",
    "        question_tokens, _ = simple_tokenizer(question_str)\n",
    "    \n",
    "        context_indices = self.tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "        question_indices = self.tokenizer.convert_tokens_to_ids(question_tokens)\n",
    "    \n",
    "        # Character-level answer span\n",
    "        try:\n",
    "            char_start = answers['answer_start'][0]\n",
    "            answer_text = answers['text'][0].strip()\n",
    "            char_end = char_start + len(answer_text)\n",
    "        except Exception as e:\n",
    "            return None  # Invalid or empty answer\n",
    "    \n",
    "        # Convert character span to token indices\n",
    "        token_start, token_end = None, None\n",
    "        for i, (s, e) in enumerate(context_spans):\n",
    "            if s <= char_start < e:\n",
    "                token_start = i\n",
    "            if s < char_end <= e:\n",
    "                token_end = i\n",
    "            if token_start is not None and token_end is not None:\n",
    "                break\n",
    "    \n",
    "        if token_start is None or token_end is None:\n",
    "            return None  # Skip misaligned sample\n",
    "    \n",
    "        return {\n",
    "            'context': torch.tensor(context_indices, dtype=torch.long),\n",
    "            'question': torch.tensor(question_indices, dtype=torch.long),\n",
    "            'answer_start': torch.tensor(token_start, dtype=torch.long),\n",
    "            'answer_end': torch.tensor(token_end, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Model Architecture</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:28:58.070964Z",
     "iopub.status.busy": "2025-04-18T11:28:58.070616Z",
     "iopub.status.idle": "2025-04-18T11:28:58.102623Z",
     "shell.execute_reply": "2025-04-18T11:28:58.101576Z",
     "shell.execute_reply.started": "2025-04-18T11:28:58.070936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "#                                                                                                      #\n",
    "#                                       Transformer Blocks                                             #\n",
    "#                                                                                                      #\n",
    "########################################################################################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism for the Transformer model.\n",
    "    This class implements the multi-head attention mechanism as described in the paper \"Attention is All You Need\".\n",
    "    \n",
    "    Args:\n",
    "        d_model (Tensor): Dimensionality of the input\n",
    "        num_heads (int): The number of attention heads to split the input into it.\n",
    "\n",
    "    Returns:\n",
    "        context: (Tensor): The context vector after applying attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # Ensure that the model dimension is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "\n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Calculate the context vector as a weighted sum of values\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split the input tensor into multiple heads for multi-head attention.\n",
    "        \"\"\"\n",
    "        # Reshape the input to have num_heads for multi-head attention.\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape.\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Apply linear transformations and split heads.\n",
    "        query = self.split_heads(self.W_q(query))  # (batch_size, num_heads, seq_length, d_k)\n",
    "        key = self.split_heads(self.W_k(key))  # (batch_size, num_heads, seq_length, d_k)\n",
    "        value = self.split_heads(self.W_v(value))  # (batch_size, num_heads, seq_length, d_k)\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        context = self.scaled_dot_product_attention(query, key, value, mask)  # (batch_size, num_heads, seq_length, d_k)\n",
    "\n",
    "        # Combine the heads and apply output transformation.\n",
    "        output = self.W_o(self.combine_heads(context))\n",
    "\n",
    "        return output\n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward netowrk consists of two linear transformations with a ReLU activation in between.\n",
    "    In the context of transformer models, this feed-forward network is applied to each position separately and identically.\n",
    "    It helps in transforming the features learned by the attention mechanisms within the transformer, \n",
    "    acting as an additional processing step for the attention outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout_prob=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the input.\n",
    "            d_ff (int): Dimensionality of the feed-forward layer.\n",
    "            dropout_prob (float): Dropout probability for regularization.\n",
    "        \"\"\"\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        # Feed-forward network with two linear layers and ReLU activation.\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            out (Tensor): Output tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        # Apply the first linear transformation, activation, and dropout.\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply the second linear transformation.\n",
    "        out = self.fc2(x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding is used to inject information about the relative or absolute position of the tokens in the sequence.\n",
    "    It helps the model understand the order of the tokens, as the transformer architecture does not inherently capture this information.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the model.\n",
    "            max_seq_length (int): Maximum length of the input sequences.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model) # A tensor filled with zeros, which will be populated with positional encodings.\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1) # A tensor containing the position indices for each positon in the sequence.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) # A term used to scale the position indices in a spcific way.\n",
    "        \n",
    "        # Apply the sine function to the even indices.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # Apply the cosine the function to the odd indices.\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Register pe as a buffer, which means it will be part of the module's state but will not be considered a trainable parameter.\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input tensor.\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder block that consists of multi-head self-attention and feed-forward layers.\n",
    "    The encoder processes the input sequences and generates a contextual representation of the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_prob=0.3):\n",
    "        \"\"\"\n",
    "        Transformer Encoder block that consists of multi-head self-attention and feed-forward layers.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the model.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimensionality of the feed-forward layer.\n",
    "            dropout_prob (float): Dropout probability for regularization.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads) # multi-head attention mechanism.\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout_prob) # position-wise feed-forward neural network.\n",
    "        self.norm1 = nn.LayerNorm(d_model) # layer normalization, applied to smooth the layer's input.\n",
    "        self.norm2 = nn.LayerNorm(d_model) # layer normalization, applied to smooth the layer's input.\n",
    "        self.dropout = nn.Dropout(dropout_prob) # dropout layer, used to prevent overfitting by randomly setting some activatons to zero during training.\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer encoder block.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (Tensor): Mask tensor to prevent attention to certain positions.\n",
    "\n",
    "        Returns:\n",
    "            out (Tensor): Output tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        # Apply multi-head self-attention\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "\n",
    "        # Apply dropout and layer normalization.\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # Apply feed-forward network\n",
    "        ff_output = self.feed_forward(x)\n",
    "\n",
    "        # Apply dropout and layer normalization.\n",
    "        out = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return out\n",
    "\n",
    "########################################################################################################\n",
    "#                                                                                                      #\n",
    "#                                    QATransformerBasedModel                                           #\n",
    "#                                                                                                      #\n",
    "########################################################################################################\n",
    "class QATransformerBasedModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab, vocab_decoder, embedding_dim, d_ff,\n",
    "        num_layers, num_heads, max_seq_length, dropout_prob=0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding + Positional Encoding\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=len(vocab),\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=vocab.encoding[PAD_TOKEN]\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(embedding_dim, max_seq_length)\n",
    "\n",
    "        # Split layers into pre- and post-cross-attention\n",
    "        num_pre = num_layers // 2\n",
    "        num_post = num_layers - num_pre\n",
    "        self.pre_encoders = nn.ModuleList([\n",
    "            TransformerEncoder(embedding_dim, num_heads, d_ff, dropout_prob)\n",
    "            for _ in range(num_pre)\n",
    "        ])\n",
    "        self.post_encoders = nn.ModuleList([\n",
    "            TransformerEncoder(embedding_dim, num_heads, d_ff, dropout_prob)\n",
    "            for _ in range(num_post)\n",
    "        ])\n",
    "\n",
    "        # Cross-Attention\n",
    "        self.cross_attn = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.cross_attn_norm = nn.LayerNorm(embedding_dim, embedding_dim)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Span Prediction Head (FFN)\n",
    "        self.ffn1 = nn.Linear(embedding_dim, 2 * embedding_dim)\n",
    "        self.norm1 = nn.LayerNorm(2 * embedding_dim)\n",
    "        self.ffn2 = nn.Linear(2 * embedding_dim, embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.classifier = nn.Linear(embedding_dim, 2)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def forward(self, context_ids, question_ids):\n",
    "        # Embedding + Positional Encoding\n",
    "        c = self.embedding(context_ids)\n",
    "        c = self.pos_encoding(c)\n",
    "        q = self.embedding(question_ids)\n",
    "        q = self.pos_encoding(q)\n",
    "\n",
    "        # Masks\n",
    "        c_mask = (context_ids != self.vocab.encoding[PAD_TOKEN]).unsqueeze(1).unsqueeze(2)\n",
    "        q_mask = (question_ids != self.vocab.encoding[PAD_TOKEN]).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # 1) Pre-cross self-attention stacks\n",
    "        for enc in self.pre_encoders:\n",
    "            c = enc(c, c_mask)\n",
    "            q = enc(q, q_mask)\n",
    "\n",
    "        # 2) Cross-attention: context attends to question\n",
    "        c = self.cross_attn(query=c, key=q, value=q, mask=q_mask)\n",
    "        c = self.cross_attn_dropout(c)\n",
    "        c = self.cross_attn_norm(c)\n",
    "\n",
    "        # 3) Post-cross self-attention stacks\n",
    "        for enc in self.post_encoders:\n",
    "            c = enc(c, c_mask)\n",
    "\n",
    "        # 4) Span prediction feed-forward head\n",
    "        x = self.dropout(F.relu(self.norm1(self.ffn1(c))))\n",
    "        x = self.dropout(F.relu(self.norm2(self.ffn2(x))))\n",
    "        logits = self.classifier(x)  # (B, L, 2)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        return start_logits.squeeze(-1), end_logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Model Training</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:29:02.713629Z",
     "iopub.status.busy": "2025-04-18T11:29:02.712711Z",
     "iopub.status.idle": "2025-04-18T11:29:02.720055Z",
     "shell.execute_reply": "2025-04-18T11:29:02.718988Z",
     "shell.execute_reply.started": "2025-04-18T11:29:02.713597Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluation Functions\n",
    "def compute_em(predicted, actual):\n",
    "    return int(predicted.strip().lower() == actual.strip().lower())\n",
    "\n",
    "def compute_f1(predicted, actual):\n",
    "    pred_tokens = predicted.strip().lower().split()\n",
    "    actual_tokens = actual.strip().lower().split()\n",
    "\n",
    "    common = set(pred_tokens) & set(actual_tokens)\n",
    "    if not common:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(actual_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Data paths & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:18:14.570930Z",
     "iopub.status.busy": "2025-04-18T12:18:14.570612Z",
     "iopub.status.idle": "2025-04-18T12:18:14.580274Z",
     "shell.execute_reply": "2025-04-18T12:18:14.579105Z",
     "shell.execute_reply.started": "2025-04-18T12:18:14.570911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "# Kaggle dataset paths\n",
    "MODEL_NAME = 'QATransformerBasedModel'  # or '# Kaggle dataset paths\n",
    "TRAIN_PATH = '/kaggle/input/squad-v2/train.json'\n",
    "VAL_PATH = '/kaggle/input/validation/validation.json'\n",
    "WORKING_DIR = '/kaggle/working'\n",
    "MODEL_DIR = os.path.join(WORKING_DIR, 'model')\n",
    "OUT_DIR = os.path.join(WORKING_DIR, 'out')\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-3\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_HEADS = 6\n",
    "D_FF = 512\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "MAX_CONTEXT_LEN = 400\n",
    "MODEL_NAME = 'QATransformerBasedModel'\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:29:16.710659Z",
     "iopub.status.busy": "2025-04-18T11:29:16.710303Z",
     "iopub.status.idle": "2025-04-18T11:32:29.813991Z",
     "shell.execute_reply": "2025-04-18T11:32:29.812626Z",
     "shell.execute_reply.started": "2025-04-18T11:29:16.710634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_ds = QADataset(path=TRAIN_PATH)\n",
    "val_ds = QADataset(path=VAL_PATH)\n",
    "\n",
    "# Build vocab and tokenizer\n",
    "vocab = Vocabulary(train_ds.samples, vocab_size=200000)\n",
    "tokenizer = Tokenizer(vocab)\n",
    "train_ds.tokenizer = tokenizer\n",
    "val_ds.tokenizer = tokenizer\n",
    "\n",
    "# Filter long contexts\n",
    "def filter_len(df):\n",
    "    return df[df['context'].map(len) <= MAX_CONTEXT_LEN].reset_index(drop=True)\n",
    "    \n",
    "train_ds.samples = filter_len(train_ds.samples)\n",
    "val_ds.samples = filter_len(val_ds.samples)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=train_ds._collate_batch, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        collate_fn=val_ds._collate_batch, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T11:46:39.741153Z",
     "iopub.status.busy": "2025-04-18T11:46:39.739576Z",
     "iopub.status.idle": "2025-04-18T11:46:39.749090Z",
     "shell.execute_reply": "2025-04-18T11:46:39.747661Z",
     "shell.execute_reply.started": "2025-04-18T11:46:39.741112Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17822"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:18:21.432059Z",
     "iopub.status.busy": "2025-04-18T12:18:21.431462Z",
     "iopub.status.idle": "2025-04-18T12:18:21.466596Z",
     "shell.execute_reply": "2025-04-18T12:18:21.465477Z",
     "shell.execute_reply.started": "2025-04-18T12:18:21.432012Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples:          17822\n",
      "Unique contexts count:   12533\n"
     ]
    }
   ],
   "source": [
    "total_examples = len(train_ds.samples)\n",
    "num_unique_contexts = train_ds.samples['context'].nunique()\n",
    "\n",
    "print(f\"Total examples:          {total_examples}\")\n",
    "print(f\"Unique contexts count:   {num_unique_contexts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:37:27.140297Z",
     "iopub.status.busy": "2025-04-18T12:37:27.139874Z",
     "iopub.status.idle": "2025-04-18T12:37:27.147023Z",
     "shell.execute_reply": "2025-04-18T12:37:27.145853Z",
     "shell.execute_reply.started": "2025-04-18T12:37:27.140269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def span_loss_no_mask(start_logits, end_logits, start_pos, end_pos):\n",
    "    # 1) start loss over full distribution\n",
    "    start_loss = F.cross_entropy(start_logits, start_pos)\n",
    "    \n",
    "    # 2) predicted start (detach so no grad through argmax)\n",
    "    with torch.no_grad():\n",
    "        s_pred = start_logits.argmax(dim=1)  # (B,)\n",
    "    \n",
    "    # 3) end loss over full distribution (unconstrained)\n",
    "    end_loss = F.cross_entropy(end_logits, end_pos)\n",
    "    \n",
    "    return start_loss, end_loss, s_pred, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:37:50.258900Z",
     "iopub.status.busy": "2025-04-18T12:37:50.258522Z",
     "iopub.status.idle": "2025-04-18T18:42:52.402049Z",
     "shell.execute_reply": "2025-04-18T18:42:52.400217Z",
     "shell.execute_reply.started": "2025-04-18T12:37:50.258870Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 — Loss: 7.7616 — Start Acc: 5.97% — End Acc: 7.21%\n",
      "Epoch 1 — Val EM: 1.02%, F1: 1.48%\n",
      "Epoch 2/30 — Loss: 7.0210 — Start Acc: 8.23% — End Acc: 10.29%\n",
      "Epoch 2 — Val EM: 0.87%, F1: 1.29%\n",
      "Epoch 3/30 — Loss: 6.5355 — Start Acc: 10.24% — End Acc: 12.80%\n",
      "Epoch 3 — Val EM: 0.73%, F1: 1.07%\n",
      "Epoch 4/30 — Loss: 6.0738 — Start Acc: 12.25% — End Acc: 15.30%\n",
      "Epoch 4 — Val EM: 0.73%, F1: 0.99%\n",
      "Epoch 5/30 — Loss: 5.5287 — Start Acc: 14.45% — End Acc: 18.09%\n",
      "Epoch 5 — Val EM: 0.73%, F1: 1.08%\n",
      "Checkpoint saved: /kaggle/working/model/QATransformerBasedModel_ep5.pt\n",
      "Epoch 6/30 — Loss: 4.8340 — Start Acc: 16.91% — End Acc: 21.42%\n",
      "Epoch 6 — Val EM: 0.73%, F1: 1.06%\n",
      "Epoch 7/30 — Loss: 4.1029 — Start Acc: 19.70% — End Acc: 25.21%\n",
      "Epoch 7 — Val EM: 1.02%, F1: 1.37%\n",
      "Epoch 8/30 — Loss: 3.3435 — Start Acc: 22.69% — End Acc: 29.41%\n",
      "Epoch 8 — Val EM: 0.58%, F1: 1.15%\n",
      "Epoch 9/30 — Loss: 2.7488 — Start Acc: 25.85% — End Acc: 33.65%\n",
      "Epoch 9 — Val EM: 0.73%, F1: 1.10%\n",
      "Epoch 10/30 — Loss: 2.2137 — Start Acc: 29.12% — End Acc: 37.71%\n",
      "Epoch 10 — Val EM: 0.73%, F1: 1.17%\n",
      "Checkpoint saved: /kaggle/working/model/QATransformerBasedModel_ep10.pt\n",
      "Epoch 11/30 — Loss: 1.7982 — Start Acc: 32.49% — End Acc: 41.49%\n",
      "Epoch 11 — Val EM: 0.44%, F1: 0.83%\n",
      "Epoch 12/30 — Loss: 1.4408 — Start Acc: 35.90% — End Acc: 44.99%\n",
      "Epoch 12 — Val EM: 0.73%, F1: 1.10%\n",
      "Epoch 13/30 — Loss: 1.1526 — Start Acc: 39.20% — End Acc: 48.19%\n",
      "Epoch 13 — Val EM: 0.73%, F1: 1.07%\n",
      "Epoch 14/30 — Loss: 0.9575 — Start Acc: 42.29% — End Acc: 51.08%\n",
      "Epoch 14 — Val EM: 0.44%, F1: 0.81%\n",
      "Epoch 15/30 — Loss: 0.8008 — Start Acc: 45.18% — End Acc: 53.72%\n",
      "Epoch 15 — Val EM: 0.73%, F1: 1.10%\n",
      "Checkpoint saved: /kaggle/working/model/QATransformerBasedModel_ep15.pt\n",
      "Epoch 16/30 — Loss: 0.6731 — Start Acc: 47.86% — End Acc: 56.12%\n",
      "Epoch 16 — Val EM: 0.44%, F1: 1.10%\n",
      "Epoch 17/30 — Loss: 0.5613 — Start Acc: 50.34% — End Acc: 58.30%\n",
      "Epoch 17 — Val EM: 0.73%, F1: 1.18%\n",
      "Epoch 18/30 — Loss: 0.4775 — Start Acc: 52.63% — End Acc: 60.30%\n",
      "Epoch 18 — Val EM: 0.73%, F1: 1.20%\n",
      "Epoch 19/30 — Loss: 0.4058 — Start Acc: 54.74% — End Acc: 62.15%\n",
      "Epoch 19 — Val EM: 0.58%, F1: 1.04%\n",
      "Epoch 20/30 — Loss: 0.3520 — Start Acc: 56.71% — End Acc: 63.83%\n",
      "Epoch 20 — Val EM: 0.29%, F1: 0.65%\n",
      "Checkpoint saved: /kaggle/working/model/QATransformerBasedModel_ep20.pt\n",
      "Epoch 21/30 — Loss: 0.2982 — Start Acc: 58.54% — End Acc: 65.40%\n",
      "Epoch 21 — Val EM: 0.58%, F1: 1.00%\n",
      "Epoch 22/30 — Loss: 0.2519 — Start Acc: 60.24% — End Acc: 66.84%\n",
      "Epoch 22 — Val EM: 0.73%, F1: 1.15%\n",
      "Epoch 23/30 — Loss: 0.2119 — Start Acc: 61.83% — End Acc: 68.19%\n",
      "Epoch 23 — Val EM: 0.58%, F1: 1.00%\n",
      "Epoch 24/30 — Loss: 0.1709 — Start Acc: 63.33% — End Acc: 69.44%\n",
      "Epoch 24 — Val EM: 0.58%, F1: 1.08%\n",
      "Epoch 25/30 — Loss: 0.1375 — Start Acc: 64.72% — End Acc: 70.61%\n",
      "Epoch 25 — Val EM: 0.58%, F1: 1.02%\n",
      "Checkpoint saved: /kaggle/working/model/QATransformerBasedModel_ep25.pt\n",
      "Epoch 26/30 — Loss: 0.1017 — Start Acc: 66.02% — End Acc: 71.70%\n",
      "Epoch 26 — Val EM: 0.44%, F1: 0.93%\n",
      "Epoch 27/30 — Loss: 0.0852 — Start Acc: 67.25% — End Acc: 72.72%\n",
      "Epoch 27 — Val EM: 0.44%, F1: 0.86%\n",
      "Epoch 28/30 — Loss: 0.0502 — Start Acc: 68.39% — End Acc: 73.68%\n",
      "Epoch 28 — Val EM: 0.58%, F1: 0.96%\n",
      "Epoch 29/30 — Loss: 0.0381 — Start Acc: 69.47% — End Acc: 74.57%\n",
      "Epoch 29 — Val EM: 0.58%, F1: 0.96%\n",
      "Epoch 30/30 — Loss: 0.0297 — Start Acc: 70.47% — End Acc: 75.41%\n",
      "Epoch 30 — Val EM: 0.58%, F1: 0.95%\n",
      "Checkpoint saved: /kaggle/working/model/QATransformerBasedModel_ep30.pt\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = QATransformerBasedModel(\n",
    "    vocab=vocab,\n",
    "    vocab_decoder=vocab.decoding,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    max_seq_length=MAX_CONTEXT_LEN,\n",
    "    dropout_prob=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=NUM_EPOCHS * len(train_loader)\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "train_losses      = []\n",
    "train_start_accs  = []\n",
    "train_end_accs    = []\n",
    "val_em_scores     = []\n",
    "val_f1_scores     = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        if batch is None:\n",
    "            continue\n",
    "        \n",
    "        ctx = batch['context'].to(device)\n",
    "        qry = batch['question'].to(device)\n",
    "        start_gt = batch['answer_start'].to(device)\n",
    "        end_gt   = batch['answer_end'].to(device)\n",
    "\n",
    "        # compute context lengths once\n",
    "        PAD_ID = tokenizer.pad_token_id\n",
    "        context_lengths = (ctx != PAD_ID).sum(dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        s_logit, e_logit = model(ctx, qry)                            # (B, L)\n",
    "\n",
    "        # use predicted start for masking end\n",
    "        start_loss, end_loss, s_pred, e_logit_masked = span_loss_no_mask(\n",
    "            s_logit, e_logit, start_gt, end_gt\n",
    "        )\n",
    "\n",
    "        loss = start_loss + 1.2 * end_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # track per-batch accuracies\n",
    "        start_acc = (s_pred == start_gt).float().mean().item()\n",
    "        e_pred = e_logit_masked.argmax(dim=1)\n",
    "        end_acc   = (e_pred == end_gt).float().mean().item()\n",
    "\n",
    "        train_start_accs.append(start_acc)\n",
    "        train_end_accs.append(end_acc)\n",
    "\n",
    "    avg_loss      = epoch_loss / len(train_loader)\n",
    "    avg_start_acc = np.mean(train_start_accs) * 100\n",
    "    avg_end_acc   = np.mean(train_end_accs)   * 100\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} — \"\n",
    "          f\"Loss: {avg_loss:.4f} — \"\n",
    "          f\"Start Acc: {avg_start_acc:.2f}% — \"\n",
    "          f\"End Acc: {avg_end_acc:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    ems, f1s = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "                \n",
    "            ctx = batch['context'].to(device)\n",
    "            qry = batch['question'].to(device)\n",
    "            start_gt = batch['answer_start'].to(device)\n",
    "            end_gt = batch['answer_end'].to(device)\n",
    "\n",
    "            s_logit, e_logit = model(ctx, qry)\n",
    "            e_logit = enforce_position_constraints(e_logit, start_gt)\n",
    "            \n",
    "            s_pred = s_logit.argmax(dim=1)\n",
    "            e_pred = e_logit.argmax(dim=1)\n",
    "            e_pred = torch.where(e_pred < s_pred, s_pred, e_pred)\n",
    "\n",
    "            for i in range(ctx.size(0)):\n",
    "                tok_ids = ctx[i].cpu().tolist()\n",
    "                tokens  = tokenizer.convert_ids_to_tokens(tok_ids)\n",
    "            \n",
    "                s_i = s_pred[i].item()\n",
    "                e_i = e_pred[i].item()\n",
    "            \n",
    "                pred = \" \".join(tokens[s_i : e_i+1]).strip()\n",
    "                gold = val_ds.samples.iloc[i]['answers']['text'][0].strip()\n",
    "            \n",
    "                ems.append(compute_em(pred, gold))\n",
    "                f1s.append(compute_f1(pred, gold))\n",
    "\n",
    "    mean_em = np.mean(ems) * 100\n",
    "    mean_f1 = np.mean(f1s) * 100\n",
    "    val_em_scores.append(mean_em)\n",
    "    val_f1_scores.append(mean_f1)\n",
    "    print(f\"Epoch {epoch} — Val EM: {mean_em:.2f}%, F1: {mean_f1:.2f}%\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt = os.path.join(MODEL_DIR, f\"{MODEL_NAME}_ep{epoch}.pt\")\n",
    "        torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'vocab': vocab}, ckpt)\n",
    "        print(f\"Checkpoint saved: {ckpt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:11:51.282108Z",
     "iopub.status.busy": "2025-04-18T19:11:51.281100Z",
     "iopub.status.idle": "2025-04-18T19:11:51.326692Z",
     "shell.execute_reply": "2025-04-18T19:11:51.325383Z",
     "shell.execute_reply.started": "2025-04-18T19:11:51.282065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics written to /kaggle/working/out/metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save metrics\n",
    "metrics = pd.DataFrame({\n",
    "    'epoch': range(1, NUM_EPOCHS+1),\n",
    "    'train_loss': train_losses,\n",
    "    'val_em': val_em_scores,\n",
    "    'val_f1': val_f1_scores\n",
    "})\n",
    "metrics.to_csv(os.path.join(OUT_DIR, 'metrics.csv'), index=False)\n",
    "print(f\"Metrics written to {os.path.join(OUT_DIR, 'metrics.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:11:52.828387Z",
     "iopub.status.busy": "2025-04-18T19:11:52.827780Z",
     "iopub.status.idle": "2025-04-18T19:13:31.846886Z",
     "shell.execute_reply": "2025-04-18T19:13:31.845758Z",
     "shell.execute_reply.started": "2025-04-18T19:11:52.828346Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/models.zip'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('models', 'zip', '/kaggle/working/model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:13:31.849119Z",
     "iopub.status.busy": "2025-04-18T19:13:31.848727Z",
     "iopub.status.idle": "2025-04-18T19:13:31.857180Z",
     "shell.execute_reply": "2025-04-18T19:13:31.855886Z",
     "shell.execute_reply.started": "2025-04-18T19:13:31.849078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='models.zip' target='_blank'>models.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/models.zip"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'models.zip')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7107330,
     "sourceId": 11356685,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7109338,
     "sourceId": 11359226,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 298640,
     "modelInstanceId": 277744,
     "sourceId": 331064,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
