{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T10:22:26.405451Z",
     "iopub.status.busy": "2025-04-11T10:22:26.405111Z",
     "iopub.status.idle": "2025-04-11T10:23:56.827859Z",
     "shell.execute_reply": "2025-04-11T10:23:56.827001Z",
     "shell.execute_reply.started": "2025-04-11T10:22:26.405426Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T07:00:03.460869Z",
     "iopub.status.busy": "2025-04-12T07:00:03.460351Z",
     "iopub.status.idle": "2025-04-12T07:00:09.769874Z",
     "shell.execute_reply": "2025-04-12T07:00:09.769110Z",
     "shell.execute_reply.started": "2025-04-12T07:00:03.460844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T07:00:13.417684Z",
     "iopub.status.busy": "2025-04-12T07:00:13.417245Z",
     "iopub.status.idle": "2025-04-12T07:00:13.684371Z",
     "shell.execute_reply": "2025-04-12T07:00:13.683563Z",
     "shell.execute_reply.started": "2025-04-12T07:00:13.417661Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T07:00:15.649978Z",
     "iopub.status.busy": "2025-04-12T07:00:15.649445Z",
     "iopub.status.idle": "2025-04-12T07:00:19.138395Z",
     "shell.execute_reply": "2025-04-12T07:00:19.137822Z",
     "shell.execute_reply.started": "2025-04-12T07:00:15.649953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d60c7f7e0704dc7b986bb4320ce4f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c106a22f735e4e45a3139f3556fe6b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0dff7cdc264ffbbb2e3ea617b8f37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98203b4aadd04d6aadaf2ea4269cc026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PAD_TOKEN = '[PAD]'\n",
    "UNK_TOKEN = '[UNK]'\n",
    "\n",
    "auto_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T07:09:58.425549Z",
     "iopub.status.busy": "2025-04-12T07:09:58.424953Z",
     "iopub.status.idle": "2025-04-12T07:09:58.443848Z",
     "shell.execute_reply": "2025-04-12T07:09:58.443013Z",
     "shell.execute_reply.started": "2025-04-12T07:09:58.425526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cuda(args, tensor):\n",
    "    \"\"\"\n",
    "    Places tensor on CUDA device (by default, uses cuda:0).\n",
    "    \n",
    "    Returns:\n",
    "        Tensor on CUDA device.\n",
    "    \"\"\"\n",
    "    if args.use_gpu and torch:\n",
    "        return tensor.cuda()\n",
    "    else:\n",
    "        return tensor\n",
    "\n",
    "def unpack(tensor):\n",
    "    \"\"\"\n",
    "    Unpacks a tensor into a Python list.\n",
    "\n",
    "    Args:\n",
    "        tensor: PyTorch tensor.\n",
    "\n",
    "    Returns:\n",
    "        Python list with tensor contents.\n",
    "    \"\"\"\n",
    "    if tensor.requires_grad:\n",
    "        tensor = tensor.detach()\n",
    "    return tensor.cpu().numpy().tolist()\n",
    "\n",
    "def load_embeddings(path):\n",
    "    \"\"\"\n",
    "    Loads GloVe-style embeddings into memory.\n",
    "    Args:\n",
    "        path: Embedding path, e.g. \"glove/glove.6B.300d.txt\".\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping words (strings) to vectors (list of floats).\n",
    "    \"\"\"\n",
    "    embedding_map = {}\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                pieces = line.rstrip().split()\n",
    "                word = pieces[0].lower()  # Normalize to lowercase\n",
    "                embedding_map[word] = [float(weight) for weight in pieces[1:]]\n",
    "                \n",
    "                # Also store lemma if different\n",
    "                lemma = lemmatizer.lemmatize(word)\n",
    "                if lemma != word and lemma not in embedding_map:\n",
    "                    embedding_map[lemma] = [float(weight) for weight in pieces[1:]]\n",
    "            except:\n",
    "                pass\n",
    "    return embedding_map\n",
    "\n",
    "def embed_batch(embedding_map, embedding_layer, batch_token_ids, idx2word, embed_dim):\n",
    "    \"\"\"\n",
    "    Iteratively converts a batch of token id sequences into their embeddings.\n",
    "\n",
    "    Args:\n",
    "        embedding_map (dict): Mapping from to embedding vectors.\n",
    "        batch_token_ids (List[List[int]]): Batch where each element is a list of token ids.\n",
    "        idx2word (dict): Mapping from token ID (int) to the corresponding word (str)\n",
    "        embed_dim (int): The dimensionality of the embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array of shape (batch_size, seq_len, embed_dim) containing the embeddings.\n",
    "    \"\"\"\n",
    "    batch_embeddings = []\n",
    "    \n",
    "    for token_ids in batch_token_ids:\n",
    "        sequence_embeddings = []\n",
    "        for token_id in token_ids:\n",
    "            # Retrieve the corresponding word for the token id.\n",
    "            token = idx2word.get(token_id.item(), None)\n",
    "            # print(\"Token\", token_id.item(), token)\n",
    "            if token is None and token not in embedding_map:\n",
    "                token_embedding = np.zeros(embed_dim)\n",
    "            else:\n",
    "                try:\n",
    "                    token_tensor = torch.tensor([token_id.item()], device=device)\n",
    "                    token_embedding = embedding_layer(token_tensor).squeeze(0).cpu().detach().numpy()\n",
    "                except Exception as e:\n",
    "                    print(f\"Token ID {token_id} caused error: {e}\")\n",
    "                    token_embedding = np.zeros(embed_dim)\n",
    "\n",
    "            sequence_embeddings.append(token_embedding)\n",
    "        batch_embeddings.append(sequence_embeddings)\n",
    "    return np.array(batch_embeddings)\n",
    "\n",
    "def co_attention(context_embedding, question_embedding, conv=True):\n",
    "    \"\"\"\n",
    "    Co-attention mechanism that computes attention between context and question encodings.\n",
    "    If `convolution=True`, applies local smoothing to the affinity matrix.\n",
    "\n",
    "    Args:\n",
    "        context_embedding (Tensor): (B, context_len, d)\n",
    "        question_embedding (Tensor): (B, question_len, d)\n",
    "        convolution (bool): whether to apply convolution-based smoothing.\n",
    "\n",
    "    Returns:\n",
    "        CP (Tensor): passage attention context\n",
    "        E_Out (Tensor): final encoder output\n",
    "    \"\"\"\n",
    "    # Step 1: Affinity matrix A ∈ (B, context_len, question_len)\n",
    "    A = torch.bmm(context_embedding, question_embedding.transpose(1, 2))\n",
    "    # print(\"context_embedding = \", context_embedding[0])\n",
    "    # print(\"question_embedding = \", question_embedding[0])\n",
    "    \n",
    "    # print(\"context_embedding:\", context_embedding)\n",
    "    # print(\"question_embedding:\", question_embedding)\n",
    "    # print(\"Affinity range:\", A.min().item(), A.max().item())\n",
    "\n",
    "    # Apply learned smoothing\n",
    "    if conv:\n",
    "        # A = conv_co_attention(A)\n",
    "        A = conv_co_attention(A)\n",
    "\n",
    "    # Step 2: Passage-to-question attention (row-wise)\n",
    "    A_P = F.softmax(A, dim=2)\n",
    "\n",
    "    # Step 3: Question-to-passage attention (column-wise)\n",
    "    A_Q = F.softmax(A.transpose(1, 2), dim=2)\n",
    "\n",
    "    # Step 4: Passage attention context: CP = H^P × A^Q\n",
    "    # print(\"Context Embedding Shape\", context_embedding.shape)\n",
    "    # print(\"Question Embedding Shape\", question_embedding.shape)\n",
    "    # print(\"A_Q Shape\", A_Q.shape)\n",
    "    CP = torch.bmm(A_Q, context_embedding)\n",
    "    # print(\"CP Shape\", CP.shape)  # (B, Lq, d)\n",
    "\n",
    "    # Step 5: Encoder output: concat(H^P, [H^Q; CP] × A^P)\n",
    "    # QC = torch.cat([question_embedding, CP], dim=1)\n",
    "    QC_1 = torch.bmm(A_P, question_embedding)  # (B, Lq, d)\n",
    "    # print(\"QC_1 Shape\", QC_1.shape)\n",
    "\n",
    "    QC_2 = torch.bmm(A_P, CP)  # (B, Lq, d)\n",
    "    # print(\"QC_2 Shape\", QC_2.shape)\n",
    "\n",
    "    # QC = torch.cat([QC_1, QC_2], dim=1) # (B, Lq, 2d)\n",
    "    QC = torch.cat([QC_1, QC_2], dim=-1)  # (B, Lq, 2d)\n",
    "    # print(\"QC Shape\", QC.shape)\n",
    "\n",
    "    # Final encoder output\n",
    "    # E_Out = torch.cat([context_embedding, QC], dim=2)\n",
    "    E_Out = torch.cat([context_embedding, QC], dim=-1)  # (B, Lq, 3d)\n",
    "    E_Out = nn.LayerNorm(E_Out.shape[-1]).to(E_Out.device)(E_Out)\n",
    "    E_Out = torch.tanh(E_Out)  # Apply non-linearity\n",
    "    # print(\"E_Out Shape\", E_Out.shape)\n",
    "\n",
    "    return CP, E_Out\n",
    "\n",
    "def create_gaussian_kernel(kernel_width, device, sigma=1.0):\n",
    "    \"\"\"Creates a 1D Gaussian kernel.\"\"\"\n",
    "    x = torch.arange(-kernel_width//2 + 1, kernel_width//2 + 1, dtype=torch.float, device=device)\n",
    "    kernel = torch.exp(-x**2 / (2*sigma**2))\n",
    "    kernel /= kernel.sum()  # Normalize to sum to 1\n",
    "    return kernel.view(1, 1, -1)\n",
    "\n",
    "def conv_co_attention(A, kernel_width=11):\n",
    "    \"\"\"\n",
    "    Enhanced convolution to shift attention to neighboring words.\n",
    "    Applies 1D convolution along context dimension per question word.\n",
    "    \"\"\"\n",
    "    B, Lp, Lq = A.shape\n",
    "    # Permute A for per-question-word processing: (B, Lq, Lp) -> (B*Lq, 1, Lp)\n",
    "    A_reshaped = A.permute(0, 2, 1).reshape(-1, 1, Lp)\n",
    "    \n",
    "    # Create Gaussian kernel with odd kernel width (e.g., 11)\n",
    "    kernel = create_gaussian_kernel(kernel_width, A.device, sigma=1.0)\n",
    "    \n",
    "    # Use symmetric padding that keeps the sequence length unchanged.\n",
    "    padded_length = (kernel_width - 1) // 2\n",
    "    smoothed_A = F.conv1d(A_reshaped, kernel, padding=padded_length)\n",
    "    \n",
    "    # Reshape back: current shape is (B*Lq, 1, Lp) --> (B, Lq, Lp) then permute to (B, Lp, Lq)\n",
    "    smoothed_A = smoothed_A.view(B, Lq, Lp).permute(0, 2, 1)\n",
    "    A_adjusted = A + smoothed_A  # Enhance original scores with neighbor context\n",
    "    return F.softmax(A_adjusted, dim=-1)\n",
    "\n",
    "def tokenize_with_bert(text):\n",
    "    # Tokenize the text and request offset mappings.\n",
    "    encoding = auto_tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        add_special_tokens=False  # Disable adding special tokens to mimic simple whitespace tokenization.\n",
    "    )\n",
    "    \n",
    "    # Retrieve the tokens.\n",
    "    tokens = auto_tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "    \n",
    "    # Retrieve the spans from the offset mapping.\n",
    "    spans = encoding['offset_mapping']\n",
    "    return tokens, spans\n",
    "    \n",
    "def create_embedding_matrix(vocab, embedding_map, embedding_dim=300, scale=0.6):\n",
    "    \"\"\"Initialize embedding matrix with:\n",
    "    - GloVe vectors for known words\n",
    "    - Random vectors for UNK tokens\n",
    "    - Zero vector for padding\n",
    "    \"\"\"\n",
    "    # Initialize with random normal distribution (match GloVe scale)\n",
    "    embedding_matrix = np.random.normal(\n",
    "        scale=scale, \n",
    "        size=(len(vocab), embedding_dim)\n",
    "    )\n",
    "    \n",
    "    # Handle special tokens\n",
    "    embedding_matrix[vocab.encoding[PAD_TOKEN]] = np.zeros(embedding_dim)\n",
    "    unk_idx = vocab.encoding[UNK_TOKEN]\n",
    "    embedding_matrix[unk_idx] = np.random.normal(scale=scale, size=embedding_dim)\n",
    "    \n",
    "    for word, idx in vocab.encoding.items():\n",
    "        if word in [PAD_TOKEN, UNK_TOKEN]:\n",
    "            continue\n",
    "            \n",
    "        # Try direct match\n",
    "        if word in embedding_map:\n",
    "            embedding_matrix[idx] = embedding_map[word]\n",
    "            continue\n",
    "            \n",
    "        # Try lemma\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if lemma in embedding_map:\n",
    "            embedding_matrix[idx] = embedding_map[lemma]\n",
    "            continue\n",
    "            \n",
    "        # Try lowercase lemma\n",
    "        lower_lemma = lemmatizer.lemmatize(word.lower())\n",
    "        if lower_lemma in embedding_map:\n",
    "            embedding_matrix[idx] = embedding_map[lower_lemma]\n",
    "\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T07:05:45.096565Z",
     "iopub.status.busy": "2025-04-12T07:05:45.096290Z",
     "iopub.status.idle": "2025-04-12T07:05:45.111644Z",
     "shell.execute_reply": "2025-04-12T07:05:45.111016Z",
     "shell.execute_reply.started": "2025-04-12T07:05:45.096544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Creates mappings for words → indices and indices → words.\n",
    "    \"\"\"\n",
    "    def __init__(self, samples, vocab_size):\n",
    "        self.samples = samples\n",
    "        self.vocab_size = vocab_size\n",
    "        self.words = self._initialize(samples, vocab_size)\n",
    "        self.encoding = {word: idx for idx, word in enumerate(self.words)}\n",
    "        self.decoding = {idx: word for idx, word in enumerate(self.words)}\n",
    "\n",
    "    def _initialize(self, samples, vocab_size):\n",
    "        \"\"\"Build vocabulary with lemma support\"\"\"\n",
    "        embedding_map = load_embeddings(\"/kaggle/input/glove/other/default/1/glove.6B.300d.txt\")\n",
    "        vocab_counts = collections.defaultdict(int)\n",
    "        \n",
    "        for _, row in samples.iterrows():\n",
    "            # Get base tokens\n",
    "            tokens = re.findall(r\"\\w+(?:[-']\\w+)*\", row['context'].lower()) + \\\n",
    "                     re.findall(r\"\\w+(?:[-']\\w+)*\", row['question'].lower())\n",
    "            \n",
    "            # Count both original and lemma forms\n",
    "            for token in tokens:\n",
    "                vocab_counts[token] += 1\n",
    "                lemma = lemmatizer.lemmatize(token)\n",
    "                if lemma != token:\n",
    "                    vocab_counts[lemma] += 0.5  # Partial count for lemmas\n",
    "        \n",
    "        # Sort by combined frequency\n",
    "        sorted_words = sorted(vocab_counts.items(), \n",
    "                            key=lambda x: (-x[1], x[0]))[:vocab_size-2]\n",
    "        \n",
    "        return [PAD_TOKEN, UNK_TOKEN] + [w[0] for w in sorted_words]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Converts lists of words to indices and vice versa.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.pad_token_id = vocabulary.encoding[PAD_TOKEN]\n",
    "        self.unk_token_id = vocabulary.encoding[UNK_TOKEN]\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.vocabulary.encoding.get(token.lower(), self.unk_token_id) for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        return [self.vocabulary.decoding.get(token_id, UNK_TOKEN) for token_id in token_ids]\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Data generator for a QA task; the JSON file should contain character-level answer indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        # Load JSON-lines file; each line is a JSON object.\n",
    "        self.samples = pd.read_json(path, lines=True)\n",
    "        self.tokenizer = None\n",
    "        # Default pad token id; updated after tokenizer registration.\n",
    "        self.pad_token_id = 0\n",
    "\n",
    "    def _collate_batch(self, batch):\n",
    "        batch = [sample for sample in batch if sample is not None]\n",
    "        if len(batch) == 0:\n",
    "            return None  # All samples failed\n",
    "    \n",
    "        max_context_len = max(sample['context'].size(0) for sample in batch)\n",
    "        max_question_len = max(sample['question'].size(0) for sample in batch)\n",
    "    \n",
    "        contexts = torch.stack([\n",
    "            torch.cat([\n",
    "                sample['context'],\n",
    "                torch.full((max_context_len - sample['context'].size(0),), self.pad_token_id, dtype=torch.long)\n",
    "            ]) for sample in batch\n",
    "        ])\n",
    "    \n",
    "        questions = torch.stack([\n",
    "            torch.cat([\n",
    "                sample['question'],\n",
    "                torch.full((max_question_len - sample['question'].size(0),), self.pad_token_id, dtype=torch.long)\n",
    "            ]) for sample in batch\n",
    "        ])\n",
    "    \n",
    "        answer_starts = torch.stack([sample['answer_start'] for sample in batch])\n",
    "        answer_ends = torch.stack([sample['answer_end'] for sample in batch])\n",
    "    \n",
    "        return {\n",
    "            'context': contexts,\n",
    "            'question': questions,\n",
    "            'answer_start': answer_starts,\n",
    "            'answer_end': answer_ends\n",
    "        }\n",
    "\n",
    "    def register_tokenizer(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Registers a Tokenizer instance and updates pad token id.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples.iloc[idx]\n",
    "        context_str = sample['context']\n",
    "        question_str = sample['question']\n",
    "        answers = sample['answers']\n",
    "    \n",
    "        context_tokens, context_spans = tokenize_with_bert(context_str)\n",
    "        question_tokens, _ = tokenize_with_bert(question_str)\n",
    "        context_indices = self.tokenizer.convert_tokens_to_ids(context_tokens)\n",
    "        question_indices = self.tokenizer.convert_tokens_to_ids(question_tokens)\n",
    "    \n",
    "        answer_text = answers['text'][0].strip()\n",
    "        answer_tokens, _ = tokenize_with_bert(answer_text)\n",
    "    \n",
    "        context_tokens_lower = [t.lower() for t in context_tokens]\n",
    "        answer_tokens_lower = [t.lower() for t in answer_tokens]\n",
    "    \n",
    "        token_start, token_end = -1, -1\n",
    "        for i in range(len(context_tokens_lower) - len(answer_tokens_lower) + 1):\n",
    "            if context_tokens_lower[i:i+len(answer_tokens_lower)] == answer_tokens_lower:\n",
    "                token_start = i\n",
    "                token_end = i + len(answer_tokens_lower) - 1\n",
    "                break\n",
    "    \n",
    "        if token_start == -1 or token_end == -1:\n",
    "            # skip\n",
    "            return None\n",
    "    \n",
    "        return {\n",
    "            'context': torch.tensor(context_indices, dtype=torch.long),\n",
    "            'question': torch.tensor(question_indices, dtype=torch.long),\n",
    "            'answer_start': torch.tensor(token_start, dtype=torch.long),\n",
    "            'answer_end': torch.tensor(token_end, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T07:10:12.779234Z",
     "iopub.status.busy": "2025-04-12T07:10:12.778540Z",
     "iopub.status.idle": "2025-04-12T07:10:12.793027Z",
     "shell.execute_reply": "2025-04-12T07:10:12.792215Z",
     "shell.execute_reply.started": "2025-04-12T07:10:12.779210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.3):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Create a bidirectional LSTM layer; note batch_first=True keeps tensors as (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x, return_sequence=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_size).\n",
    "            return_sequence: If True, return the entire sequence; if False, return the last hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Run the input sequence through the LSTM layer\n",
    "        output, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply dropout to the output of the LSTM layer\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # Return the entire sequence\n",
    "        return output # Shape: (batch_size, seq_len, hidden_size * 2)\n",
    "\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(self, vocab_size, vocab_decoder, embedding_dim, hidden_size, num_layers, output_size=300, dropout_prob=0.3):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab_decoder = vocab_decoder\n",
    "\n",
    "        # Embedding map using pre-trained GloVe embeddings\n",
    "        self.embedding_map = load_embeddings(\"/kaggle/input/glove/other/default/1/glove.6B.300d.txt\")\n",
    "\n",
    "        # Initialize Embedding layer\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings = len(vocab),\n",
    "            embedding_dim = embedding_dim,\n",
    "            padding_idx = vocab.encoding[PAD_TOKEN]\n",
    "        )\n",
    "\n",
    "        # Load pretrained weights\n",
    "        embedding_matrix = create_embedding_matrix(vocab, self.embedding_map)\n",
    "        self.embedding_layer.weight.data.copy_(embedding_matrix)\n",
    "\n",
    "        # Freeze only GloVe vectors\n",
    "        for i, word in enumerate(vocab.words):\n",
    "            if word in self.embedding_map and word != UNK_TOKEN:\n",
    "                self.embedding_layer.weight.requires_grad_(False)\n",
    "\n",
    "        # Context Modeling\n",
    "        self.start_decoder = BiLSTMModel(embedding_dim * 3, hidden_size, num_layers, output_size=output_size)\n",
    "        self.end_decoder = BiLSTMModel(embedding_dim * 6, hidden_size, num_layers, output_size)\n",
    "\n",
    "        # Prediction Layers - two linear layers for start and end index predictions.\n",
    "        self.start_linear = nn.Linear(3 * embedding_dim + 2 * hidden_size, 1)\n",
    "        self.end_linear = nn.Linear(3 * embedding_dim + 2 * hidden_size, 1)\n",
    "    \n",
    "    def forward(self, context_ids, question_ids):\n",
    "        \"\"\"\n",
    "        Forward pass that includes contextual encoding.\n",
    "\n",
    "        Args:\n",
    "            context_ids (Tensor): shape (batch, context_len)\n",
    "            question_ids (Tensor): shape (batch, question_len)\n",
    "\n",
    "        Returns:\n",
    "            start_logits: Tensor of shape (batch, context_len)\n",
    "            end_logits: Tensor of question word IDs.\n",
    "            affinity: (batch, context_len, question_len)\n",
    "        \"\"\"\n",
    "        ### Word Embedding\n",
    "        context_emb_np = embed_batch(embedding_map=self.embedding_map, embedding_layer=self.embedding_layer, batch_token_ids=context_ids, idx2word=self.vocab_decoder, embed_dim=300)\n",
    "        question_emb_np = embed_batch(embedding_map=self.embedding_map, embedding_layer=self.embedding_layer, batch_token_ids=question_ids, idx2word=self.vocab_decoder, embed_dim=300)\n",
    "\n",
    "        # Convert numpy arrays to torch tensors (and ensure they are float type).\n",
    "        context_emb = torch.from_numpy(context_emb_np).float().contiguous().to(device)\n",
    "        question_emb = torch.from_numpy(question_emb_np).float().contiguous().to(device)\n",
    "\n",
    "        # --- Encoder: Contextual Embedding via CoAttention ---\n",
    "        # passage_attention_context, encoder_out = co_attention(context_emb, question_emb, True)\n",
    "        passage_attention_context, encoder_out = co_attention(\n",
    "            context_emb, question_emb, conv=True\n",
    "        )\n",
    "        # encoder_out shape: (B, L, 3*embedding_dim)\n",
    "\n",
    "        # --- Decoder for the Start Index Prediction ---\n",
    "        start_decoded = self.start_decoder(encoder_out)\n",
    "        # start_decoded shape: (B, L, 2*hidden_size)\n",
    "\n",
    "        # Concatenate encoder output with the decoded representation\n",
    "        start_input = torch.cat([encoder_out, start_decoded], dim=-1)  # (B, L, 3*embedding_dim + 2*hidden_size)\n",
    "        start_logits = self.start_linear(start_input).squeeze(-1)\n",
    "        start_probs = F.softmax(start_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        # Compute weighted summary using start_probs\n",
    "        start_summary = torch.sum(encoder_out * start_probs, dim=1, keepdim=True)\n",
    "        start_summary_expanded = start_summary.repeat(1, encoder_out.size(1), 1)  # (B, L, 3*embedding_dim)\n",
    "\n",
    "        # --- Prepare Features for End Index Prediction ---\n",
    "        combined_for_end = torch.cat([encoder_out, start_summary_expanded], dim=-1)  # (B, L, 6*embedding_dim)\n",
    "        \n",
    "        # --- Decoder for the End Index Prediction ---\n",
    "        end_decoded = self.end_decoder(combined_for_end)\n",
    "        \n",
    "        # Concatenate encoder output with the end decoded representation\n",
    "        # (Note: ensure arguments to torch.cat are provided as a list)\n",
    "        end_input = torch.cat([encoder_out, end_decoded], dim=-1)  # (B, L, 3*embedding_dim + 2*hidden_size)\n",
    "        end_logits = self.end_linear(end_input).squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T07:05:54.618436Z",
     "iopub.status.busy": "2025-04-12T07:05:54.617919Z",
     "iopub.status.idle": "2025-04-12T07:08:01.862022Z",
     "shell.execute_reply": "2025-04-12T07:08:01.861483Z",
     "shell.execute_reply.started": "2025-04-12T07:05:54.618412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_dataset = QADataset(path=\"/kaggle/input/squad-v2/train.json\")\n",
    "\n",
    "# Instantiate the vocabulary and tokenizer\n",
    "vocab = Vocabulary(train_dataset.samples, vocab_size=200000)\n",
    "tokenizer = Tokenizer(vocab)\n",
    "train_dataset.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T07:08:44.711554Z",
     "iopub.status.busy": "2025-04-12T07:08:44.710969Z",
     "iopub.status.idle": "2025-04-12T07:08:44.991796Z",
     "shell.execute_reply": "2025-04-12T07:08:44.991127Z",
     "shell.execute_reply.started": "2025-04-12T07:08:44.711532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 17822\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Make a shallow copy of the dataset object\n",
    "filtered_train_dataset = copy.deepcopy(train_dataset)\n",
    "\n",
    "# Replace only the samples with the filtered ones\n",
    "filtered_train_dataset.samples = train_dataset.samples[\n",
    "    train_dataset.samples['context'].apply(lambda x: len(x) <= 400)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(\"Number of samples:\", len(filtered_train_dataset.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T07:10:17.710596Z",
     "iopub.status.busy": "2025-04-12T07:10:17.710325Z",
     "iopub.status.idle": "2025-04-12T08:18:15.244912Z",
     "shell.execute_reply": "2025-04-12T08:18:15.244202Z",
     "shell.execute_reply.started": "2025-04-12T07:10:17.710576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "Tesla P100-PCIE-16GB\n",
      "Using device: cuda\n",
      "Training Epoch 1/15...\n",
      "Epoch [1/15], Loss: 3.6013\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 2/15...\n",
      "Epoch [2/15], Loss: 3.1754\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 3/15...\n",
      "Epoch [3/15], Loss: 2.8634\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 4/15...\n",
      "Epoch [4/15], Loss: 2.5626\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 5/15...\n",
      "Epoch [5/15], Loss: 2.2613\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Checkpoint saved at /kaggle/working/model/qa_model_epoch_5.pt\n",
      "Training Epoch 6/15...\n",
      "Epoch [6/15], Loss: 1.9481\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 7/15...\n",
      "Epoch [7/15], Loss: 1.7025\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 8/15...\n",
      "Epoch [8/15], Loss: 1.4443\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 9/15...\n",
      "Epoch [9/15], Loss: 1.2351\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 10/15...\n",
      "Epoch [10/15], Loss: 1.0648\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Checkpoint saved at /kaggle/working/model/qa_model_epoch_10.pt\n",
      "Training Epoch 11/15...\n",
      "Epoch [11/15], Loss: 0.9152\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 12/15...\n",
      "Epoch [12/15], Loss: 0.8004\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 13/15...\n",
      "Epoch [13/15], Loss: 0.7112\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 14/15...\n",
      "Epoch [14/15], Loss: 0.6222\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Training Epoch 15/15...\n",
      "Epoch [15/15], Loss: 0.5481\n",
      "✅ Valid training samples this epoch: 17722\n",
      "Checkpoint saved at /kaggle/working/model/qa_model_epoch_15.pt\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available())        # Should return True\n",
    "print(torch.cuda.device_count())        # Should be > 0\n",
    "print(torch.cuda.get_device_name(0))    # Should show your NVIDIA GPU\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 200000\n",
    "embedding_dim = 300         # Embedding size for word embeddings\n",
    "hidden_size = 128           # Hidden size for LSTM\n",
    "num_layers = 1              # Number of LSTM layers\n",
    "output_size = 400           # Not used in our prediction layers for QA; prediction layers output logits per token\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(filtered_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset._collate_batch)\n",
    "\n",
    "# Initialize the model\n",
    "model = QAModel(vocab_size=vocab_size,\n",
    "                vocab_decoder=vocab.decoding,\n",
    "                embedding_dim=embedding_dim,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                output_size=output_size).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Checkpoint directory (modify as needed)\n",
    "checkpoint_dir = \"/kaggle/working/model/\"\n",
    "import os\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    valid_sample_count = 0\n",
    "\n",
    "    print(f\"Training Epoch {epoch + 1}/{num_epochs}...\")\n",
    "\n",
    "    # Iterate over the DataLoader\n",
    "    for batch in train_loader:\n",
    "        if batch is None:\n",
    "            continue  # skip batch if all samples failed\n",
    "                    \n",
    "        batch_size = batch['context'].size(0)\n",
    "        valid_sample_count += batch_size  # <-- Count valid samples\n",
    "            \n",
    "        # Unpack the batch\n",
    "        contexts = batch['context'].to(device)        # shape: (batch_size, context_len)\n",
    "        questions = batch['question'].to(device)       # shape: (batch_size, question_len)\n",
    "        start_positions = batch['answer_start'].to(device)  # shape: (batch_size,)\n",
    "        end_positions = batch['answer_end'].to(device)      # shape: (batch_size,)\n",
    "\n",
    "        # Forward pass: model returns start_logits, end_logits\n",
    "        start_logits, end_logits = model(contexts, questions)\n",
    "\n",
    "        def enforce_position_constraints(end_logits, start_positions):\n",
    "            \"\"\"\n",
    "            Mask positions in end_logits that are before the corresponding start positions.\n",
    "            \n",
    "            Args:\n",
    "                end_logits (Tensor): shape (batch_size, seq_len)\n",
    "                start_positions (Tensor): shape (batch_size,)\n",
    "                \n",
    "            Returns:\n",
    "                Tensor of the same shape as end_logits with positions before the start masked to -inf.\n",
    "            \"\"\"\n",
    "            # Ensure end_probs is 2D (batch_size, seq_len)\n",
    "            if end_logits.dim() == 3:\n",
    "                end_logits = end_logits.squeeze(-1)\n",
    "                \n",
    "            batch_size, seq_len = end_logits.shape\n",
    "            \n",
    "            # Create a tensor of position indices for each sequence\n",
    "            positions = torch.arange(seq_len, device=end_logits.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "            \n",
    "            # Create a mask where positions are before the corresponding start position\n",
    "            mask = positions < start_positions.unsqueeze(1)\n",
    "            \n",
    "            # Mask out positions by setting them to -inf\n",
    "            end_logits = end_logits.masked_fill(mask, -float('inf'))\n",
    "            \n",
    "            return end_logits\n",
    "            \n",
    "        end_logits = enforce_position_constraints(end_logits, start_positions)\n",
    "        \n",
    "        # Compute loss for start and end positions\n",
    "        start_loss = F.cross_entropy(start_logits, start_positions, reduction='none')\n",
    "        end_loss = F.cross_entropy(end_logits, end_positions, reduction='none')\n",
    "        ce_loss = 0.7 * start_loss.mean() + 0.3 * end_loss.mean()\n",
    "\n",
    "        # Span length regularization\n",
    "        pred_lengths = end_logits.argmax(-1) - start_logits.argmax(-1)\n",
    "        length_loss = F.relu(1 - pred_lengths.float()).mean()  # Penalize invalid spans\n",
    "\n",
    "        loss = ce_loss + 0.1 * length_loss\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # Additional Text Loss based on predicted and ground truth answer spans.\n",
    "        # -------------------------------------------------------------------------\n",
    "        \n",
    "        # Retrieve the context token embeddings using the model's embedding layer.\n",
    "        # Assumes model.embedding is the embedding layer and returns (batch_size, seq_len, embedding_dim)\n",
    "        context_embeddings = model.embedding_layer(contexts)\n",
    "        \n",
    "        # Determine predicted answer spans from model outputs.\n",
    "        pred_start_indices = start_logits.argmax(dim=1)  # shape: (batch_size,)\n",
    "        pred_end_indices   = end_logits.argmax(dim=1)      # shape: (batch_size,)\n",
    "    \n",
    "        predicted_embeddings = []\n",
    "        gt_embeddings = []\n",
    "\n",
    "        for i in range(contexts.size(0)):\n",
    "            # ---------------------------\n",
    "            # Predicted answer embedding\n",
    "            # ---------------------------\n",
    "            pred_s = pred_start_indices[i].item()\n",
    "            pred_e = pred_end_indices[i].item()\n",
    "            \n",
    "            # Ensure a valid span; if not, fallback to a single token.\n",
    "            if pred_e < pred_s:\n",
    "                pred_e = pred_s\n",
    "                \n",
    "            pred_span_embeds = context_embeddings[i, pred_s:pred_e+1, :]  # (span_length, embedding_dim)\n",
    "            \n",
    "            if pred_span_embeds.size(0) > 0:\n",
    "                pred_emb = pred_span_embeds.mean(dim=0)\n",
    "            else:\n",
    "                pred_emb = context_embeddings[i, pred_s, :]\n",
    "                \n",
    "            predicted_embeddings.append(pred_emb)\n",
    "            \n",
    "            # -------------------------------------\n",
    "            # Ground truth answer embedding\n",
    "            # -------------------------------------\n",
    "            # Use provided start_positions and end_positions (which are word indices)\n",
    "            gt_s = start_positions[i].item()\n",
    "            gt_e = end_positions[i].item()\n",
    "            \n",
    "            if gt_e < gt_s:\n",
    "                gt_e = gt_s\n",
    "                \n",
    "            gt_span_embeds = context_embeddings[i, gt_s:gt_e+1, :]  # (span_length, embedding_dim)\n",
    "            \n",
    "            if gt_span_embeds.size(0) > 0:\n",
    "                gt_emb = gt_span_embeds.mean(dim=0)\n",
    "            else:\n",
    "                gt_emb = context_embeddings[i, gt_s, :]\n",
    "                \n",
    "            gt_embeddings.append(gt_emb)\n",
    "        \n",
    "        predicted_embeddings = torch.stack(predicted_embeddings)  # (batch_size, embedding_dim)\n",
    "        gt_embeddings = torch.stack(gt_embeddings)                # (batch_size, embedding_dim)\n",
    "        \n",
    "        # Compute cosine similarity between predicted and ground truth embeddings.\n",
    "        cos_sim = F.cosine_similarity(predicted_embeddings, gt_embeddings, dim=1)\n",
    "        \n",
    "        # Define text loss as 1 minus the cosine similarity (perfect match gives 0 loss).\n",
    "        text_loss = (1 - cos_sim).mean()\n",
    "        \n",
    "        # Weight the text loss and add it to the overall loss.\n",
    "        alpha = 0.1  # Adjust weighting factor as needed.\n",
    "        loss = loss + alpha * text_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        max_grad_norm = 1.0\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm, norm_type=2)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    print(f\"✅ Valid training samples this epoch: {valid_sample_count}\")\n",
    "\n",
    "    # Save a checkpoint every 5 epochs (or customize as needed)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"qa_model_epoch_{epoch+1}.pt\")\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'vocab': vocab\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:18:50.732281Z",
     "iopub.status.busy": "2025-04-12T08:18:50.731998Z",
     "iopub.status.idle": "2025-04-12T08:19:32.248766Z",
     "shell.execute_reply": "2025-04-12T08:19:32.248048Z",
     "shell.execute_reply.started": "2025-04-12T08:18:50.732264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/models.zip'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('models', 'zip', '/kaggle/working/model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:19:37.029185Z",
     "iopub.status.busy": "2025-04-12T08:19:37.028468Z",
     "iopub.status.idle": "2025-04-12T08:19:37.034048Z",
     "shell.execute_reply": "2025-04-12T08:19:37.033418Z",
     "shell.execute_reply.started": "2025-04-12T08:19:37.029163Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='models.zip' target='_blank'>models.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/models.zip"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'models.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:30:39.755761Z",
     "iopub.status.busy": "2025-04-12T08:30:39.755477Z",
     "iopub.status.idle": "2025-04-12T08:30:39.760966Z",
     "shell.execute_reply": "2025-04-12T08:30:39.760349Z",
     "shell.execute_reply.started": "2025-04-12T08:30:39.755740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Utility functions ---\n",
    "def compute_em(predicted, actual):\n",
    "    return int(predicted.strip().lower() == actual.strip().lower())\n",
    "\n",
    "def compute_f1(predicted, actual):\n",
    "    pred_tokens = predicted.strip().lower().split()\n",
    "    actual_tokens = actual.strip().lower().split()\n",
    "    \n",
    "    common = set(pred_tokens) & set(actual_tokens)\n",
    "    if not common:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(actual_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:43:51.367303Z",
     "iopub.status.busy": "2025-04-12T08:43:51.367006Z",
     "iopub.status.idle": "2025-04-12T08:46:14.006288Z",
     "shell.execute_reply": "2025-04-12T08:46:14.005488Z",
     "shell.execute_reply.started": "2025-04-12T08:43:51.367282Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3961438651.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sample_checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoints[-1]), map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating qa_model_epoch_10.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3961438651.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(checkpoint_dir, ckpt_file), map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ EM: 6.12%, F1: 14.75%\n",
      "✅ Valid testing samples this epoch: 735\n",
      "\n",
      "Evaluating qa_model_epoch_15.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3961438651.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(checkpoint_dir, ckpt_file), map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ EM: 6.53%, F1: 13.95%\n",
      "✅ Valid testing samples this epoch: 735\n",
      "\n",
      "Evaluating qa_model_epoch_5.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3961438651.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(checkpoint_dir, ckpt_file), map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ EM: 6.26%, F1: 13.91%\n",
      "✅ Valid testing samples this epoch: 735\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Checkpoint directory (modify as needed)\n",
    "checkpoint_dir = \"/kaggle/working/out/\"\n",
    "import os\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = QADataset(path=\"/kaggle/input/validation/validation.json\")\n",
    "checkpoint_dir = \"/kaggle/working/model/\"\n",
    "checkpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith(\".pt\")])\n",
    "\n",
    "# Prepare validation vocab from any one checkpoint (they all use same vocab)\n",
    "sample_checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoints[-1]), map_location=device)\n",
    "val_vocab = sample_checkpoint['vocab']\n",
    "val_tokenizer = Tokenizer(val_vocab)\n",
    "val_dataset.tokenizer = val_tokenizer\n",
    "\n",
    "# Filter short contexts only\n",
    "val_dataset.samples = val_dataset.samples[val_dataset.samples['context'].apply(lambda x: len(x) <= 400)].reset_index(drop=True)\n",
    "\n",
    "# Shared hyperparameters\n",
    "vocab_size = 200000\n",
    "embedding_dim = 300\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "output_size = 400\n",
    "\n",
    "# Output summary\n",
    "summary = []\n",
    "\n",
    "# === Loop Over Checkpoints ===\n",
    "for ckpt_file in checkpoints:\n",
    "    print(f\"\\nEvaluating {ckpt_file}...\")\n",
    "\n",
    "    # Load model\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_dir, ckpt_file), map_location=device)\n",
    "    model = QAModel(\n",
    "        vocab_size=vocab_size,\n",
    "        vocab_decoder=val_vocab.decoding,\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        output_size=output_size\n",
    "    ).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    em_scores = []\n",
    "    f1_scores = []\n",
    "    output_data = []\n",
    "\n",
    "    valid_sample_count = 0\n",
    "    for i in range(len(val_dataset)):\n",
    "        sample = val_dataset[i]\n",
    "        \n",
    "        if sample is None:\n",
    "            continue\n",
    "        \n",
    "        row = val_dataset.samples.iloc[i]\n",
    "        valid_sample_count = valid_sample_count + 1\n",
    "\n",
    "        context_text = row['context']\n",
    "        question_text = row['question']\n",
    "        answer_text = row['answers']['text'][0].strip()\n",
    "        answer_start = row['answers']['answer_start'][0]\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "\n",
    "        context_tensor = sample['context'].unsqueeze(0).to(device)\n",
    "        question_tensor = sample['question'].unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            start_logits, end_logits = model(context_tensor, question_tensor)\n",
    "            start_idx = torch.argmax(start_logits, dim=1).item()\n",
    "            end_idx = torch.argmax(end_logits, dim=1).item()\n",
    "            if end_idx < start_idx:\n",
    "                end_idx = start_idx\n",
    "\n",
    "        context_tokens = val_tokenizer.convert_ids_to_tokens(sample['context'].tolist())\n",
    "        predicted_answer = \" \".join(context_tokens[start_idx:end_idx + 1]).strip()\n",
    "\n",
    "        em = compute_em(predicted_answer, answer_text)\n",
    "        f1 = compute_f1(predicted_answer, answer_text)\n",
    "\n",
    "        em_scores.append(em)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        output_data.append({\n",
    "            \"context\": context_text,\n",
    "            \"question\": question_text,\n",
    "            \"answer\": answer_text,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"answer_start_index\": answer_start,\n",
    "            \"answer_end_index\": answer_end,\n",
    "            \"predicted_start_index\": start_idx,\n",
    "            \"predicted_end_index\": end_idx\n",
    "        })\n",
    "\n",
    "    # Save per-checkpoint results\n",
    "    epoch_num = ckpt_file.split(\"_\")[-1].replace(\".pt\", \"\")\n",
    "    df_output = pd.DataFrame(output_data)\n",
    "    df_output.to_csv(f\"/kaggle/working/out/validation_output_epoch_{epoch_num}.csv\", index=False)\n",
    "\n",
    "    # Log summary\n",
    "    mean_em = np.mean(em_scores) * 100\n",
    "    mean_f1 = np.mean(f1_scores) * 100\n",
    "    summary.append({\n",
    "        \"checkpoint\": ckpt_file,\n",
    "        \"EM\": mean_em,\n",
    "        \"F1\": mean_f1\n",
    "    })\n",
    "\n",
    "    print(f\"→ EM: {mean_em:.2f}%, F1: {mean_f1:.2f}%\")\n",
    "    print(f\"✅ Valid testing samples this epoch: {valid_sample_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:46:29.215729Z",
     "iopub.status.busy": "2025-04-12T08:46:29.215170Z",
     "iopub.status.idle": "2025-04-12T08:46:29.221668Z",
     "shell.execute_reply": "2025-04-12T08:46:29.221129Z",
     "shell.execute_reply.started": "2025-04-12T08:46:29.215707Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation complete! Results saved to:\n",
      "- /kaggle/working/out/eval_summary.csv (summary)\n",
      "- /kaggle/working/out/validation_output_epoch_*.csv (detailed per checkpoint)\n"
     ]
    }
   ],
   "source": [
    "# Save summary CSV\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df.to_csv(\"/kaggle/working/out/eval_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Evaluation complete! Results saved to:\")\n",
    "print(\"- /kaggle/working/out/eval_summary.csv (summary)\")\n",
    "print(\"- /kaggle/working/out/validation_output_epoch_*.csv (detailed per checkpoint)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:46:32.149166Z",
     "iopub.status.busy": "2025-04-12T08:46:32.148870Z",
     "iopub.status.idle": "2025-04-12T08:46:32.177980Z",
     "shell.execute_reply": "2025-04-12T08:46:32.177443Z",
     "shell.execute_reply.started": "2025-04-12T08:46:32.149146Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/out.zip'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('out', 'zip', '/kaggle/working/out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:46:33.496697Z",
     "iopub.status.busy": "2025-04-12T08:46:33.495985Z",
     "iopub.status.idle": "2025-04-12T08:46:33.501261Z",
     "shell.execute_reply": "2025-04-12T08:46:33.500582Z",
     "shell.execute_reply.started": "2025-04-12T08:46:33.496667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='out.zip' target='_blank'>out.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/out.zip"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'out.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T08:47:08.566608Z",
     "iopub.status.busy": "2025-04-12T08:47:08.565951Z",
     "iopub.status.idle": "2025-04-12T08:47:08.583668Z",
     "shell.execute_reply": "2025-04-12T08:47:08.583031Z",
     "shell.execute_reply.started": "2025-04-12T08:47:08.566585Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checkpoint</th>\n",
       "      <th>EM</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qa_model_epoch_10.pt</td>\n",
       "      <td>6.122449</td>\n",
       "      <td>14.748026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa_model_epoch_15.pt</td>\n",
       "      <td>6.530612</td>\n",
       "      <td>13.952793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qa_model_epoch_5.pt</td>\n",
       "      <td>6.258503</td>\n",
       "      <td>13.914011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             checkpoint        EM         F1\n",
       "0  qa_model_epoch_10.pt  6.122449  14.748026\n",
       "1  qa_model_epoch_15.pt  6.530612  13.952793\n",
       "2   qa_model_epoch_5.pt  6.258503  13.914011"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7107330,
     "sourceId": 11356685,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7109338,
     "sourceId": 11359226,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 298640,
     "modelInstanceId": 277744,
     "sourceId": 331064,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
