{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11356685,"sourceType":"datasetVersion","datasetId":7107330},{"sourceId":11359226,"sourceType":"datasetVersion","datasetId":7109338},{"sourceId":331064,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":277744,"modelId":298640}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install torch\n!pip install numpy\n!pip install pandas\n!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T10:12:21.651350Z","iopub.execute_input":"2025-04-17T10:12:21.651952Z","iopub.status.idle":"2025-04-17T10:12:31.778446Z"}},"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m^C\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip3\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 78, in main\n    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/__init__.py\", line 114, in create_command\n    module = importlib.import_module(module_path)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 15, in <module>\n    from pip._internal.cli.req_command import (\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 18, in <module>\n    from pip._internal.index.collector import LinkCollector\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/index/collector.py\", line 31, in <module>\n    from pip._vendor import requests\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/requests/__init__.py\", line 159, in <module>\n    from .api import delete, get, head, options, patch, post, put, request\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/requests/api.py\", line 11, in <module>\n    from . import sessions\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/requests/sessions.py\", line 15, in <module>\n    from .adapters import HTTPAdapter\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/requests/adapters.py\", line 81, in <module>\n    _preloaded_ssl_context.load_verify_locations(\nKeyboardInterrupt\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import re\nimport torch\nimport argparse\nimport numpy as np\nimport transformers\nimport torch.nn.functional as F\nimport itertools\nimport collections\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport nltk\nfrom nltk.stem import WordNetLemmatizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:53:18.301816Z","iopub.execute_input":"2025-04-17T15:53:18.302675Z","iopub.status.idle":"2025-04-17T15:53:29.374697Z","shell.execute_reply.started":"2025-04-17T15:53:18.302638Z","shell.execute_reply":"2025-04-17T15:53:29.373771Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"nltk.download('wordnet')\nnltk.download('omw-1.4')\nlemmatizer = WordNetLemmatizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:53:32.038106Z","iopub.execute_input":"2025-04-17T15:53:32.038582Z","iopub.status.idle":"2025-04-17T15:53:32.375917Z","shell.execute_reply.started":"2025-04-17T15:53:32.038555Z","shell.execute_reply":"2025-04-17T15:53:32.374905Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"PAD_TOKEN = '[PAD]'\nUNK_TOKEN = '[UNK]'\n\nauto_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T15:53:34.244339Z","iopub.execute_input":"2025-04-17T15:53:34.244626Z","iopub.status.idle":"2025-04-17T15:53:35.013425Z","shell.execute_reply.started":"2025-04-17T15:53:34.244607Z","shell.execute_reply":"2025-04-17T15:53:35.012315Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68969414a2184a379de53486fe31ef23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e65355bf59eb4653b4bc9d8369cd42cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d56c377e91443193659fb689681312"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ed38fda485442f8cc2584a10746643"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Helper Functions</p>","metadata":{}},{"cell_type":"code","source":"def cuda(args, tensor):\n    \"\"\"\n    Places tensor on CUDA device (by default, uses cuda:0).\n    \n    Returns:\n        Tensor on CUDA device.\n    \"\"\"\n    if args.use_gpu and torch:\n        return tensor.cuda()\n    else:\n        return tensor\n\ndef unpack(tensor):\n    \"\"\"\n    Unpacks a tensor into a Python list.\n\n    Args:\n        tensor: PyTorch tensor.\n\n    Returns:\n        Python list with tensor contents.\n    \"\"\"\n    if tensor.requires_grad:\n        tensor = tensor.detach()\n    return tensor.cpu().numpy().tolist()\n\ndef load_embeddings(path):\n    \"\"\"\n    Loads GloVe-style embeddings into memory.\n    Args:\n        path: Embedding path, e.g. \"glove/glove.6B.300d.txt\".\n\n    Returns:\n        Dictionary mapping words (strings) to vectors (list of floats).\n    \"\"\"\n    embedding_map = {}\n    with open(path, 'r', encoding=\"utf-8\") as f:\n        for line in f:\n            try:\n                pieces = line.rstrip().split()\n                word = pieces[0].lower()  # Normalize to lowercase\n                embedding_map[word] = [float(weight) for weight in pieces[1:]]\n                \n                # Also store lemma if different\n                lemma = lemmatizer.lemmatize(word)\n                if lemma != word and lemma not in embedding_map:\n                    embedding_map[lemma] = [float(weight) for weight in pieces[1:]]\n            except:\n                pass\n    return embedding_map\n\ndef embed_batch(embedding_map, embedding_layer, batch_token_ids, idx2word, embed_dim):\n    \"\"\"\n    Iteratively converts a batch of token id sequences into their embeddings.\n\n    Args:\n        embedding_map (dict): Mapping from to embedding vectors.\n        batch_token_ids (List[List[int]]): Batch where each element is a list of token ids.\n        idx2word (dict): Mapping from token ID (int) to the corresponding word (str)\n        embed_dim (int): The dimensionality of the embeddings.\n    \n    Returns:\n        Numpy array of shape (batch_size, seq_len, embed_dim) containing the embeddings.\n    \"\"\"\n    batch_embeddings = []\n    \n    for token_ids in batch_token_ids:\n        sequence_embeddings = []\n        for token_id in token_ids:\n            # Retrieve the corresponding word for the token id.\n            token = idx2word.get(token_id.item(), None)\n            # print(\"Token\", token_id.item(), token)\n            if token is None and token not in embedding_map:\n                token_embedding = np.zeros(embed_dim)\n            else:\n                try:\n                    token_tensor = torch.tensor([token_id.item()], device=device)\n                    token_embedding = embedding_layer(token_tensor).squeeze(0).cpu().detach().numpy()\n                except Exception as e:\n                    print(f\"Token ID {token_id} caused error: {e}\")\n                    token_embedding = np.zeros(embed_dim)\n\n            sequence_embeddings.append(token_embedding)\n        batch_embeddings.append(sequence_embeddings)\n    return np.array(batch_embeddings)\n\ndef co_attention(context_embedding, question_embedding, conv=True):\n    \"\"\"\n    Co-attention mechanism that computes attention between context and question encodings.\n    If `convolution=True`, applies local smoothing to the affinity matrix.\n\n    Args:\n        context_embedding (Tensor): (B, context_len, d)\n        question_embedding (Tensor): (B, question_len, d)\n        convolution (bool): whether to apply convolution-based smoothing.\n\n    Returns:\n        CP (Tensor): passage attention context\n        E_Out (Tensor): final encoder output\n    \"\"\"\n    # Step 1: Affinity matrix A ∈ (B, context_len, question_len)\n    A = torch.bmm(context_embedding, question_embedding.transpose(1, 2))\n    # print(\"context_embedding = \", context_embedding[0])\n    # print(\"question_embedding = \", question_embedding[0])\n    \n    # print(\"context_embedding:\", context_embedding)\n    # print(\"question_embedding:\", question_embedding)\n    # print(\"Affinity range:\", A.min().item(), A.max().item())\n\n    # Apply learned smoothing\n    if conv:\n        A = conv_co_attention(A)\n\n    # Step 2: Passage-to-question attention (row-wise)\n    A_P = F.softmax(A, dim=2)\n\n    # Step 3: Question-to-passage attention (column-wise)\n    A_Q = F.softmax(A.transpose(1, 2), dim=2)\n\n    # Step 4: Passage attention context: CP = H^P × A^Q\n    # print(\"Context Embedding Shape\", context_embedding.shape)\n    # print(\"Question Embedding Shape\", question_embedding.shape)\n    # print(\"A_Q Shape\", A_Q.shape)\n    CP = torch.bmm(A_Q, context_embedding)\n    # print(\"CP Shape\", CP.shape)  # (B, Lq, d)\n\n    # Step 5: Encoder output: concat(H^P, [H^Q; CP] × A^P)\n    # QC = torch.cat([question_embedding, CP], dim=1)\n    QC_1 = torch.bmm(A_P, question_embedding)  # (B, Lq, d)\n    # print(\"QC_1 Shape\", QC_1.shape)\n\n    QC_2 = torch.bmm(A_P, CP)  # (B, Lq, d)\n    # print(\"QC_2 Shape\", QC_2.shape)\n\n    # add & norm\n    norm_1 = nn.LayerNorm(QC_1.shape[-1]).to(QC_1.device)\n    QC = norm_1(QC_1 + QC_2)  # (B, Lq, d)\n\n    # QC = torch.cat([QC_1, QC_2], dim=1) # (B, Lq, 2d)\n    # QC = torch.cat([QC_1, QC_2], dim=-1)  # (B, Lq, 2d)\n    # print(\"QC Shape\", QC.shape)\n\n    # Final encoder output\n    # E_Out = torch.cat([context_embedding, QC], dim=2)\n    E_Out = torch.cat([context_embedding, QC], dim=-1)  # (B, Lq, 3d)\n    E_Out = nn.LayerNorm(E_Out.shape[-1]).to(E_Out.device)(E_Out)\n    E_Out = torch.tanh(E_Out)  # Apply non-linearity\n\n    # project to original dimension\n    E_Out = nn.Linear(E_Out.shape[-1], context_embedding.shape[-1]).to(E_Out.device)(E_Out)\n    # print(\"E_Out Shape\", E_Out.shape)\n\n    return CP, E_Out\n\ndef create_gaussian_kernel(kernel_width, device, sigma=1.0):\n    \"\"\"Creates a 1D Gaussian kernel.\"\"\"\n    x = torch.arange(-kernel_width//2 + 1, kernel_width//2 + 1, dtype=torch.float, device=device)\n    kernel = torch.exp(-x**2 / (2*sigma**2))\n    kernel /= kernel.sum()  # Normalize to sum to 1\n    return kernel.view(1, 1, -1)\n\ndef conv_co_attention(A, kernel_width=11):\n    \"\"\"\n    Enhanced convolution to shift attention to neighboring words.\n    Applies 1D convolution along context dimension per question word.\n    \"\"\"\n    B, Lp, Lq = A.shape\n    # Permute A for per-question-word processing: (B, Lq, Lp) -> (B*Lq, 1, Lp)\n    A_reshaped = A.permute(0, 2, 1).reshape(-1, 1, Lp)\n    \n    # Create Gaussian kernel with odd kernel width (e.g., 11)\n    kernel = create_gaussian_kernel(kernel_width, A.device, sigma=1.0)\n    \n    # Use symmetric padding that keeps the sequence length unchanged.\n    padded_length = (kernel_width - 1) // 2\n    smoothed_A = F.conv1d(A_reshaped, kernel, padding=padded_length)\n    \n    # Reshape back: current shape is (B*Lq, 1, Lp) --> (B, Lq, Lp) then permute to (B, Lp, Lq)\n    smoothed_A = smoothed_A.view(B, Lq, Lp).permute(0, 2, 1)\n    A_adjusted = A + smoothed_A  # Enhance original scores with neighbor context\n    return F.softmax(A_adjusted, dim=-1)\n\ndef tokenize_with_bert(text):\n    # Tokenize the text and request offset mappings.\n    encoding = auto_tokenizer(\n        text,\n        return_offsets_mapping=True,\n        add_special_tokens=False  # Disable adding special tokens to mimic simple whitespace tokenization.\n    )\n    \n    # Retrieve the tokens.\n    tokens = auto_tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n    \n    # Retrieve the spans from the offset mapping.\n    spans = encoding['offset_mapping']\n    return tokens, spans\n    \ndef create_embedding_matrix(vocab, embedding_map, embedding_dim=300, scale=0.6):\n    \"\"\"Initialize embedding matrix with:\n    - GloVe vectors for known words\n    - Random vectors for UNK tokens\n    - Zero vector for padding\n    \"\"\"\n    # Initialize with random normal distribution (match GloVe scale)\n    embedding_matrix = np.random.normal(\n        scale=scale, \n        size=(len(vocab), embedding_dim)\n    )\n    \n    # Handle special tokens\n    embedding_matrix[vocab.encoding[PAD_TOKEN]] = np.zeros(embedding_dim)\n    unk_idx = vocab.encoding[UNK_TOKEN]\n    embedding_matrix[unk_idx] = np.random.normal(scale=scale, size=embedding_dim)\n    \n    for word, idx in vocab.encoding.items():\n        if word in [PAD_TOKEN, UNK_TOKEN]:\n            continue\n            \n        # Try direct match\n        if word in embedding_map:\n            embedding_matrix[idx] = embedding_map[word]\n            continue\n            \n        # Try lemma\n        lemma = lemmatizer.lemmatize(word)\n        if lemma in embedding_map:\n            embedding_matrix[idx] = embedding_map[lemma]\n            continue\n            \n        # Try lowercase lemma\n        lower_lemma = lemmatizer.lemmatize(word.lower())\n        if lower_lemma in embedding_map:\n            embedding_matrix[idx] = embedding_map[lower_lemma]\n\n    return torch.tensor(embedding_matrix, dtype=torch.float32)\n\ndef enforce_position_constraints(end_logits, start_positions):\n    \"\"\"\n    Mask end_logits positions before the corresponding start_positions.\n    \"\"\"\n    batch_size, seq_len = end_logits.size()\n    positions = torch.arange(seq_len, device=end_logits.device).unsqueeze(0).expand(batch_size, seq_len)\n    mask = positions < start_positions.unsqueeze(1)\n    return end_logits.masked_fill(mask, float('-inf'))\n\ndef simple_tokenizer(text):\n    tokens = re.findall(r\"\\w+(?:[-']\\w+)*\", text.lower())\n    spans = []\n    start = 0\n    for token in tokens:\n        start = text.lower().find(token, start)\n        spans.append((start, start + len(token)))\n        start += len(token)\n    return tokens, spans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T18:21:30.903318Z","iopub.execute_input":"2025-04-17T18:21:30.903732Z","iopub.status.idle":"2025-04-17T18:21:30.956105Z","shell.execute_reply.started":"2025-04-17T18:21:30.903696Z","shell.execute_reply":"2025-04-17T18:21:30.954988Z"}},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":"# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Data Handling</p>","metadata":{}},{"cell_type":"code","source":"########################################################################################################\n#                                                                                                      #\n#                                                Vocabulary                                            #\n#                                                                                                      #\n########################################################################################################\nclass Vocabulary:\n    \"\"\"\n    Creates mappings for words → indices and indices → words.\n    \"\"\"\n    def __init__(self, samples, vocab_size):\n        self.samples = samples\n        self.vocab_size = vocab_size\n        self.words = self._initialize(samples, vocab_size)\n        self.encoding = {word: idx for idx, word in enumerate(self.words)}\n        self.decoding = {idx: word for idx, word in enumerate(self.words)}\n\n    def _initialize(self, samples, vocab_size):\n        \"\"\"Build vocabulary with lemma support\"\"\"\n        embedding_map = load_embeddings(\"/kaggle/input/glove/other/default/1/glove.6B.300d.txt\")\n        vocab_counts = collections.defaultdict(int)\n        \n        for _, row in samples.iterrows():\n            # Get base tokens\n            tokens = re.findall(r\"\\w+(?:[-']\\w+)*\", row['context'].lower()) + \\\n                     re.findall(r\"\\w+(?:[-']\\w+)*\", row['question'].lower())\n            \n            # Count both original and lemma forms\n            for token in tokens:\n                vocab_counts[token] += 1\n                lemma = lemmatizer.lemmatize(token)\n                if lemma != token:\n                    vocab_counts[lemma] += 0.5  # Partial count for lemmas\n        \n        # Sort by combined frequency\n        sorted_words = sorted(vocab_counts.items(), \n                            key=lambda x: (-x[1], x[0]))[:vocab_size-2]\n        \n        return [PAD_TOKEN, UNK_TOKEN] + [w[0] for w in sorted_words]\n        \n    def __len__(self):\n        return len(self.words)\n\n########################################################################################################\n#                                                                                                      #\n#                                                Tokenizer                                             #\n#                                                                                                      #\n########################################################################################################\nclass Tokenizer:\n    \"\"\"\n    Converts lists of words to indices and vice versa.\n    \"\"\"\n    def __init__(self, vocabulary):\n        self.vocabulary = vocabulary\n        self.pad_token_id = vocabulary.encoding[PAD_TOKEN]\n        self.unk_token_id = vocabulary.encoding[UNK_TOKEN]\n\n    def convert_tokens_to_ids(self, tokens):\n        return [self.vocabulary.encoding.get(token.lower(), self.unk_token_id) for token in tokens]\n\n    def convert_ids_to_tokens(self, token_ids):\n        return [self.vocabulary.decoding.get(token_id, UNK_TOKEN) for token_id in token_ids]\n\n########################################################################################################\n#                                                                                                      #\n#                                                QADataset                                             #\n#                                                                                                      #\n########################################################################################################\nclass QADataset(Dataset):\n    \"\"\"\n    Data generator for a QA task; the JSON file should contain character-level answer indices.\n    \"\"\"\n    def __init__(self, path):\n        # Load JSON-lines file; each line is a JSON object.\n        self.samples = pd.read_json(path, lines=True)\n        self.tokenizer = None\n        # Default pad token id; updated after tokenizer registration.\n        self.pad_token_id = 0\n\n    def _collate_batch(self, batch):\n        batch = [sample for sample in batch if sample is not None]\n        if len(batch) == 0:\n            return None  # All samples failed\n    \n        max_context_len = max(sample['context'].size(0) for sample in batch)\n        max_question_len = max(sample['question'].size(0) for sample in batch)\n        \n        contexts = torch.stack([\n            torch.cat([\n                sample['context'],\n                torch.full((max_context_len - sample['context'].size(0),), self.pad_token_id, dtype=torch.long)\n            ]) for sample in batch\n        ])\n    \n        questions = torch.stack([\n            torch.cat([\n                sample['question'],\n                torch.full((max_question_len - sample['question'].size(0),), self.pad_token_id, dtype=torch.long)\n            ]) for sample in batch\n        ])\n    \n        answer_starts = torch.stack([sample['answer_start'] for sample in batch])\n        answer_ends = torch.stack([sample['answer_end'] for sample in batch])\n    \n        return {\n            'context': contexts,\n            'question': questions,\n            'answer_start': answer_starts,\n            'answer_end': answer_ends\n        }\n\n    def register_tokenizer(self, tokenizer):\n        \"\"\"\n        Registers a Tokenizer instance and updates pad token id.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.pad_token_id = tokenizer.pad_token_id\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples.iloc[idx]\n        context_str = sample['context']\n        question_str = sample['question']\n        answers = sample['answers']\n    \n        # Tokenize context and question\n        # context_tokens, context_spans = tokenize_with_bert(context_str)\n        # question_tokens, _ = tokenize_with_bert(question_str)\n\n        context_tokens, context_spans = simple_tokenizer(context_str)\n        question_tokens, _ = simple_tokenizer(question_str)\n    \n        context_indices = self.tokenizer.convert_tokens_to_ids(context_tokens)\n        question_indices = self.tokenizer.convert_tokens_to_ids(question_tokens)\n    \n        # Character-level answer span\n        try:\n            char_start = answers['answer_start'][0]\n            answer_text = answers['text'][0].strip()\n            char_end = char_start + len(answer_text)\n        except Exception as e:\n            return None  # Invalid or empty answer\n    \n        # Convert character span to token indices\n        token_start, token_end = None, None\n        for i, (s, e) in enumerate(context_spans):\n            if s <= char_start < e:\n                token_start = i\n            if s < char_end <= e:\n                token_end = i\n            if token_start is not None and token_end is not None:\n                break\n    \n        if token_start is None or token_end is None:\n            return None  # Skip misaligned sample\n    \n        return {\n            'context': torch.tensor(context_indices, dtype=torch.long),\n            'question': torch.tensor(question_indices, dtype=torch.long),\n            'answer_start': torch.tensor(token_start, dtype=torch.long),\n            'answer_end': torch.tensor(token_end, dtype=torch.long),\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T18:21:32.506011Z","iopub.execute_input":"2025-04-17T18:21:32.506376Z","iopub.status.idle":"2025-04-17T18:21:32.530659Z","shell.execute_reply.started":"2025-04-17T18:21:32.506351Z","shell.execute_reply":"2025-04-17T18:21:32.529624Z"}},"outputs":[],"execution_count":90},{"cell_type":"markdown","source":"# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Model Architecture</p>","metadata":{}},{"cell_type":"code","source":"########################################################################################################\n#                                                                                                      #\n#                                       Transformer Blocks                                             #\n#                                                                                                      #\n########################################################################################################\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-head attention mechanism for the Transformer model.\n    This class implements the multi-head attention mechanism as described in the paper \"Attention is All You Need\".\n    \n    Args:\n        d_model (Tensor): Dimensionality of the input\n        num_heads (int): The number of attention heads to split the input into it.\n\n    Returns:\n        context: (Tensor): The context vector after applying attention.\n    \"\"\"\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n\n        # Ensure that the model dimension is divisible by the number of heads\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        # Initialize dimensions\n        self.d_model = d_model # Model's dimension\n        self.num_heads = num_heads # Number of attention heads\n        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n\n        # Linear layers for transforming inputs\n        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n\n    def scaled_dot_product_attention(self, query, key, value, mask=None):\n        # Calculate attention scores\n        attn_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n\n        # Apply softmax to get attention weights\n        attn_weights = F.softmax(attn_scores, dim=-1)\n\n        # Calculate the context vector as a weighted sum of values\n        context = torch.matmul(attn_weights, value)\n\n        return context\n\n    def split_heads(self, x):\n        \"\"\"\n        Split the input tensor into multiple heads for multi-head attention.\n        \"\"\"\n        # Reshape the input to have num_heads for multi-head attention.\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n\n    def combine_heads(self, x):\n        # Combine the multiple heads back to original shape.\n        batch_size, _, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n\n    def forward(self, query, key, value, mask=None):\n        # Apply linear transformations and split heads.\n        query = self.split_heads(self.W_q(query))  # (batch_size, num_heads, seq_length, d_k)\n        key = self.split_heads(self.W_k(key))  # (batch_size, num_heads, seq_length, d_k)\n        value = self.split_heads(self.W_v(value))  # (batch_size, num_heads, seq_length, d_k)\n\n        # Perform scaled dot-product attention\n        context = self.scaled_dot_product_attention(query, key, value, mask)  # (batch_size, num_heads, seq_length, d_k)\n\n        # Combine the heads and apply output transformation.\n        output = self.W_o(self.combine_heads(context))\n\n        return output\n    \nclass PositionWiseFeedForward(nn.Module):\n    \"\"\"\n    Position-wise feed-forward netowrk consists of two linear transformations with a ReLU activation in between.\n    In the context of transformer models, this feed-forward network is applied to each position separately and identically.\n    It helps in transforming the features learned by the attention mechanisms within the transformer, \n    acting as an additional processing step for the attention outputs.\n    \"\"\"\n    def __init__(self, d_model, d_ff, dropout_prob=0.1):\n        \"\"\"\n        Args:\n            d_model (int): Dimensionality of the input.\n            d_ff (int): Dimensionality of the feed-forward layer.\n            dropout_prob (float): Dropout probability for regularization.\n        \"\"\"\n        super(PositionWiseFeedForward, self).__init__()\n\n        # Feed-forward network with two linear layers and ReLU activation.\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the feed-forward network.\n        \n        Args:\n            x (Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n\n        Returns:\n            out (Tensor): Output tensor of the same shape as input.\n        \"\"\"\n        # Apply the first linear transformation, activation, and dropout.\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n\n        # Apply the second linear transformation.\n        out = self.fc2(x)\n\n        return out\n    \nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Positional encoding is used to inject information about the relative or absolute position of the tokens in the sequence.\n    It helps the model understand the order of the tokens, as the transformer architecture does not inherently capture this information.\n    \"\"\"\n    def __init__(self, d_model, max_seq_length=5000):\n        \"\"\"\n        Args:\n            d_model (int): Dimensionality of the model.\n            max_seq_length (int): Maximum length of the input sequences.\n        \"\"\"\n        super(PositionalEncoding, self).__init__()\n\n        pe = torch.zeros(max_seq_length, d_model) # A tensor filled with zeros, which will be populated with positional encodings.\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1) # A tensor containing the position indices for each positon in the sequence.\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) # A term used to scale the position indices in a spcific way.\n        \n        # Apply the sine function to the even indices.\n        pe[:, 0::2] = torch.sin(position * div_term)\n\n        # Apply the cosine the function to the odd indices.\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        # Register pe as a buffer, which means it will be part of the module's state but will not be considered a trainable parameter.\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        # Add positional encoding to input tensor.\n        return x + self.pe[:, :x.size(1)]\n\nclass TransformerEncoder(nn.Module):\n    \"\"\"\n    Transformer Encoder block that consists of multi-head self-attention and feed-forward layers.\n    The encoder processes the input sequences and generates a contextual representation of the input.\n    \"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout_prob=0.3):\n        \"\"\"\n        Transformer Encoder block that consists of multi-head self-attention and feed-forward layers.\n\n        Args:\n            d_model (int): Dimensionality of the model.\n            num_heads (int): Number of attention heads.\n            d_ff (int): Dimensionality of the feed-forward layer.\n            dropout_prob (float): Dropout probability for regularization.\n        \"\"\"\n        super(TransformerEncoder, self).__init__()\n\n        self.self_attn = MultiHeadAttention(d_model, num_heads) # multi-head attention mechanism.\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout_prob) # position-wise feed-forward neural network.\n        self.norm1 = nn.LayerNorm(d_model) # layer normalization, applied to smooth the layer's input.\n        self.norm2 = nn.LayerNorm(d_model) # layer normalization, applied to smooth the layer's input.\n        self.dropout = nn.Dropout(dropout_prob) # dropout layer, used to prevent overfitting by randomly setting some activatons to zero during training.\n        \n    def forward(self, x, mask):\n        \"\"\"\n        Forward pass through the transformer encoder block.\n\n        Args:\n            x (Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n            mask (Tensor): Mask tensor to prevent attention to certain positions.\n\n        Returns:\n            out (Tensor): Output tensor of the same shape as input.\n        \"\"\"\n        # Apply multi-head self-attention\n        attn_output = self.self_attn(x, x, x, mask)\n\n        # Apply dropout and layer normalization.\n        x = self.norm1(x + self.dropout(attn_output))\n\n        # Apply feed-forward network\n        ff_output = self.feed_forward(x)\n\n        # Apply dropout and layer normalization.\n        out = self.norm2(x + self.dropout(ff_output))\n\n        return out\n\n########################################################################################################\n#                                                                                                      #\n#                                    QATransformerBasedModel                                           #\n#                                                                                                      #\n########################################################################################################\nclass QATransformerBasedModel(nn.Module):\n    def __init__(self,\n                 vocab,                 \n                 vocab_decoder,         \n                 embedding_dim,         \n                 d_ff,                  \n                 num_layers,            \n                 num_heads,\n                 max_seq_length,\n                 dropout_prob=0.3):      \n\n        super(QATransformerBasedModel, self).__init__()\n\n        # Load GloVe embedding map\n        self.embedding_map = load_embeddings(\"/kaggle/input/glove/other/default/1/glove.6B.300d.txt\")\n\n        # Word embedding layer\n        self.embedding_layer = nn.Embedding(\n            num_embeddings = len(vocab),\n            embedding_dim = embedding_dim,\n            padding_idx = vocab.encoding[PAD_TOKEN]\n        )\n\n        # Initialize embedding weights from GloVe\n        self.embedding_matrix = create_embedding_matrix(vocab, self.embedding_map)\n        self.embedding_layer.weight.data.copy_(self.embedding_matrix)\n\n        # Positional Encoding\n        self.positional_encoding = PositionalEncoding(embedding_dim, max_seq_length=max_seq_length)\n\n        # Transformer encoder\n        self.encoder_layer = TransformerEncoder(\n            d_model=embedding_dim, \n            num_heads=num_heads, \n            d_ff=d_ff, \n            dropout_prob=dropout_prob\n        )\n\n        # Cross-attention (context attends to question)\n        self.cross_attn = MultiHeadAttention(embedding_dim, num_heads)\n\n        # Span Predictor\n        self.dense_1 = nn.Linear(embedding_dim, 2 * embedding_dim)\n        self.norm_1 = nn.LayerNorm(2 * embedding_dim)\n        self.dense_2 = nn.Linear(2 * embedding_dim, embedding_dim)\n        self.norm_2 = nn.LayerNorm(embedding_dim)\n        self.dense_3 = nn.Linear(embedding_dim, 2)\n\n        self.dropout = nn.Dropout(dropout_prob)\n\n        self.vocab = vocab\n\n    def forward(self, context_ids, question_ids):\n        \"\"\"\n        Args:\n            context_ids: LongTensor of shape (batch, context_len)\n            question_ids: LongTensor of shape (batch, question_len)\n        Returns:\n            start_logits, end_logits: shape (batch, context_len)\n        \"\"\"\n\n        # Embedding + positional encoding\n        context_emb = self.positional_encoding(self.embedding_layer(context_ids))\n        question_emb = self.embedding_layer(question_ids)\n        \n        # Create padding masks (batch, seq_len)\n        context_mask = (context_ids != self.vocab.encoding[PAD_TOKEN])\n        question_pad_mask = (question_ids != self.vocab.encoding[PAD_TOKEN])\n        \n        # Reshape for cross-attention: (B, 1, 1, Q_len)\n        cross_attn_mask = question_pad_mask.unsqueeze(1).unsqueeze(1)\n    \n        # Reshape for encoder self-attention: (B, 1, 1, C_len)\n        encoder_mask = context_mask.unsqueeze(1).unsqueeze(1)\n\n        # Cross-attention: context attends to question\n        cross_attn_output = self.cross_attn(\n            query=context_emb,\n            key=question_emb,\n            value=question_emb,\n            mask=cross_attn_mask\n        )\n        cross_attn_output = cross_attn_output\n        \n        # Transformer encoder on context + cross-attended question info\n        enc_output = self.encoder_layer(\n            cross_attn_output,\n            mask=encoder_mask\n        )\n\n        # Span prediction head\n        out = self.dropout(F.relu(self.norm_1(self.dense_1(enc_output))))\n        out = self.dropout(F.relu(self.norm_2(self.dense_2(out))))\n        logits = self.dense_3(out)  # (batch, context_len, 2)\n\n        return logits[:, :, 0], logits[:, :, 1]  # (batch, context_len)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T18:21:36.307122Z","iopub.execute_input":"2025-04-17T18:21:36.307450Z","iopub.status.idle":"2025-04-17T18:21:36.337955Z","shell.execute_reply.started":"2025-04-17T18:21:36.307428Z","shell.execute_reply":"2025-04-17T18:21:36.336925Z"}},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":"# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Model Training</p>","metadata":{}},{"cell_type":"markdown","source":"## Evaluation Functions","metadata":{}},{"cell_type":"code","source":"# Evaluation Functions\ndef compute_em(predicted, actual):\n    return int(predicted.strip().lower() == actual.strip().lower())\n\ndef compute_f1(predicted, actual):\n    pred_tokens = predicted.strip().lower().split()\n    actual_tokens = actual.strip().lower().split()\n\n    common = set(pred_tokens) & set(actual_tokens)\n    if not common:\n        return 0.0\n    \n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(actual_tokens)\n    f1 = 2 * precision * recall / (precision + recall)\n    \n    return f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T18:21:40.883122Z","iopub.execute_input":"2025-04-17T18:21:40.883476Z","iopub.status.idle":"2025-04-17T18:21:40.891181Z","shell.execute_reply.started":"2025-04-17T18:21:40.883450Z","shell.execute_reply":"2025-04-17T18:21:40.890101Z"}},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":"## Setting Data paths & Hyperparameters","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nfrom copy import deepcopy\n\n# Kaggle dataset paths\nMODEL_NAME = 'QATransformerBasedModel'  # or '# Kaggle dataset paths\nTRAIN_PATH = '/kaggle/input/squad-v2/train.json'\nVAL_PATH = '/kaggle/input/validation/validation.json'\nWORKING_DIR = '/kaggle/working'\nMODEL_DIR = os.path.join(WORKING_DIR, 'model')\nOUT_DIR = os.path.join(WORKING_DIR, 'out')\n\n# Hyperparameters\nBATCH_SIZE = 16\nNUM_EPOCHS = 30\nLEARNING_RATE = 1e-3\nEMBEDDING_DIM = 300\nNUM_HEADS = 6\nD_FF = 512\nNUM_LAYERS = 2\nDROPOUT = 0.3\nMAX_CONTEXT_LEN = 400\nMODEL_NAME = 'QATransformerBasedModel'\n\n# Ensure directories exist\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# Device config\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T18:21:42.374016Z","iopub.execute_input":"2025-04-17T18:21:42.374402Z","iopub.status.idle":"2025-04-17T18:21:42.383896Z","shell.execute_reply.started":"2025-04-17T18:21:42.374372Z","shell.execute_reply":"2025-04-17T18:21:42.382834Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":93},{"cell_type":"markdown","source":"## Loading Dataset","metadata":{}},{"cell_type":"code","source":"# Load datasets\ntrain_ds = QADataset(path=TRAIN_PATH)\nval_ds = QADataset(path=VAL_PATH)\n\n# Build vocab and tokenizer\nvocab = Vocabulary(train_ds.samples, vocab_size=200000)\ntokenizer = Tokenizer(vocab)\ntrain_ds.tokenizer = tokenizer\nval_ds.tokenizer = tokenizer\n\n# Filter long contexts\ndef filter_len(df):\n    return df[df['context'].map(len) <= MAX_CONTEXT_LEN].reset_index(drop=True)\n    \ntrain_ds.samples = filter_len(train_ds.samples)\nval_ds.samples = filter_len(val_ds.samples)\n\n# DataLoaders\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=train_ds._collate_batch, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n                        collate_fn=val_ds._collate_batch, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T18:21:44.013419Z","iopub.execute_input":"2025-04-17T18:21:44.014194Z","iopub.status.idle":"2025-04-17T18:24:27.587504Z","shell.execute_reply.started":"2025-04-17T18:21:44.014160Z","shell.execute_reply":"2025-04-17T18:24:27.586352Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"len(train_ds.samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T17:21:36.470817Z","iopub.execute_input":"2025-04-17T17:21:36.471218Z","iopub.status.idle":"2025-04-17T17:21:36.478257Z","shell.execute_reply.started":"2025-04-17T17:21:36.471189Z","shell.execute_reply":"2025-04-17T17:21:36.477266Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"17822"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"total_examples = len(train_ds.samples)\nnum_unique_contexts = train_ds.samples['context'].nunique()\n\nprint(f\"Total examples:          {total_examples}\")\nprint(f\"Unique contexts count:   {num_unique_contexts}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:03:45.598790Z","iopub.execute_input":"2025-04-17T20:03:45.599753Z","iopub.status.idle":"2025-04-17T20:03:45.638812Z","shell.execute_reply.started":"2025-04-17T20:03:45.599721Z","shell.execute_reply":"2025-04-17T20:03:45.637873Z"}},"outputs":[{"name":"stdout","text":"Total examples:          17822\nUnique contexts count:   12533\n","output_type":"stream"}],"execution_count":98},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"def constrained_span_loss(start_logits, end_logits, start_pos, end_pos, context_lengths):\n    # Mask invalid positions\n    batch_size, max_len = start_logits.shape\n    mask = torch.arange(max_len).expand(batch_size, max_len).to(device) < context_lengths.unsqueeze(1)\n    \n    # Start loss with valid positions\n    start_loss = F.cross_entropy(\n        start_logits.masked_fill(~mask, float('-inf')), \n        start_pos\n    )\n    \n    # End loss must be >= start\n    s_pred_train = start_logits.argmax(dim=1)\n    end_mask_train = mask & (positions >= s_pred_train.unsqueeze(1))\n    end_loss = F.cross_entropy(\n        end_logits.masked_fill(~end_mask_train, -inf),\n        end_pos\n    )\n    \n    # Dynamic weighting based on error magnitude\n    return start_loss + 1.2 * end_loss  # Empirically better weighting","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T20:04:20.111153Z","iopub.execute_input":"2025-04-17T20:04:20.111516Z","iopub.status.idle":"2025-04-17T20:04:20.118199Z","shell.execute_reply.started":"2025-04-17T20:04:20.111493Z","shell.execute_reply":"2025-04-17T20:04:20.117124Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"# Model\nmodel = QATransformerBasedModel(\n    vocab=vocab,\n    vocab_decoder=vocab.decoding,\n    embedding_dim=EMBEDDING_DIM,\n    num_heads=NUM_HEADS,\n    d_ff=D_FF,\n    num_layers=NUM_LAYERS,\n    max_seq_length=MAX_CONTEXT_LEN,\n    dropout_prob=DROPOUT\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=100,\n    num_training_steps=NUM_EPOCHS * len(train_loader)\n)\n\n# Training loop\ntrain_losses      = []\ntrain_start_accs  = []\ntrain_end_accs    = []\nval_em_scores     = []\nval_f1_scores     = []\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    model.train()\n    epoch_loss = 0.0\n    for batch in train_loader:\n        if batch is None:\n            continue\n\n        ctx = batch['context'].to(device)\n        qry = batch['question'].to(device)\n        start_gt = batch['answer_start'].to(device)\n        end_gt   = batch['answer_end'].to(device)\n\n        # compute context lengths once\n        context_lengths = (ctx != PAD_ID).sum(dim=1)\n\n        optimizer.zero_grad()\n        s_logit, e_logit = model(ctx, qry)                            # (B, L)\n\n        # use predicted start for masking end\n        start_loss, end_loss, s_pred, e_logit_masked = constrained_span_loss(\n            s_logit, e_logit, start_gt, end_gt, context_lengths,\n            use_predicted_start=True\n        )\n\n        loss = start_loss + 1.2 * end_loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n        epoch_loss += loss.item()\n\n        # track per-batch accuracies\n        start_acc = (s_pred == start_gt).float().mean().item()\n        e_pred = e_logit_masked.argmax(dim=1)\n        end_acc   = (e_pred == end_gt).float().mean().item()\n\n        train_start_accs.append(start_acc)\n        train_end_accs.append(end_acc)\n\n    avg_loss      = epoch_loss / len(train_loader)\n    avg_start_acc = np.mean(train_start_accs) * 100\n    avg_end_acc   = np.mean(train_end_accs)   * 100\n    train_losses.append(avg_loss)\n\n    print(f\"Epoch {epoch}/{NUM_EPOCHS} — \"\n          f\"Loss: {avg_loss:.4f} — \"\n          f\"Start Acc: {avg_start_acc:.2f}% — \"\n          f\"End Acc: {avg_end_acc:.2f}%\")\n\n    # Validation\n    model.eval()\n    ems, f1s = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            if batch is None:\n                continue\n                \n            ctx = batch['context'].to(device)\n            qry = batch['question'].to(device)\n            start_gt = batch['answer_start'].to(device)\n            end_gt = batch['answer_end'].to(device)\n\n            s_logit, e_logit = model(ctx, qry)\n            e_logit = enforce_position_constraints(e_logit, start_gt)\n            s_pred = s_logit.argmax(dim=1)\n            e_pred = e_logit.argmax(dim=1)\n            for i in range(ctx.size(0)):\n                tok_ids = ctx[i].cpu().tolist()\n                tokens = tokenizer.convert_ids_to_tokens(tok_ids)\n                s, e = s_pred[i].item(), e_pred[i].item()\n                if e < s: e = s\n                pred = \" \".join(tokens[s:e+1]).strip()\n                gold = val_ds.samples.iloc[i]['answers']['text'][0].strip()\n                ems.append(compute_em(pred, gold))\n                f1s.append(compute_f1(pred, gold))\n\n    mean_em = np.mean(ems) * 100\n    mean_f1 = np.mean(f1s) * 100\n    val_em_scores.append(mean_em)\n    val_f1_scores.append(mean_f1)\n    print(f\"Epoch {epoch} — Val EM: {mean_em:.2f}%, F1: {mean_f1:.2f}%\")\n\n    if epoch % 5 == 0:\n        ckpt = os.path.join(MODEL_DIR, f\"{MODEL_NAME}_ep{epoch}.pt\")\n        torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'vocab': vocab}, ckpt)\n        print(f\"Checkpoint saved: {ckpt}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T18:25:09.964804Z","iopub.execute_input":"2025-04-17T18:25:09.965201Z","iopub.status.idle":"2025-04-17T19:59:05.254466Z","shell.execute_reply.started":"2025-04-17T18:25:09.965171Z","shell.execute_reply":"2025-04-17T19:59:05.251974Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30 — Train Loss: 5.9136\nEpoch 1 — Val EM: 0.29%, F1: 0.77%\nEpoch 2/30 — Train Loss: 4.8875\nEpoch 2 — Val EM: 0.44%, F1: 0.91%\nEpoch 3/30 — Train Loss: 4.4014\nEpoch 3 — Val EM: 0.44%, F1: 0.87%\nEpoch 4/30 — Train Loss: 3.9264\nEpoch 4 — Val EM: 0.58%, F1: 1.01%\nEpoch 5/30 — Train Loss: 3.4415\nEpoch 5 — Val EM: 0.58%, F1: 1.25%\nCheckpoint saved: /kaggle/working/model/QATransformerBasedModel_ep5.pt\nEpoch 6/30 — Train Loss: 2.9361\nEpoch 6 — Val EM: 0.58%, F1: 1.22%\nEpoch 7/30 — Train Loss: 2.4525\nEpoch 7 — Val EM: 0.44%, F1: 1.16%\nEpoch 8/30 — Train Loss: 1.9672\nEpoch 8 — Val EM: 0.44%, F1: 1.04%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3322524568.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;31m# Lastly, switch back to complex view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":95},{"cell_type":"code","source":"# Save metrics\nmetrics = pd.DataFrame({\n    'epoch': range(1, NUM_EPOCHS+1),\n    'train_loss': train_losses,\n    'val_em': val_em_scores,\n    'val_f1': val_f1_scores\n})\nmetrics.to_csv(os.path.join(OUT_DIR, 'metrics.csv'), index=False)\nprint(f\"Metrics written to {os.path.join(OUT_DIR, 'metrics.csv')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:32:21.170035Z","iopub.execute_input":"2025-04-17T05:32:21.170820Z","iopub.status.idle":"2025-04-17T05:32:21.181446Z","shell.execute_reply.started":"2025-04-17T05:32:21.170782Z","shell.execute_reply":"2025-04-17T05:32:21.180499Z"}},"outputs":[{"name":"stdout","text":"10\n10\nMetrics written to /kaggle/working/out/metrics.csv\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('models', 'zip', '/kaggle/working/model/')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'models.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Model Evaluation</p>","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Checkpoint directory (modify as needed)\ncheckpoint_dir = \"/kaggle/working/out/\"\nimport os\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\n# Load validation dataset\nval_dataset = QADataset(path=\"/kaggle/input/validation/validation.json\")\ncheckpoint_dir = \"/kaggle/working/model/\"\ncheckpoints = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith(\".pt\")])\n\n# Prepare validation vocab from any one checkpoint (they all use same vocab)\nsample_checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoints[-1]), map_location=device)\nval_vocab = sample_checkpoint['vocab']\nval_tokenizer = Tokenizer(val_vocab)\nval_dataset.tokenizer = val_tokenizer\n\n# Filter short contexts only\nval_dataset.samples = val_dataset.samples[val_dataset.samples['context'].apply(lambda x: len(x) <= 400)].reset_index(drop=True)\n\n# Shared hyperparameters\nvocab_size = 200000\nembedding_dim = 300\nhidden_size = 128\nnum_layers = 1\noutput_size = 400\n\n# Output summary\nsummary = []\n\n# === Loop Over Checkpoints ===\nfor ckpt_file in checkpoints:\n    print(f\"\\nEvaluating {ckpt_file}...\")\n\n    # Load model\n    checkpoint = torch.load(os.path.join(checkpoint_dir, ckpt_file), map_location=device)\n    model = QAModel(\n        vocab_size=vocab_size,\n        vocab_decoder=val_vocab.decoding,\n        embedding_dim=embedding_dim,\n        hidden_size=hidden_size,\n        num_layers=num_layers,\n        output_size=output_size\n    ).to(device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n\n    em_scores = []\n    f1_scores = []\n    output_data = []\n\n    valid_sample_count = 0\n    for i in range(len(val_dataset)):\n        sample = val_dataset[i]\n        \n        if sample is None:\n            continue\n        \n        row = val_dataset.samples.iloc[i]\n        valid_sample_count = valid_sample_count + 1\n\n        context_text = row['context']\n        question_text = row['question']\n        answer_text = row['answers']['text'][0].strip()\n        answer_start = row['answers']['answer_start'][0]\n        answer_end = answer_start + len(answer_text)\n\n        context_tensor = sample['context'].unsqueeze(0).to(device)\n        question_tensor = sample['question'].unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            start_logits, end_logits = model(context_tensor, question_tensor)\n            start_idx = torch.argmax(start_logits, dim=1).item()\n            end_idx = torch.argmax(end_logits, dim=1).item()\n            if end_idx < start_idx:\n                end_idx = start_idx\n\n        context_tokens = val_tokenizer.convert_ids_to_tokens(sample['context'].tolist())\n        predicted_answer = \" \".join(context_tokens[start_idx:end_idx + 1]).strip()\n\n        em = compute_em(predicted_answer, answer_text)\n        f1 = compute_f1(predicted_answer, answer_text)\n\n        em_scores.append(em)\n        f1_scores.append(f1)\n\n        output_data.append({\n            \"context\": context_text,\n            \"question\": question_text,\n            \"answer\": answer_text,\n            \"predicted_answer\": predicted_answer,\n            \"answer_start_index\": answer_start,\n            \"answer_end_index\": answer_end,\n            \"predicted_start_index\": start_idx,\n            \"predicted_end_index\": end_idx\n        })\n\n    # Save per-checkpoint results\n    epoch_num = ckpt_file.split(\"_\")[-1].replace(\".pt\", \"\")\n    df_output = pd.DataFrame(output_data)\n    df_output.to_csv(f\"/kaggle/working/out/validation_output_epoch_{epoch_num}.csv\", index=False)\n\n    # Log summary\n    mean_em = np.mean(em_scores) * 100\n    mean_f1 = np.mean(f1_scores) * 100\n    summary.append({\n        \"checkpoint\": ckpt_file,\n        \"EM\": mean_em,\n        \"F1\": mean_f1\n    })\n\n    print(f\"→ EM: {mean_em:.2f}%, F1: {mean_f1:.2f}%\")\n    print(f\"✅ Valid testing samples this epoch: {valid_sample_count}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save summary CSV\nsummary_df = pd.DataFrame(summary)\nsummary_df.to_csv(\"/kaggle/working/out/eval_summary.csv\", index=False)\n\nprint(\"\\n✅ Evaluation complete! Results saved to:\")\nprint(\"- /kaggle/working/out/eval_summary.csv (summary)\")\nprint(\"- /kaggle/working/out/validation_output_epoch_*.csv (detailed per checkpoint)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('out', 'zip', '/kaggle/working/out/')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'out.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}