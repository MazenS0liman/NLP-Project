{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a82d72",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Experiment 2: Zero-Shot Full Finetuned LlaMa vs Zero-Shot Gemini</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43e04f",
   "metadata": {},
   "source": [
    "**Description:** This experiment is about comparing zero-shot Finetuned LlaMa-3B-Instruct model and zero-shot Google Gemini based on the SQUAD's validation dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5eeb0",
   "metadata": {},
   "source": [
    "## <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Imports</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0d6cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Callable, Dict\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Convert_system_message_to_human will be deprecated!\"\n",
    ")\n",
    "\n",
    "# LangChain components\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58bc17",
   "metadata": {},
   "source": [
    "## <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">RAG</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d20a0",
   "metadata": {},
   "source": [
    "### 1. Define Data Source Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "301b81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = Path(\"experiments/data/validation.json\")\n",
    "OUTPUT_FILE = Path(\"experiments/data/context.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f5a56",
   "metadata": {},
   "source": [
    "### 2. Define Extract Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d4aa785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(input_path):\n",
    "    context_set = set()\n",
    "    with input_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            record = json.loads(line)\n",
    "            ctx = record.get(\"context\")\n",
    "            if ctx and ctx not in context_set:\n",
    "                context_set.add(ctx)\n",
    "                yield ctx\n",
    "\n",
    "def write_contexts_to_file(contexts, output_path):\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i, ctx in enumerate(contexts, start=1):\n",
    "            f.write(ctx)\n",
    "            f.write(\"\\n\\n\")\n",
    "    print(f\"Wrote {i} contexts to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bd38287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2067 contexts to experiments\\data\\context.txt\n"
     ]
    }
   ],
   "source": [
    "contexts = extract(INPUT_FILE)\n",
    "write_contexts_to_file(contexts, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5aa06",
   "metadata": {},
   "source": [
    "### 3. Store Data in Qdrant DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c39e7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b352b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fastembed_model():\n",
    "    return FastEmbedEmbeddings()\n",
    "\n",
    "def create_embeddings():\n",
    "    return get_fastembed_model()\n",
    "\n",
    "def load_text_files(txt_file: str) -> list[Document]:\n",
    "    text = Path(txt_file).read_text(encoding=\"utf-8\")\n",
    "    blocks = [blk.strip() for blk in text.split(\"\\n\\n\") if blk.strip()]\n",
    "    docs = []\n",
    "    for idx, blk in enumerate(blocks, start=1):\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=blk,\n",
    "                metadata={\n",
    "                    \"source\": Path(txt_file).name,\n",
    "                    \"block_index\": idx,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "def build_qdrant_index(docs: list[Document], embeddings):\n",
    "    return Qdrant.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        location=\":memory:\",\n",
    "        collection_name=\"text_chunks\",\n",
    "    )\n",
    "\n",
    "def generate_retriever(txt_dir: str = \"./data/all_contexts.txt\"):\n",
    "    raw_docs = load_text_files(txt_dir)\n",
    "    print(f\"Loaded {len(raw_docs)} documents from {txt_dir}\")\n",
    "    embeddings = create_embeddings()\n",
    "    vectorstore = build_qdrant_index(raw_docs, embeddings)\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5},\n",
    "    )\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "230d14f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2067 documents from ./experiments/data/context.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MET\\Semester 10\\[CSEN1076] Natural Language Processing and Information Retrieval\\Project\\NLP-Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:53<00:00, 10.65s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever = generate_retriever(txt_dir=\"./experiments/data/context.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f443ce",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc580108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mazen\\AppData\\Local\\Temp\\ipykernel_21948\\397147675.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Since its inception, ABC has had many affiliated stations, which include WABC-TV and WPVI-TV, the first two stations to carry the network's programming. As of March 2015[update], ABC has eight owned-and-operated stations, and current and pending affiliation agreements with 235 additional television stations encompassing 49 states, the District of Columbia, four U.S. possessions, Bermuda and Saba; this makes ABC the largest U.S. broadcast television network by total number of affiliates. The network has an estimated national reach of 96.26% of all households in the United States (or 300,794,157 Americans with at least one television set). relevant documents.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is ABC?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "# get text from the first 5 results\n",
    "print(f\"Found {results[0].page_content} relevant documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57b355",
   "metadata": {},
   "source": [
    "## <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Evaluation Functions</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d71658",
   "metadata": {},
   "source": [
    "### 1. Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(prediction: str, ground_truth: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the token-level F1 score between prediction and ground_truth.\n",
    "    \"\"\"\n",
    "    pred_tokens = prediction.split()\n",
    "    gt_tokens = ground_truth.split()\n",
    "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def evaluate_model(\n",
    "    predict_fn: Callable[[str, str], str],\n",
    "    data_path: Path,\n",
    "    max_rows: int = 50\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a model over the first `max_rows` entries of a JSONL file with fields: context, question, answers.text.\n",
    "    Returns average F1 score and number of samples evaluated.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    with data_path.open('r', encoding='utf-8') as f:\n",
    "        for line in itertools.islice(f, max_rows):\n",
    "            record = json.loads(line)\n",
    "            question = record.get('question', '')\n",
    "            docs = retriever.get_relevant_documents(question)\n",
    "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "            gold_texts = record.get('answers', {}).get('text', [])\n",
    "            \n",
    "            # get prediction\n",
    "            pred = predict_fn(question, context)\n",
    "            \n",
    "            # compute best F1 against all golds\n",
    "            if gold_texts:\n",
    "                sample_f1 = max(f1_score(pred, gt) for gt in gold_texts)\n",
    "            else:\n",
    "                sample_f1 = 0.0\n",
    "            scores.append(sample_f1)\n",
    "\n",
    "    num_samples = len(scores)\n",
    "    avg_f1 = sum(scores) / num_samples if num_samples else 0.0\n",
    "    return {\"average_f1\": avg_f1, \"num_samples\": num_samples}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1e8e2",
   "metadata": {},
   "source": [
    "## <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">LLM Model 1: Zero-Shot Google Gemini</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fef92",
   "metadata": {},
   "source": [
    "### 1. Define Model & API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0342a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"gemini-1.5-flash-8b\"\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "LANGCHAIN_PROJECT = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276bacc",
   "metadata": {},
   "source": [
    "### 2. Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3d76a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected argument 'prompt' provided to ChatGoogleGenerativeAI.\n",
      "d:\\MET\\Semester 10\\[CSEN1076] Natural Language Processing and Information Retrieval\\Project\\NLP-Project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3670: UserWarning: WARNING! prompt is not default parameter.\n",
      "                prompt was transferred to model_kwargs.\n",
      "                Please confirm that prompt is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\mazen\\AppData\\Local\\Temp\\ipykernel_21948\\579764530.py:23: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n",
      "C:\\Users\\mazen\\AppData\\Local\\Temp\\ipykernel_21948\\579764530.py:31: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"context\"],\n",
    "    template=\"\"\"\n",
    "        ### Instructions\\n\n",
    "        You are an expert QA assistant that answer user's question based on the context.\\n\\n\n",
    "        ### Chat history\\n\n",
    "        {chat_history}\\n\\n\n",
    "        ### Context:\\n\n",
    "        {context}\\n\\n\n",
    "        ### User question:\\n\n",
    "        {input}\\n\\n\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=MODEL_NAME,\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    convert_system_message_to_human=True,\n",
    "    prompt=PROMPT,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key='input',\n",
    "    output_key='text',\n",
    "    return_messages=True,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a14e7",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cace0dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Paris.\n"
     ]
    }
   ],
   "source": [
    "response_text = llm_chain.invoke({\n",
    "    \"input\": \"What is the capital of France?\",\n",
    "    \"context\": \"The capital of France is Paris.\"\n",
    "})\n",
    "\n",
    "print(\"Assistant:\", response_text[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcfab46",
   "metadata": {},
   "source": [
    "### 3. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82aae9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model1(question: str, context: str) -> str:\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            result = llm_chain.invoke({\"input\": question, \"context\": context})\n",
    "            return result.get(\"text\", \"\")\n",
    "        except ResourceExhausted as e:\n",
    "            wait = 60  # sleep long enough to clear the per-minute bucket\n",
    "            print(f\"Quota exceeded, sleeping for {wait}s… (attempt {attempt+1}/5)\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(\"Max retries hit for model1 due to quota limits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eaeb85af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded, sleeping for 60s… (attempt 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded, sleeping for 60s… (attempt 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quota exceeded, sleeping for 60s… (attempt 1/5)\n"
     ]
    }
   ],
   "source": [
    "data_file = Path('experiments/data/validation.json')\n",
    "print('Evaluating Model 1...')\n",
    "res1 = evaluate_model(predict_model1, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb35e456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_f1': 0.5457342444277429, 'num_samples': 50}\n"
     ]
    }
   ],
   "source": [
    "print(res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3b65c",
   "metadata": {},
   "source": [
    "## <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">LLM Model 2: Zero-Shot Finetuned LlaMa-3B-Instruct Model</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68650f8e",
   "metadata": {},
   "source": [
    "### 1. Load the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1956008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.llama_3b_instruct_finetuned_llm import load_model, generate_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cb14226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 06:34:23.028 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-15 06:34:23.366 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run d:\\MET\\Semester 10\\[CSEN1076] Natural Language Processing and Information Retrieval\\Project\\NLP-Project\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-15 06:34:23.367 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-15 06:34:23.368 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-15 06:34:23.942 Thread 'Thread-19': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-15 06:34:23.951 Thread 'Thread-19': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "d:\\MET\\Semester 10\\[CSEN1076] Natural Language Processing and Information Retrieval\\Project\\NLP-Project\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:807: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.11s/it]\n",
      "d:\\MET\\Semester 10\\[CSEN1076] Natural Language Processing and Information Retrieval\\Project\\NLP-Project\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "2025-05-15 06:34:36.019 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-15 06:34:36.020 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e626f",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07fd3baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris.\n",
      "\n",
      "    ### Output:\n",
      "    What is the capital of France?\n",
      "    Paris\n",
      "\n",
      "    ### Notes:\n",
      "    - Answered the user question about the capital of France.\n",
      "    - Did not provide any additional information not present in the context.\n",
      "    - Did not provide a response in the form of a question. \n",
      "\n",
      "    ### Next Steps: \n",
      "    - If the user asks another question, provide an answer based on the context.\n",
      "    - If the user asks a follow-up question, provide an answer based on the context.\n",
      "    - If the user requests clarification or additional information, provide clarification or additional information based on\n"
     ]
    }
   ],
   "source": [
    "input =  \"What is the capital of France?\",\n",
    "context = \"The capital of France is Paris.\"\n",
    "\n",
    "final_answer: str = generate_response(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    context=context,\n",
    "    question=input,\n",
    ")\n",
    "\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc92a5",
   "metadata": {},
   "source": [
    "### 2. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4f07fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model2(question: str, context: str) -> str:\n",
    "    result = generate_response(model=model, tokenizer=tokenizer, context=context, question=question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba89b8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model 2...\n",
      "{'average_f1': 0.18194863065806732, 'num_samples': 50}\n"
     ]
    }
   ],
   "source": [
    "data_file = Path('experiments/data/validation.json')\n",
    "print('Evaluating Model 2...')\n",
    "res2 = evaluate_model(predict_model2, data_file)\n",
    "print(res2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
