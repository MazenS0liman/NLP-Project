{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:23:52.751396Z",
     "iopub.status.busy": "2025-05-01T01:23:52.750858Z",
     "iopub.status.idle": "2025-05-01T01:26:48.065087Z",
     "shell.execute_reply": "2025-05-01T01:26:48.064031Z",
     "shell.execute_reply.started": "2025-05-01T01:23:52.751367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers\n",
    "%pip install -U datasets\n",
    "%pip install -U accelerate\n",
    "%pip install -U peft\n",
    "%pip install -U trl\n",
    "%pip install -U bitsandbytes\n",
    "%pip install -U scipy\n",
    "%pip install -U wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 3.2 3B Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:30:37.880744Z",
     "iopub.status.busy": "2025-05-01T01:30:37.879960Z",
     "iopub.status.idle": "2025-05-01T01:31:16.083280Z",
     "shell.execute_reply": "2025-05-01T01:31:16.082703Z",
     "shell.execute_reply.started": "2025-05-01T01:30:37.880715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 01:30:54.544251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746063055.015804      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746063055.144529      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import torch\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:31:19.247646Z",
     "iopub.status.busy": "2025-05-01T01:31:19.246664Z",
     "iopub.status.idle": "2025-05-01T01:31:19.252280Z",
     "shell.execute_reply": "2025-05-01T01:31:19.251527Z",
     "shell.execute_reply.started": "2025-05-01T01:31:19.247619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:31:20.675350Z",
     "iopub.status.busy": "2025-05-01T01:31:20.674593Z",
     "iopub.status.idle": "2025-05-01T01:31:20.982937Z",
     "shell.execute_reply": "2025-05-01T01:31:20.982180Z",
     "shell.execute_reply.started": "2025-05-01T01:31:20.675321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "hf_token = user_secrets.get_secret(\"HFToken\")\n",
    "wb_token = user_secrets.get_secret(\"WandB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:31:23.673833Z",
     "iopub.status.busy": "2025-05-01T01:31:23.672901Z",
     "iopub.status.idle": "2025-05-01T01:31:23.776047Z",
     "shell.execute_reply": "2025-05-01T01:31:23.775215Z",
     "shell.execute_reply.started": "2025-05-01T01:31:23.673806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:31:25.216193Z",
     "iopub.status.busy": "2025-05-01T01:31:25.215573Z",
     "iopub.status.idle": "2025-05-01T01:31:37.400440Z",
     "shell.execute_reply": "2025-05-01T01:31:37.399877Z",
     "shell.execute_reply.started": "2025-05-01T01:31:25.216167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmazen-soliman\u001b[0m (\u001b[33mmazen-m-soliman\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250501_013131-oesbhti5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mazen-m-soliman/Fine-tune%20Llama%203.2%20on%20Summarization%20Dataset/runs/oesbhti5?apiKey=c4a3b12f91cf8d5d4cc1c68151d948a721d328f8' target=\"_blank\">giddy-donkey-19</a></strong> to <a href='https://wandb.ai/mazen-m-soliman/Fine-tune%20Llama%203.2%20on%20Summarization%20Dataset?apiKey=c4a3b12f91cf8d5d4cc1c68151d948a721d328f8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mazen-m-soliman/Fine-tune%20Llama%203.2%20on%20Summarization%20Dataset?apiKey=c4a3b12f91cf8d5d4cc1c68151d948a721d328f8' target=\"_blank\">https://wandb.ai/mazen-m-soliman/Fine-tune%20Llama%203.2%20on%20Summarization%20Dataset?apiKey=c4a3b12f91cf8d5d4cc1c68151d948a721d328f8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mazen-m-soliman/Fine-tune%20Llama%203.2%20on%20Summarization%20Dataset/runs/oesbhti5?apiKey=c4a3b12f91cf8d5d4cc1c68151d948a721d328f8' target=\"_blank\">https://wandb.ai/mazen-m-soliman/Fine-tune%20Llama%203.2%20on%20Summarization%20Dataset/runs/oesbhti5?apiKey=c4a3b12f91cf8d5d4cc1c68151d948a721d328f8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune Llama 3.2 on Summarization Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:31:37.401746Z",
     "iopub.status.busy": "2025-05-01T01:31:37.401507Z",
     "iopub.status.idle": "2025-05-01T01:31:37.405856Z",
     "shell.execute_reply": "2025-05-01T01:31:37.405192Z",
     "shell.execute_reply.started": "2025-05-01T01:31:37.401714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "new_model = \"llama-3.2-3b-Summarization-Bot\"\n",
    "dataset_name = \"abisee/cnn_dailymail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:31:37.406931Z",
     "iopub.status.busy": "2025-05-01T01:31:37.406734Z",
     "iopub.status.idle": "2025-05-01T01:31:37.423683Z",
     "shell.execute_reply": "2025-05-01T01:31:37.422974Z",
     "shell.execute_reply.started": "2025-05-01T01:31:37.406906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:31:37.425289Z",
     "iopub.status.busy": "2025-05-01T01:31:37.425073Z",
     "iopub.status.idle": "2025-05-01T01:32:16.286965Z",
     "shell.execute_reply": "2025-05-01T01:32:16.286324Z",
     "shell.execute_reply.started": "2025-05-01T01:31:37.425274Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c2845b21644292aa9200adf7a4c762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142b99f303364afab1ba1283a71152ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9ef19326f640a29c1536011e15e8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b28f7230e24dffae866e821e29fb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5444d3d1604eb0aa9e206f74d9bc90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73656fc1b02a427f840a21b8f00ac547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6606d8b7bd4493931af28afcf25b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5afb42b32c24a8088b94b0010d676b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97aa2d9da20b46f58840c82ef3915b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62525ad6ea4476fb945dda0ca323c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QLoRa config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=\"uint8\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    use_safetensors=True,\n",
    "    # bnb_4bit_quant_type='nf4',\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    # device_map=\"auto\",\n",
    "    device_map=\"balanced\",\n",
    "    attn_implementation=attn_implementation,\n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:32:16.288357Z",
     "iopub.status.busy": "2025-05-01T01:32:16.288141Z",
     "iopub.status.idle": "2025-05-01T01:32:16.295202Z",
     "shell.execute_reply": "2025-05-01T01:32:16.294502Z",
     "shell.execute_reply.started": "2025-05-01T01:32:16.288340Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 3212749824, VRAM ≈ 12.85 GB\n"
     ]
    }
   ],
   "source": [
    "n = model.num_parameters()\n",
    "print(f\"Params: {n}, VRAM ≈ {n*4/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:32:16.296116Z",
     "iopub.status.busy": "2025-05-01T01:32:16.295871Z",
     "iopub.status.idle": "2025-05-01T01:32:16.319973Z",
     "shell.execute_reply": "2025-05-01T01:32:16.319148Z",
     "shell.execute_reply.started": "2025-05-01T01:32:16.296092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
       " '_load_in_8bit': False,\n",
       " '_load_in_4bit': True,\n",
       " 'llm_int8_threshold': 6.0,\n",
       " 'llm_int8_skip_modules': None,\n",
       " 'llm_int8_enable_fp32_cpu_offload': False,\n",
       " 'llm_int8_has_fp16_weight': False,\n",
       " 'bnb_4bit_quant_type': 'nf4',\n",
       " 'bnb_4bit_use_double_quant': True,\n",
       " 'bnb_4bit_compute_dtype': 'float16',\n",
       " 'bnb_4bit_quant_storage': 'uint8',\n",
       " 'load_in_4bit': True,\n",
       " 'load_in_8bit': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.quantization_config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:32:16.322520Z",
     "iopub.status.busy": "2025-05-01T01:32:16.321920Z",
     "iopub.status.idle": "2025-05-01T01:32:16.338622Z",
     "shell.execute_reply": "2025-05-01T01:32:16.337849Z",
     "shell.execute_reply.started": "2025-05-01T01:32:16.322502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: Below is a paragraph on a topic. Write a summary of the paragraph.\n",
      "\n",
      "            ### Input:\n",
      "            #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today? #Person2#: I found it would...\n",
      "             \n",
      "            ### Response:\n",
      "            Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll gi...\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "Below is a paragraph on a topic. Write a summary of the paragraph.\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_training_prompt(\n",
    "    conversation: str, summary: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "            ### Input:\n",
    "            {conversation.strip()}\n",
    "             \n",
    "            ### Response:\n",
    "            {summary}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def create_paragraph_text(data_point):\n",
    "    return data_point[\"article\"]\n",
    "\n",
    "def generate_text(data_point):\n",
    "    summary = data_point[\"highlights\"]\n",
    "    paragraph_text = create_paragraph_text(data_point)\n",
    "    return {\n",
    "        \"article\": paragraph_text,\n",
    "        \"summary\": summary,\n",
    "        \"text\": generate_training_prompt(paragraph_text, summary),\n",
    "    }\n",
    "\n",
    "# Example usage with a new dataset format\n",
    "example_data_point = {\n",
    "    \"id\": \"train_0\",\n",
    "    \"article\": \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today? #Person2#: I found it would...\",\n",
    "    \"highlights\": \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll gi...\",\n",
    "    \"topic\": \"get a check-up\"\n",
    "}\n",
    "\n",
    "example = generate_text(example_data_point)\n",
    "print(example[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:32:16.339609Z",
     "iopub.status.busy": "2025-05-01T01:32:16.339401Z",
     "iopub.status.idle": "2025-05-01T01:32:28.096601Z",
     "shell.execute_reply": "2025-05-01T01:32:28.096047Z",
     "shell.execute_reply.started": "2025-05-01T01:32:16.339586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e9ca6b35ce4cd9b10bd2ac9e3c38e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fca5bae1a50410c91309cdafc0b5d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9758655f434c4c8cbaf3820c1a45fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaa2d8abaa84fbe9d983afaa4e76404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c712c38826be4752951e4b0c2ed90942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40d62165dd5428a9d0b59450c4bca41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9328966237094d68af71c149cbea27ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2a7343e97c497b8127dfebaedb9bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9786f03856b24ccb9d1bc2343845f925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name, '1.0.0', split=\"train\", keep_in_memory=True).shuffle(seed=65).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:32:28.097458Z",
     "iopub.status.busy": "2025-05-01T01:32:28.097277Z",
     "iopub.status.idle": "2025-05-01T01:32:28.102779Z",
     "shell.execute_reply": "2025-05-01T01:32:28.102139Z",
     "shell.execute_reply.started": "2025-05-01T01:32:28.097444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def process_dataset(data: Dataset) -> Dataset:\n",
    "    \"\"\"\n",
    "    This function processes the dataset to include only the necessary columns.\n",
    "    \"\"\"\n",
    "    # First, apply generate_text to each record in the dataset\n",
    "    processed_data = data.map(generate_text)\n",
    "\n",
    "    # Then, remove unnecessary columns\n",
    "    columns_to_remove = [col for col in processed_data.column_names if col not in [\"article\", \"summary\", \"text\"]]\n",
    "    return processed_data.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:32:28.103840Z",
     "iopub.status.busy": "2025-05-01T01:32:28.103632Z",
     "iopub.status.idle": "2025-05-01T01:32:28.497317Z",
     "shell.execute_reply": "2025-05-01T01:32:28.496468Z",
     "shell.execute_reply.started": "2025-05-01T01:32:28.103826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ffd2737e8b4a00b2f21fc52d39e8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process the entire dataset\n",
    "processed_dataset = process_dataset(dataset)\n",
    "\n",
    "# Split the processed dataset into train, validation, and test sets\n",
    "train_dataset = processed_dataset.shuffle(seed=42).select(range(0, int(0.8 * len(processed_dataset))))\n",
    "validation_dataset = processed_dataset.shuffle(seed=42).select(range(int(0.8 * len(processed_dataset)), int(0.9 * len(processed_dataset))))\n",
    "test_dataset = processed_dataset.shuffle(seed=42).select(range(int(0.9 * len(processed_dataset)), len(processed_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T21:34:21.070695Z",
     "iopub.status.busy": "2025-04-30T21:34:21.070336Z",
     "iopub.status.idle": "2025-04-30T21:34:21.077571Z",
     "shell.execute_reply": "2025-04-30T21:34:21.076765Z",
     "shell.execute_reply.started": "2025-04-30T21:34:21.070670Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': \"By . Ryan Gorman . PUBLISHED: . 18:14 EST, 16 August 2013 . | . UPDATED: . 18:15 EST, 16 August 2013 . Having not heard from Goldman Sachs after applying online, Michael Penn took matters into his own hands by handing out coffee and doughnuts August 8 on the sidewalk outside the bank's lower Manhattan headquarters. Facing the end of school and having no job offers, Penn, 23, decided he had to do something to stand out. So he set up a table on the sidewalk outside Goldman Sachs advertising a website with his resume and contact information - all because he wants to work for the famous investment bank. ‘It is the pinnacle, it’s everything we . learn about in school, it is the top,’ Penn told Fox Business when . asked why he wanted to work for Goldman Sachs. Resourceful: Having not heard from Goldman Sachs after applying online, Michael Penn took his message directly to them - handing out coffee and doughnuts on the sidewalk outside the bank's lower Manhattan headquarters . Though Penn told Fox Business that . he never did get an interview with the famous investment bank, he has . had several interviews, and for that he’s grateful. Penn is graduating with a Masters in Global Management from Fordham University in two weeks and is looking for work in business development. The idea was to stand out by doing something unique, as well as getting in front of potential decision makers, Penn told multiple media outlets. ‘It’s just simple economics — supply and demand. The jobs are there. There are just more master’s and MBAs out there now. You really need to get out there and differentiate yourself,’ he told ABC News. Custom coffee cups: Custom sleeves made for the coffee he was handing out had a miniature resume on them, Penn told Fox Business he will answer every message sent to him - even hate mail . His table had a banner with his website, www.HireMichaelPenn.com, and his coffee cups came with custom-made sleeves that had contact information and a short job seeking pitch to anyone interested. It didn’t take long for Penn to hear back from someone interested in his brand of ingenuity. ‘My girlfriend saw you doing this, she took a picture,’ Penn told WPTV a potential employer said to him. ‘I'm intrigued. I want to speak with you, how fast can you get down to my office?’ Penn was there later that morning for an interview, and has had several since, according to reports. Though he still has no job offers, Penn told Fox Business the drastic action was necessary because he couldn’t see himself having to live in his parents’ basement like so many of his fellow graduates. The whole setup set Penn back only $150, he told Fox Business, and even that had to be earned. Penn’s mother funded the operation by paying him to design a website, he added. His website lists his experience, multiple degrees and even that he was a US Chess Federation National Co-Champion at age nine, in 1998, his studies on three continents and his co-founding of Finding Refuge, a non-governmental organization whose goal is to eradicate child slavery in Ghana – Archbishop Desmond Tutu is a spokesperson. Blessed: Penn told Fox Business that he feels lucky and blessed to have the opportunities he now has . Despite the down economy and considerable student loan debt from graduate school Penn remains hopeful. ‘My life has changed for the better,’ Penn said, ‘I’m so fortunate, so blessed and so grateful.’ Penn is just the latest in a series of people who have been forced to use innovative means to find jobs during the so-called Great Recession. In 2011, several people put their faces on billboards in multiple stations of San Francisco's BART transit system, and set up the website www.weallneedjobs.com. The campaign resulted in eight people finding work, according to the site. During the height of the recession, in 2009, Jamie Varon was out of work and started the website TwitterShouldHireMe.com. The site went viral on social media and Twitter executives brought her in for a meeting, she told CNN. Something else happened though, she discovered her talent for web design, so did many others. Varon has since started Shatterboxx, a web design company. 'You're not always going to get the company you're going after,Varon told CNN, 'but you could get something better.'\",\n",
       " 'summary': 'Michael Penn will graduate from Fordham University with a Masters in Global Management in two weeks . The industrious student has not heard from Goldman Sachs, but has had several interviews . Penn says the experience and surrounding attention has changed his life for the better .',\n",
       " 'text': \"### Instruction: Below is a paragraph on a topic. Write a summary of the paragraph.\\n\\n            ### Input:\\n            By . Ryan Gorman . PUBLISHED: . 18:14 EST, 16 August 2013 . | . UPDATED: . 18:15 EST, 16 August 2013 . Having not heard from Goldman Sachs after applying online, Michael Penn took matters into his own hands by handing out coffee and doughnuts August 8 on the sidewalk outside the bank's lower Manhattan headquarters. Facing the end of school and having no job offers, Penn, 23, decided he had to do something to stand out. So he set up a table on the sidewalk outside Goldman Sachs advertising a website with his resume and contact information - all because he wants to work for the famous investment bank. ‘It is the pinnacle, it’s everything we . learn about in school, it is the top,’ Penn told Fox Business when . asked why he wanted to work for Goldman Sachs. Resourceful: Having not heard from Goldman Sachs after applying online, Michael Penn took his message directly to them - handing out coffee and doughnuts on the sidewalk outside the bank's lower Manhattan headquarters . Though Penn told Fox Business that . he never did get an interview with the famous investment bank, he has . had several interviews, and for that he’s grateful. Penn is graduating with a Masters in Global Management from Fordham University in two weeks and is looking for work in business development. The idea was to stand out by doing something unique, as well as getting in front of potential decision makers, Penn told multiple media outlets. ‘It’s just simple economics — supply and demand. The jobs are there. There are just more master’s and MBAs out there now. You really need to get out there and differentiate yourself,’ he told ABC News. Custom coffee cups: Custom sleeves made for the coffee he was handing out had a miniature resume on them, Penn told Fox Business he will answer every message sent to him - even hate mail . His table had a banner with his website, www.HireMichaelPenn.com, and his coffee cups came with custom-made sleeves that had contact information and a short job seeking pitch to anyone interested. It didn’t take long for Penn to hear back from someone interested in his brand of ingenuity. ‘My girlfriend saw you doing this, she took a picture,’ Penn told WPTV a potential employer said to him. ‘I'm intrigued. I want to speak with you, how fast can you get down to my office?’ Penn was there later that morning for an interview, and has had several since, according to reports. Though he still has no job offers, Penn told Fox Business the drastic action was necessary because he couldn’t see himself having to live in his parents’ basement like so many of his fellow graduates. The whole setup set Penn back only $150, he told Fox Business, and even that had to be earned. Penn’s mother funded the operation by paying him to design a website, he added. His website lists his experience, multiple degrees and even that he was a US Chess Federation National Co-Champion at age nine, in 1998, his studies on three continents and his co-founding of Finding Refuge, a non-governmental organization whose goal is to eradicate child slavery in Ghana – Archbishop Desmond Tutu is a spokesperson. Blessed: Penn told Fox Business that he feels lucky and blessed to have the opportunities he now has . Despite the down economy and considerable student loan debt from graduate school Penn remains hopeful. ‘My life has changed for the better,’ Penn said, ‘I’m so fortunate, so blessed and so grateful.’ Penn is just the latest in a series of people who have been forced to use innovative means to find jobs during the so-called Great Recession. In 2011, several people put their faces on billboards in multiple stations of San Francisco's BART transit system, and set up the website www.weallneedjobs.com. The campaign resulted in eight people finding work, according to the site. During the height of the recession, in 2009, Jamie Varon was out of work and started the website TwitterShouldHireMe.com. The site went viral on social media and Twitter executives brought her in for a meeting, she told CNN. Something else happened though, she discovered her talent for web design, so did many others. Varon has since started Shatterboxx, a web design company. 'You're not always going to get the company you're going after,Varon told CNN, 'but you could get something better.'\\n             \\n            ### Response:\\n            Michael Penn will graduate from Fordham University with a Masters in Global Management in two weeks . The industrious student has not heard from Goldman Sachs, but has had several interviews . Penn says the experience and surrounding attention has changed his life for the better .\"}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:32:28.498453Z",
     "iopub.status.busy": "2025-05-01T01:32:28.498193Z",
     "iopub.status.idle": "2025-05-01T01:32:28.507656Z",
     "shell.execute_reply": "2025-05-01T01:32:28.506874Z",
     "shell.execute_reply.started": "2025-05-01T01:32:28.498417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:32:28.508866Z",
     "iopub.status.busy": "2025-05-01T01:32:28.508567Z",
     "iopub.status.idle": "2025-05-01T01:32:29.762655Z",
     "shell.execute_reply": "2025-05-01T01:32:29.761768Z",
     "shell.execute_reply.started": "2025-05-01T01:32:28.508840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_r = 16\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # target_modules=modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:32:29.765845Z",
     "iopub.status.busy": "2025-05-01T01:32:29.765496Z",
     "iopub.status.idle": "2025-05-01T01:32:30.656171Z",
     "shell.execute_reply": "2025-05-01T01:32:30.655369Z",
     "shell.execute_reply.started": "2025-05-01T01:32:29.765816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_prompt(\n",
    "    conversation: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"### Instruction: {system_prompt}\n",
    "\n",
    "\n",
    "### Input:\n",
    "{conversation.strip()}\n",
    "\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "def summarize(model, text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    inputs_length = len(inputs[\"input_ids\"][0])\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.0001)\n",
    "    return tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True)\n",
    "\n",
    "def generate_summaries(model, dataset, tokenizer, num_samples=5):\n",
    "    summaries = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        print(i)\n",
    "        prompt = generate_prompt(example['article'])\n",
    "        summary = summarize(model, prompt)\n",
    "        summaries.append({'article': example['article'], 'generated_summary': summary})\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T21:23:17.646677Z",
     "iopub.status.busy": "2025-04-30T21:23:17.645772Z",
     "iopub.status.idle": "2025-04-30T21:24:15.826103Z",
     "shell.execute_reply": "2025-04-30T21:24:15.825358Z",
     "shell.execute_reply.started": "2025-04-30T21:23:17.646641Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Artifacts logged anonymously cannot be claimed and expire after 7 days.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Generate summaries before fine-tuning\n",
    "original_summaries = generate_summaries(model, test_dataset, tokenizer, num_samples=5)\n",
    "\n",
    "# Convert to DataFrame and log to W&B\n",
    "df_original = pd.DataFrame(original_summaries)\n",
    "wandb.log({\"original_summaries\": wandb.Table(dataframe=df_original)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:37:18.865612Z",
     "iopub.status.busy": "2025-05-01T01:37:18.864842Z",
     "iopub.status.idle": "2025-05-01T01:37:18.977954Z",
     "shell.execute_reply": "2025-05-01T01:37:18.977315Z",
     "shell.execute_reply.started": "2025-05-01T01:37:18.865586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./out/\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=5,\n",
    "    \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,              \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100, \n",
    "    \n",
    "    learning_rate=1e-4,\n",
    "    warmup_ratio=0.05,\n",
    "    max_grad_norm=0.3,\n",
    "    \n",
    "    fp16=True,\n",
    "    group_by_length=True,\n",
    "    \n",
    "    report_to=\"wandb\",\n",
    "    \n",
    "    save_safetensors=True,\n",
    "    logging_steps=1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=42,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=512,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T21:34:36.538739Z",
     "iopub.status.busy": "2025-04-30T21:34:36.538133Z",
     "iopub.status.idle": "2025-04-30T21:34:43.387883Z",
     "shell.execute_reply": "2025-04-30T21:34:43.387078Z",
     "shell.execute_reply.started": "2025-04-30T21:34:36.538714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de22bdbd91144438e8a371c990ed14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7510582ae19c4cc5967ba009d69bd532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c8498fdcb149bf915cf921f9499363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a0c58087134c18891a9f211242fdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa594a69e8c4ba68696109aa1943b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4663a7cee6b412eb6849483417b5f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6875c19eec8d46849901268e9718e68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e715b16e28f146ef8014c3e56ebae86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T21:35:15.406485Z",
     "iopub.status.busy": "2025-04-30T21:35:15.406210Z",
     "iopub.status.idle": "2025-04-30T23:11:13.378589Z",
     "shell.execute_reply": "2025-04-30T23:11:13.377808Z",
     "shell.execute_reply.started": "2025-04-30T21:35:15.406464Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 1:35:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.946900</td>\n",
       "      <td>2.274068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.899700</td>\n",
       "      <td>2.252434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.858300</td>\n",
       "      <td>2.245346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.786600</td>\n",
       "      <td>2.236853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.928300</td>\n",
       "      <td>2.235565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.832000</td>\n",
       "      <td>2.233439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.877900</td>\n",
       "      <td>2.235057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.786400</td>\n",
       "      <td>2.233490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.869000</td>\n",
       "      <td>2.232645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.856200</td>\n",
       "      <td>2.234315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=2.1924550327062606, metrics={'train_runtime': 5754.6011, 'train_samples_per_second': 1.39, 'train_steps_per_second': 0.174, 'total_flos': 6.790213068745114e+16, 'train_loss': 2.1924550327062606})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:32:37.879351Z",
     "iopub.status.busy": "2025-05-01T01:32:37.879047Z",
     "iopub.status.idle": "2025-05-01T01:33:00.644145Z",
     "shell.execute_reply": "2025-05-01T01:33:00.643273Z",
     "shell.execute_reply.started": "2025-05-01T01:32:37.879330Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0903f45341b4c8aa6df8133938ad166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def preprocess(batch):\n",
    "    # tokenize the article text\n",
    "    tokens = tokenizer(batch[\"article\"], truncation=True, padding=\"max_length\")\n",
    "    # tokenize the summary as labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"summary\"], truncation=True, padding=\"max_length\")\n",
    "    tokens[\"labels\"] = labels[\"input_ids\"]\n",
    "    return tokens\n",
    "\n",
    "tokenized_test = test_dataset.map(preprocess, batched=True, remove_columns=[\"article\",\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:38:52.367489Z",
     "iopub.status.busy": "2025-05-01T01:38:52.367210Z",
     "iopub.status.idle": "2025-05-01T01:39:08.663989Z",
     "shell.execute_reply": "2025-05-01T01:39:08.663380Z",
     "shell.execute_reply.started": "2025-05-01T01:38:52.367468Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3200c6fef7234440bdad35fd556cfda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a104084857544c6093c846be62ecbbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86f3cc20d794a11996bb9af94d0eb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1aa55555b4c43218cbd0a48702fd9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a62de7c71a4f719553e1c1ec9d16eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3470a0c4254d5198d857569abc2791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load your best checkpoint\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/kaggle/input/llama-3b-instruct-finetuned/transformers/default/1\",\n",
    "    local_files_only=True,\n",
    ")  \n",
    "# 2. Instantiate the Trainer with your eval dataset\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=tokenized_test,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:39:11.208370Z",
     "iopub.status.busy": "2025-05-01T01:39:11.207741Z",
     "iopub.status.idle": "2025-05-01T01:43:16.855669Z",
     "shell.execute_reply": "2025-05-01T01:43:16.855152Z",
     "shell.execute_reply.started": "2025-05-01T01:39:11.208344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:43:28.232128Z",
     "iopub.status.busy": "2025-05-01T01:43:28.231791Z",
     "iopub.status.idle": "2025-05-01T01:43:28.238936Z",
     "shell.execute_reply": "2025-05-01T01:43:28.238072Z",
     "shell.execute_reply.started": "2025-05-01T01:43:28.232105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4500718116760254, 'eval_runtime': 245.5286, 'eval_samples_per_second': 0.815, 'eval_steps_per_second': 0.407}\n"
     ]
    }
   ],
   "source": [
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T23:13:45.511143Z",
     "iopub.status.busy": "2025-04-30T23:13:45.510407Z",
     "iopub.status.idle": "2025-04-30T23:13:50.149770Z",
     "shell.execute_reply": "2025-04-30T23:13:50.149108Z",
     "shell.execute_reply.started": "2025-04-30T23:13:45.511118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/models.zip'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('models', 'zip', '/kaggle/working/out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T23:13:52.130269Z",
     "iopub.status.busy": "2025-04-30T23:13:52.129797Z",
     "iopub.status.idle": "2025-04-30T23:13:52.136355Z",
     "shell.execute_reply": "2025-04-30T23:13:52.135653Z",
     "shell.execute_reply.started": "2025-04-30T23:13:52.130245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='models.zip' target='_blank'>models.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/models.zip"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'models.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T23:53:38.296863Z",
     "iopub.status.busy": "2025-04-30T23:53:38.296240Z",
     "iopub.status.idle": "2025-04-30T23:53:38.303712Z",
     "shell.execute_reply": "2025-04-30T23:53:38.302781Z",
     "shell.execute_reply.started": "2025-04-30T23:53:38.296828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "Given the text wrapped between the `<input>` tags below, generate a concise summary that:\n",
    "- Captures the main ideas and essential details\n",
    "- Does **not** include any information not present in the input\n",
    "- Uses complete sentences in paragraph form and paraphrase it if needed\n",
    "- Is no longer than 100 words\"\"\".strip()\n",
    "\n",
    "def generate_prompt(\n",
    "    conversation: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
    ") -> str:\n",
    "    return f\"\"\"\n",
    "### Role:\n",
    "    You are an expert summarizer. \n",
    "\n",
    "    \n",
    "### Instruction: {system_prompt}\n",
    "\n",
    "### Input:\n",
    "<input>\n",
    "{conversation.strip()}\n",
    "</input>\n",
    "\n",
    "### Response:\n",
    "\"\"\".strip()\n",
    "\n",
    "def summarize(model, text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    inputs_length = len(inputs[\"input_ids\"][0])\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True)\n",
    "\n",
    "def generate_response(model, text, tokenizer):\n",
    "    prompt = generate_prompt(text)\n",
    "    summary = summarize(model, prompt)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T23:56:30.989433Z",
     "iopub.status.busy": "2025-04-30T23:56:30.989174Z",
     "iopub.status.idle": "2025-04-30T23:56:41.467641Z",
     "shell.execute_reply": "2025-04-30T23:56:41.466980Z",
     "shell.execute_reply.started": "2025-04-30T23:56:30.989412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The instruction-tuned version of LaMDA-PT mixes all datasets and randomly samples from each dataset as described. The number of training examples per dataset is limited to 30k and the examples-proportional mixing scheme from T5 is followed with a mixing rate maximum of 3k. All models are fine-tuned for 30k gradient steps with a batch size of 8,192 tokens using the Adafactor Optimizer with a learning rate of 3e-5. The input and target sequence lengths used in finetuning are 1024 and 256, respectively. This instruction tuning takes around 60\n"
     ]
    }
   ],
   "source": [
    "# Replace 'input_text' with your actual input\n",
    "input_text = \"\"\"\n",
    "FLAN is the instruction-tuned version of LaMDA-PT. The instruction tuning pipeline mixes all datasets and randomly samples from each dataset as described.\n",
    "To balance the different sizes of datasets, the number of training examples per dataset is limited to 30k and the examples-proportional mixing scheme from T5 is followed with a mixing rate maximum of 3k.\n",
    "All models are fine-tuned for 30k gradient steps with a batch size of 8,192 tokens using the Adafactor Optimizer with a learning rate of 3e-5.\n",
    "The input and target sequence lengths used in finetuning are 1024 and 256, respectively.\n",
    "Packing is used to combine multiple training examples into a single sequence, separating inputs from targets using a special EOS token.\n",
    "This instruction tuning takes around 60 hours on a TPUv3 with 128 cores.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input\n",
    "summary = generate_response(model, input_text, tokenizer)\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T23:58:57.076929Z",
     "iopub.status.busy": "2025-04-30T23:58:57.076104Z",
     "iopub.status.idle": "2025-04-30T23:59:05.538501Z",
     "shell.execute_reply": "2025-04-30T23:59:05.537727Z",
     "shell.execute_reply.started": "2025-04-30T23:58:57.076893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Our model begins by embedding both the context and question token sequences and adding positional embedding. Then, it builds padding masks to ignore padded positions during attention. Next, a cross-attention layer lets each context token attend over the question representations (followed by dropout and layer normalization), injecting question-aware information into the context. Finally, a small three-layer feed-forward head projects each contextualized token into a two-dimensional logit space, from which the model slices out start and end logits for span prediction.\n"
     ]
    }
   ],
   "source": [
    "# Replace 'input_text' with your actual input\n",
    "input_text = \"\"\"\n",
    "As shown Figure 2, Our model begins by embedding both the context and question token sequences and adding positional embedding. Then, it builds padding masks to ignore padded positions during attention. The model splits its transformer layers into two halves: the “pre-cross”\n",
    "layers which represented by the first encoder layer that apply standard self-attention and\n",
    "feed-forward blocks separately to the context and question, enriching each with intra-sequence\n",
    "context. Next, a cross-attention layer lets each context token attend over the question representations (followed by dropout and layer normalization), injecting question-aware information into\n",
    "the context. After cross-attention, the “post-cross” layers represented as the second encoder\n",
    "layer again perform self-attention and feed-forward processing on the context alone, refining\n",
    "these integrated representations. Finally, a small three-layer feed-forward head, layer normalization, and dropout—projects each contextualized token into a two-dimensional logit space,\n",
    "from which the model slices out start and end logits for span prediction.\"\"\"\n",
    "\n",
    "# Tokenize the input\n",
    "summary = generate_response(model, input_text, tokenizer)\n",
    "\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "modelId": 121027,
     "modelInstanceId": 100935,
     "sourceId": 120004,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 324780,
     "modelInstanceId": 304298,
     "sourceId": 367090,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
