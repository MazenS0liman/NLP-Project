{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a40bab",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Setup</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc969863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2f0f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mazen\\AppData\\Local\\Temp\\ipykernel_4572\\644490135.py:12: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "\n",
    "from io import StringIO \n",
    "from lxml import etree\n",
    "\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import dict_to_elements\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a07bb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c891cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef0877",
   "metadata": {},
   "source": [
    "# <p style=\"padding:50px;background-color:#06402B;margin:0;color:#fafefe;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:100\">Process PDF</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a5d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\n"
     ]
    }
   ],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
    "os.environ['TESSDATA_PREFIX'] = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tessdata'\n",
    "os.environ['TESSERACT'] = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
    "\n",
    "print(rf\"{pytesseract.pytesseract.tesseract_cmd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0ae894ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./data/gpt4all.pdf\"\n",
    "\n",
    "category_counts = {}\n",
    "\n",
    "# Extract images, tables, and chunk text\n",
    "pdf_elements = partition_pdf(\n",
    "    filename=filename,\n",
    "    extract_images_in_pdf=False,\n",
    "    # strategy=\"fast\",\n",
    "    infer_table_structure=False,\n",
    "    strategy=\"hi_res\",\n",
    "    hi_res_model_name=\"yolox\",\n",
    "    max_characters=3000,\n",
    "    combine_text_under_n_chars=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fbb0b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = []\n",
    "current_title = None\n",
    "current_text = []\n",
    "\n",
    "for element in pdf_elements:\n",
    "    if element.__class__.__name__ == \"Title\":\n",
    "        if current_title is not None:\n",
    "            sections.append((current_title, \"\\n\".join(current_text)))\n",
    "        current_title = element.text.strip()\n",
    "        current_text = []\n",
    "    else:\n",
    "        txt = getattr(element, \"text\", None)\n",
    "        if txt:\n",
    "            current_text.append(txt.strip())\n",
    "\n",
    "if current_title is not None:\n",
    "    sections.append((current_title, \"\\n\".join(current_text)))\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=text, \n",
    "        metadata={\"title\": title}\n",
    "    )\n",
    "    for title, text in sections\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "017153fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "af646607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='By enabling access to large language models, the GPT4AII project also inherits many of the ethical con- cerns associated with generative models. Principal among these is the concern that unfiltered language models like GPT4AII enable malicious users to generate content that could be harmful and dangerous (e.g., in- structions on building bioweapons). While we recognize this risk, we also acknowledge the risk of concentrating this technology in the hands of a limited number of in- creasingly secretive research groups. We believe that the risk of focusing on the benefits of language model technology significantly outweighs the risk of misuse, and hence we prefer to make the technology as widely available as possible.\n",
      "Finally, we realize the challenge in assigning credit for large-scale open source initiatives. We make a first attempt at fair credit assignment by explicitly includ- ing the GPT4AII open source developers as authors on this work, but recognize that this is insufficient fully characterize everyone involved in the GPT4AII effort. Furthermore, we acknowledge the difficulty in citing open source works that do not necessarily have standard- ized citations, and do our best in this paper to provide URLs to projects whenever possible. We encourage further research in the area of open source credit as- signment, and hope to be able to support some of this research ourselves in the future.' metadata={'title': 'Limitations'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e4639b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with 16 nonempty docs\n",
      "Split into 52 total documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# 1. Filter out truly empty docs (optional but recommended)\n",
    "nonempty_docs = [d for d in docs if d.page_content and d.page_content.strip()]\n",
    "\n",
    "# 2. Set up your splitter\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    separator=\" \"\n",
    ")\n",
    "\n",
    "split_docs = splitter.split_documents(nonempty_docs)\n",
    "\n",
    "print(f\"Started with {len(nonempty_docs)} nonempty docs\")\n",
    "print(f\"Split into {len(split_docs)} total documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9f6fdd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'title': '1 Introduction'}, page_content='Questions, and a sub-sample of Bigscience/P3 (Sanh et al., 2021). Fol- lowing the approach in Stanford Alpaca (Taori et al., 2023), an open source LLaMA variant that came just be- fore GPT4AIl, we focused substantial effort on dataset curation.\\n* Shared Senior Authorship\\nThe collected dataset was loaded into Atlas (AI, 2023)—a visual interface for exploring and tagging mas- sive unstructured datasets —for data curation. Using At-\\nlas, we identified and removed subsets of the data where')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7759202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = FastEmbedEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "41e8890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take some time, patience is the key :)\n",
    "vectorstore = Qdrant.from_documents(documents=split_docs,\n",
    "                                    embedding = embeddings,\n",
    "                                    url = qdrant_url,\n",
    "                                    collection_name=\"rag\",\n",
    "                                    api_key=qdrant_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d1ac24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "3ad2b168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT4All Community', '2 The Original GPT4All Model', '3.3. The Current State of GPT4All', 'Yuvanesh Anand, Zach Nussbaum, Brandon Duder- stadt, Benjamin Schmidt, and Andriy Mulyar. 2023. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https: //github.com/nomic-ai/gpt4all.', '4 The Future of GPT4All', 'Yuvanesh Anand, Zach Nussbaum, Brandon Duder- stadt, Benjamin Schmidt, and Andriy Mulyar. 2023. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https: //github.com/nomic-ai/gpt4all.\\n\\nBBC News. 2023. Chatgpt banned in italy over privacy concerns. BBC News.', 'Yuvanesh Anand, Zach Nussbaum, Brandon Duder- stadt, Benjamin Schmidt, and Andriy Mulyar. 2023. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https: //github.com/nomic-ai/gpt4all.\\n\\nBBC News. 2023. Chatgpt banned in italy over privacy concerns. BBC News.', '2 The Original GPT4All Model\\n\\n2.1 Data Collection and Curation\\n\\n1 Introduction', '2 The Original GPT4All Model\\n\\n2.1 Data Collection and Curation\\n\\n1 Introduction', '2 The Original GPT4All Model\\n\\n2.1 Data Collection and Curation\\n\\n1 Introduction']\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"How was GPT4All trained?\")\n",
    "print([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3200b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a smart assistant helping to find the best section(s) in a document to answer a user's question.\n",
    "\n",
    "Here are the available section titles:\n",
    "{section_titles}\n",
    "\n",
    "Given the question: \"{question}\"\n",
    "\n",
    "Return a comma-separated list of the most relevant section titles to search for an answer.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "533f4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(temperature=0, model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "\n",
    "doc_chain = load_qa_with_sources_chain(llm, chain_type=\"map_reduce\")\n",
    "question_generator_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "qa_chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=question_generator_chain,\n",
    "    combine_docs_chain=doc_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e4018e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4All models were evaluated using a suite of seven reasoning tasks that were used for evaluation of the Databricks Dolly model.\n",
      "SOURCES: gpt4all.pdf\n"
     ]
    }
   ],
   "source": [
    "answer = qa_chain.run({\n",
    "    \"question\": \"How was GPT4All evaluated?\",\n",
    "    \"chat_history\": [],\n",
    "    \"filter\": filter,\n",
    "})\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
